{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Chunklet-py Documentation!","text":"<p>\u201cOne library to split them all: Sentence, Code, Docs.\u201d</p> <p>Hey there! Welcome to the Chunklet-py docs. We're stoked you're here - let's make some text chunking magic happen together! \u2728</p>"},{"location":"#why-smart-chunking-or-why-not-just-split-on-character-count","title":"Why Smart Chunking? (Or: Why Not Just Split on Character Count?)","text":"<p>You might be wondering: \"Can't I just split my text by character count or random line breaks?\" Well, sure you could... but that's like trying to cut a wedding cake with a chainsaw! \ud83c\udf82 Standard methods often give you:</p> <ul> <li>Mid-sentence surprises: Your carefully crafted thoughts get chopped right in the middle, losing all meaning</li> <li>Language confusion: Non-English text and code structures get treated like they're all the same</li> <li>Lost context: Each chunk forgets what came before, like a conversation where everyone has amnesia</li> </ul> <p>Smart chunking keeps your content's meaning and structure intact!</p>"},{"location":"#so-whats-chunklet-py-anyway-and-why-should-you-care","title":"\ud83e\udd14 So What's Chunklet-py Anyway? (And Why Should You Care?)","text":"<p>Chunklet-py is your friendly neighborhood text splitter that takes all kinds of content - from plain text to PDFs to source code - and breaks them into smart, context-aware chunks. Instead of dumb splitting, we give you specialized tools:</p> <ul> <li><code>Sentence Splitter</code></li> <li><code>Plain Text Chunker</code></li> <li><code>Document Chunker</code></li> <li><code>Code Chunker</code></li> <li><code>Chunk Visualizer</code> (Interactive web interface)</li> </ul> <p>Each tool is designed to keep your content's meaning and structure intact, plus we've got an interactive visualizer so you can see your chunks in real-time.</p> <p>Perfect for prepping data for LLMs, building RAG systems, or powering AI search - Chunklet-py gives you the precision and flexibility you need across tons of formats and languages.</p> <ul> <li> <p> Blazingly Fast</p> <p>Leverages efficient parallel processing to chunk large volumes of content with remarkable speed.</p> </li> <li> <p> Featherlight Footprint</p> <p>Designed to be lightweight and memory-efficient, ensuring optimal performance without unnecessary overhead.</p> </li> <li> <p> Rich Metadata for RAG</p> <p>Enriches chunks with valuable, context-aware metadata (source, span, document properties, code AST details) crucial for advanced RAG and LLM applications.</p> </li> <li> <p> Infinitely Customizable</p> <p>Offers extensive customization options, from pluggable token counters to custom sentence splitters and processors.</p> </li> <li> <p> Multilingual Mastery</p> <p>Supports over 50 natural languages for text and document chunking with intelligent detection and language-specific algorithms.</p> </li> <li> <p> Code-Aware Intelligence</p> <p>Language-agnostic code chunking that understands and preserves the structural integrity of your source code.</p> </li> <li> <p> Precision Chunking</p> <p>Flexible constraint-based chunking allows you to combine limits based on sentences, tokens, sections, lines, and functions.</p> </li> <li> <p> Triple Interface: CLI, Library &amp; Web</p> <p>Use it as a command-line tool, import as a library for deep integration, or launch the interactive web visualizer for real-time chunk exploration and parameter tuning.</p> </li> </ul>"},{"location":"#ready-to-get-started-lets-make-some-chunks","title":"Ready to Get Started? Let's Make Some Chunks! \ud83d\ude80","text":"<p>Welcome aboard! You're about to turn unruly walls of text into neat, manageable chunks. No more text-wrangling nightmares - Chunklet-py has your back!</p> <p>Here's your quick start guide:</p> <ul> <li> <p>Installation: Get Chunklet-py running in minutes - seriously, it's that easy!</p> </li> <li> <p>Pick Your Path:     Got a preferred way of working? We've got you covered:</p> <ul> <li> <p>CLI Fan? Love the terminal and instant results? The command line interface is perfect for quick tasks and scripting.</p> <ul> <li>Check out CLI Usage</li> </ul> </li> <li> <p>Code Ninja? Want to integrate chunking into your Python projects? The library approach gives you full control.</p> <ul> <li>Explore Programmatic Usage</li> </ul> </li> </ul> </li> </ul> <p>Whatever you choose, we're here to make chunking as smooth and maybe even a little fun. Let's do this!</p>"},{"location":"#how-does-chunklet-py-stack-up","title":"How Does Chunklet-py Stack Up?","text":"<p>Wondering how we compare to other chunking tools? Chunklet-py brings a unique mix of versatility, speed, and simplicity. Here's the quick comparison:</p> Library Key Differentiator Focus chunklet-py All-in-one, lightweight, and language-agnostic with specialized algorithms. Text, Code, Docs CintraAI Code Chunker Relies on <code>tree-sitter</code>, which can add setup complexity. Code Chonkie A feature-rich pipeline tool with cloud/vector integrations, but uses a more basic sentence splitter and <code>tree-sitter</code> for code. Pipelines, Integrations code_chunker (JimAiMoment) Uses basic regex and rules with limited language support. Code Semchunk Primarily for text, using a general-purpose sentence splitter. Text <p>Chunklet-py uses smart rule-based approaches that skip heavy dependencies (looking at you, tree-sitter!) and potential compatibility headaches. Our sentence splitting uses specialized algorithms for better accuracy, and the interactive visualizer lets you tweak settings in real-time. Perfect for projects that want power, flexibility, and a lightweight footprint.</p>"},{"location":"#the-full-tour","title":"The Full Tour","text":"<p>Curious about all the features?</p> <ul> <li>Supported Languages: See which languages Chunklet speaks fluently.</li> <li>Exceptions and Warnings: Because sometimes, things go wrong. Here's what to do when they do.</li> <li>Metadata: Understand the rich context <code>chunklet</code> attaches to your chunks.</li> <li>Troubleshooting: Solutions to common issues you might encounter.</li> </ul>"},{"location":"#stay-in-the-loop","title":"Stay in the Loop","text":"<p>Want to keep up with Chunklet-py's latest adventures?</p> <ul> <li>What's New: Discover all the exciting new features and improvements in Chunklet 2.1.0.</li> <li> <p>Migration Guide: Learn how to smoothly transition from previous versions to Chunklet 2.x.x.</p> </li> <li> <p>Changelog: See what's new, what's fixed, and what's been improved in recent versions.</p> </li> </ul>"},{"location":"#whats-working-whats-next","title":"\ud83d\uddfa What's Working &amp; What's Next","text":"<p>Already rocking these features: - [x] CLI interface for quick chunking - [x] Document chunking with rich metadata - [x] Smart code chunking that respects structure - [x] Interactive web visualizer - [x] Bonus file formats: ODT, CSV, Excel</p> <p>Coming soon (we're excited about these!): - [ ] Even more document formats</p>"},{"location":"#project-details-join-the-fun","title":"Project Details &amp; Join the Fun","text":"<p>For the behind-the-scenes info and if you're thinking of contributing:</p> <ul> <li>GitHub Repository: The main hub for all things Chunklet.</li> <li>License Information: All the necessary bits and bobs about Chunklet's license.</li> <li>Contributing: Want to help make Chunklet even better? Find out how you can contribute!                                            </li> </ul>"},{"location":"exceptions-and-warnings/","title":"Exceptions and Warnings: Navigating Chunklet's Quirks","text":"<p>Every powerful tool has its moments, and Chunklet-py is no different! This guide is here to help you understand the various messages and signals you might encounter. Most of these are straightforward to resolve, and some are simply helpful hints to keep you on track.</p>"},{"location":"exceptions-and-warnings/#exceptions-when-things-need-a-pause","title":"Exceptions: When Things Need a Pause","text":"<p>Sometimes, unexpected situations arise that require Chunklet-py to pause its operations. When you encounter an exception, it means something significant occurred that prevented the process from continuing. But don't worry, we're here to help you understand what happened!</p>"},{"location":"exceptions-and-warnings/#chunkleterror","title":"<code>ChunkletError</code>","text":"<ul> <li>Description: This is the foundational exception for all operations within the Chunklet-py library. It serves as the base class for more specific exceptions related to chunking and splitting.</li> <li>Inherits from: <code>Exception</code></li> <li>When Raised: While you typically won't encounter <code>ChunkletError</code> directly, it's the common ancestor for all our custom exceptions. This design makes it convenient to catch any Chunklet-related issues using a single <code>except ChunkletError</code> statement.</li> </ul>"},{"location":"exceptions-and-warnings/#invalidinputerror","title":"<code>InvalidInputError</code>","text":"<ul> <li>Description: Oh dear! \ud83d\ude15 This exception appears when Chunklet-py encounters input that's a little... unexpected. It's like trying to bake a cake with salt instead of sugar \u2013 the ingredients just aren't quite right! This could mean a parameter is out of place, the data type is a surprise, or the input structure is a bit wonky. Chunklet-py is always eager to help, but it does need valid input to craft those perfect chunks!</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When an invalid parameter is passed during initialization (e.g., providing a non-boolean value for a boolean flag).</li> <li>When an incorrect type of object is passed (e.g., a sentence splitter that doesn't inherit from the required base class).</li> <li>When a file path is missing a required extension.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#missingtokencountererror","title":"<code>MissingTokenCounterError</code>","text":"<ul> <li>Description: Imagine trying to bake a cake without a key ingredient like flour \u2013 that's pretty much what happens when Chunklet-py needs a <code>token_counter</code> but can't find one! This exception gently reminds you that a token counter is essential for token-based chunking.     &gt; A token_counter is required for token-based chunking.     &gt; \ud83d\udca1 Hint: Pass a token counting function to the <code>chunk</code> method, like <code>chunker.chunk(..., token_counter=tk)</code>     &gt; or configure it in the class initialization: <code>.*Chunker(token_counter=tk)</code></li> <li>Inherits from: <code>InvalidInputError</code></li> <li>When Raised:<ul> <li>When performing token-based chunking (\"token\" or \"hybrid\" modes) without a <code>token_counter</code> function having been provided.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#fileprocessingerror","title":"<code>FileProcessingError</code>","text":"<ul> <li>Description: This exception signals that Chunklet-py encountered a problem while trying to interact with a file. It could be anything from a file that's playing hide-and-seek (not found!), to permission issues, encoding troubles, or even a file that's a bit under the weather (corrupted). Essentially, something prevented us from properly loading, opening, or accessing the file.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a specified file does not exist at the given path.</li> <li>When there are issues reading a file's content due to permissions, encoding problems, or if it's a binary file.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#unsupportedfiletypeerror","title":"<code>UnsupportedFileTypeError</code>","text":"<ul> <li>Description: This exception pops up when Chunklet-py receives a file type it doesn't quite recognize. While we're always working to expand our capabilities, some file formats are simply beyond our current repertoire.</li> <li>Inherits from: <code>FileProcessingError</code></li> <li>When Raised:<ul> <li>When trying to process a file with an extension that is not supported or registered with a custom processor.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#tokenlimiterror","title":"<code>TokenLimitError</code>","text":"<ul> <li>Description: This exception occurs when a chunk exceeds the defined <code>max_tokens</code> limit. It's particularly relevant in modes where maintaining the semantic integrity of your content is paramount, even if it means not splitting a block further.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a structural block of code (like a function or class) exceeds the <code>max_tokens</code> limit while in <code>strict_mode</code>, where splitting the block would compromise its integrity.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#callbackerror","title":"<code>CallbackError</code>","text":"<ul> <li>Description: This exception occurs when a user-provided callback function (such as a <code>token_counter</code>, or a custom <code>sentence splitter</code> or <code>document processor</code>) encounters an error during its execution. It's Chunklet-py's way of letting you know that something went awry within the custom logic you've integrated.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a user-provided callback function (like a <code>token_counter</code>, or a custom <code>sentence splitter</code> or <code>document processor</code>) raises an error during its execution in any chunking or batch process.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#warnings-chunklet-pys-gentle-reminders","title":"Warnings: Chunklet-py's Gentle Reminders","text":"<p>Warnings from Chunklet-py are like friendly nudges \u2013 they let you know about situations that might warrant your attention, even though the process continues. They often highlight opportunities for optimization or important details to be aware of.</p>"},{"location":"exceptions-and-warnings/#the-language-is-set-to-auto-consider-setting-the-lang-parameter-to-a-specific-language-to-improve-reliability","title":"\"The language is set to <code>auto</code>. Consider setting the <code>lang</code> parameter to a specific language to improve reliability.\"","text":"<ul> <li>What it means: This warning appears when Chunklet-py is set to automatically detect the language of your text. While our language detection is quite capable, providing a specific <code>lang</code> parameter (e.g., <code>lang='en'</code> or <code>lang='fr'</code>) can often lead to faster and more accurate results, particularly with shorter texts. Think of it as giving Chunklet-py a helpful head start!</li> <li>Where Logged: <code>src/chunklet/sentence_splitter/sentence_splitter.py</code></li> <li>What to do: If you know the language of your text, setting the <code>lang</code> parameter explicitly is a great idea. If not, no worries \u2013 Chunklet-py will still do its best to figure it out for you.</li> </ul>"},{"location":"exceptions-and-warnings/#using-a-universal-rule-based-splitter-reason-language-not-supported-or-detected-with-low-confidence","title":"\"Using a universal rule-based splitter. Reason: Language not supported or detected with low confidence.\"","text":"<ul> <li>What it means: This warning indicates that Chunklet-py couldn't find a specialized sentence splitter for your language (or its confidence in detection was low). In such cases, it gracefully falls back to its universal rule-based regex splitter. This splitter is designed to be robust, offering a general solution for sentence segmentation.</li> <li>Where Logged: <code>src/chunklet/sentence_splitter/sentence_splitter.py</code></li> <li>What to do: For languages requiring highly accurate and nuanced sentence splitting, exploring a Custom Splitter might be beneficial. Otherwise, our universal splitter is a reliable option for general purposes.</li> </ul>"},{"location":"exceptions-and-warnings/#offset-total-sentences-returning-empty-list","title":"\"Offset {} &gt;= total sentences {}. Returning empty list.\"","text":"<ul> <li>What it means: This warning indicates that the provided <code>offset</code> value is greater than or equal to the total number of sentences in the text. Essentially, you've asked Chunklet-py to start processing beyond the available content.</li> <li>Where Logged: <code>src/chunklet/plain_text_chunker.py</code></li> <li>What to do: To resolve this, simply adjust your <code>offset</code> parameter to a value within the valid range of sentences.</li> </ul>"},{"location":"exceptions-and-warnings/#skipping-a-failed-task-nreason-error","title":"\"Skipping a failed task. \\nReason: {error}\"","text":"<ul> <li>What it means: During a batch operation, if an individual chunking task encounters an error, Chunklet-py is designed to skip that particular task and continue with the rest of the operation. This prevents the entire process from crashing and is a general warning triggered when <code>on_errors='skip'</code>.</li> <li>Where Logged: <code>src/chunklet/utils/batch_runner.py</code> (This is a general-purpose warning from the batch runner, used by <code>PlainTextChunker</code>, <code>DocumentChunker</code>, and <code>CodeChunker</code> when <code>on_errors='skip'</code>).</li> <li>What to do: We recommend checking the <code>Reason</code> provided in the warning for details about the failure. You might need to inspect the problematic input or adjust your <code>on_errors</code> parameter if you'd rather have the operation stop.</li> </ul>"},{"location":"exceptions-and-warnings/#skipping-document-at-paths-due-to-validation-failurenreason","title":"\"Skipping document '{}' at paths[{}] due to validation failure.\\nReason: {}\"","text":"<ul> <li>What it means: This warning appears when a file doesn't pass the initial validation checks before its content can be extracted. The <code>Reason</code> will give you more details about what went wrong (perhaps a corrupted file or an unsupported format). Don't worry, the invalid file is simply skipped, and processing continues.</li> <li>Where Logged: <code>src/chunklet/document_chunker/document_chunker.py</code></li> <li>What to do: Take a look at the file path and the reason provided in the warning. This should help you pinpoint and resolve the validation issue.</li> </ul>"},{"location":"exceptions-and-warnings/#no-valid-files-found-after-validation-returning-empty-generator","title":"\"No valid files found after validation. Returning empty generator.\"","text":"<ul> <li>What it means: During batch processing, after checking all your input paths, Chunklet-py discovered that none of them made the cut! \ud83e\udee3 This could happen if the paths were pointing to the wrong place, the files were in formats we don't speak, or they all got politely excused due to validation issues. In any case, we're returning an empty generator since there's nothing to work with.</li> <li>Where Logged: <code>src/chunklet/document_chunker/document_chunker.py</code> (during <code>batch_chunk</code> operations)</li> <li>What to do: Double-check those file paths and make sure they're pointing to supported file types - we're ready whenever you are!</li> </ul>"},{"location":"exceptions-and-warnings/#splitting-oversized-block-tokens-into-sub-chunks","title":"\"Splitting oversized block ({} tokens) into sub-chunks\"","text":"<ul> <li>What it means: In <code>CodeChunker</code>'s more relaxed mode, one of your code blocks turned out to be a bit of a heavyweight - too big for the <code>max_tokens</code> limit! \ud83d\udcaa Rather than throwing up its hands in defeat, Chunklet-py decided to be extra helpful and split it into more manageable sub-chunks.</li> <li>Where Logged: <code>src/chunklet/experimental/code_chunker/code_chunker.py</code> (during <code>chunk</code> operations when <code>strict_mode=False</code>)</li> <li>What to do: This is just a friendly heads-up about what we're doing. If you want us to be more strict about those token limits for code blocks, just set <code>strict_mode=True</code> in <code>CodeChunker</code>.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-path-is-path-like-but-was-not-found-or-is-not-a-processable-filedirectory-skipping","title":"\"Warning: '{path}' is path-like but was not found or is not a processable file/directory. Skipping.\"","text":"<ul> <li>What it means: The path you provided via <code>--source</code> looked promising at first glance, but when we went to find it, it was either playing hide-and-seek (doesn't exist!) or turned out to be something special like a socket or pipe that we can't process. No worries - we'll just skip this one and keep going.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Give that path another look to make sure it's correct and points to a regular file or directory we can handle.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-path-does-not-resemble-a-valid-file-system-path-failed-heuristic-check-skipping","title":"\"Warning: '{path}' does not resemble a valid file system path (failed heuristic check). Skipping.\"","text":"<ul> <li>What it means: The string you gave us via <code>--source</code> just doesn't look like a proper file system path to our trained eye. We use a clever heuristic to spot path-like strings, but this one didn't pass the test. We'll politely skip it and move along.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Make sure your source path is a proper, well-formed file system path - we're all set to handle it once it's right.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-no-processable-files-found-in-the-specified-sources-exiting","title":"\"Warning: No processable files found in the specified source(s). Exiting.\"","text":"<ul> <li>What it means: We searched high and low through all the <code>--source</code> paths you provided, but couldn't find a single file we could process. \ud83d\udd75\ufe0f\u200d\u2640\ufe0f Maybe the directories are feeling a bit empty, or they only contain file types we don't speak yet. In any case, there's nothing for us to do, so we're gracefully bowing out.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Double-check those source paths and make sure they contain files in formats we support.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-no-chunks-were-generated-this-might-be-because-the-input-was-empty-or-did-not-contain-any-processable-content","title":"\"Warning: No chunks were generated. This might be because the input was empty or did not contain any processable content.\"","text":"<ul> <li>What it means: The chunking process ran its course, but unfortunately, no chunks came out the other side. This often happens when your input text was empty, or the files you provided turned out to be text-free zones with nothing we could extract. Since there's nothing to show for our efforts, we're calling it a day.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Take a peek at your input to make sure there's some actual text content we can work with.</li> </ul>"},{"location":"migration/","title":"Migration Guide from v1 to v2: What's New and How to Adapt!","text":"<p>Important: Python Version Support</p> <p>Chunklet-py v2.x.x has dropped official support for Python 3.8 and 3.9. The minimum required Python version is now 3.10. Please ensure your environment is updated to Python 3.10 or newer for compatibility.</p> <p>Hello there, fellow Chunklet enthusiast! \ud83d\udc4b Ready to explore the exciting new world of Chunklet v2? We've been hard at work, making Chunklet-py even more robust, flexible, and, dare we say, efficient! This guide is designed to walk you through all the fantastic changes and help you smoothly transition your existing code. No need to worry, we're here to support you every step of the way!</p>"},{"location":"migration/#breaking-changes-a-quick-heads-up","title":"\ud83d\udca5 Breaking Changes: A Quick Heads-Up! \ud83d\udca5","text":"<p>We've implemented some significant changes to enhance Chunklet-py's architecture and overall usability. While these updates might require minor adjustments to your existing code, we believe the improvements are well worth it!</p>"},{"location":"migration/#renamed-chunklet-class-to-plaintextchunker","title":"Renamed <code>Chunklet</code> class to <code>PlainTextChunker</code>","text":"<p>Our core chunking class now has a new, more descriptive name!</p> <p>What's new with <code>Chunklet</code>? The class you knew as <code>Chunklet</code> has been thoughtfully renamed to <code>PlainTextChunker</code>!</p> <p>Why the glow-up? We wanted to give it a name that really screams 'I chunk plain text!' This clears the stage for other awesome chunkers (like our new DocumentChunker and CodeChunker) to shine. It's all about clarity and making Chunklet's family tree a bit more logical!</p> <p>How to adapt your code? A simple find-and-replace will do the trick! Just update your imports and class instantiations:</p> Before (v1.4.0)After (v2.x.x) <pre><code>from chunklet import Chunklet\nchunker = Chunklet()\n</code></pre> <pre><code>from chunklet import PlainTextChunker\nchunker = PlainTextChunker()\n</code></pre>"},{"location":"migration/#removed-use_cache-flag-from-plaintextchunker","title":"Removed <code>use_cache</code> flag from <code>PlainTextChunker</code>","text":"<p>The <code>use_cache</code> flag has been removed from the <code>PlainTextChunker</code>.</p> <p>Where did <code>use_cache</code> go? The <code>use_cache=False</code> flag has been officially retired! You won't find it in <code>PlainTextChunker</code>'s <code>chunk</code> or <code>batch_chunk</code> methods anymore.</p> <p>Why the change? We've streamlined Chunklet-py to manage its own caching internally, optimizing for speed without requiring any manual intervention from you. This simplifies the API, allowing you to focus on the core task of chunking!</p> <p>How to adapt your code: Simply remove the <code>use_cache</code> argument from your <code>chunk</code> and <code>batch_chunk</code> calls. It's a small change that leads to a cleaner API!</p> Before (v1.4.0)After (v2.x.x) <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, use_cache=False)\n</code></pre> <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text)\n</code></pre>"},{"location":"migration/#removed-preview_sentences-method","title":"Removed <code>preview_sentences</code> method","text":"<p>The <code>preview_sentences</code> method has been removed from the main chunker class.</p> <p>Missing <code>preview_sentences</code>? This method has transitioned out of the <code>PlainTextChunker</code> (formerly <code>Chunklet</code>) instance. It's a sign of growth!</p> <p>Why the change? We've refactored the sentence splitting logic into its own dedicated utility, SentenceSplitter! This enhances modularity and flexibility, giving sentence splitting the focused attention it deserves.</p> <p>How to access sentence splitting now? You can directly utilize the <code>SentenceSplitter</code> class. It's ready for action:</p> Before (v1.4.0)After (v2.x.x) <pre><code>from chunklet import Chunklet\nchunker = Chunklet()\nsentences, warnings = chunker.preview_sentences(text, lang=\"en\")\n</code></pre> <pre><code>from chunklet import SentenceSplitter\nsplitter = SentenceSplitter()\nsentences = splitter.split(text, lang=\"en\")\n</code></pre>"},{"location":"migration/#constraint-handling-mode-removed-explicit-limits","title":"Constraint Handling: <code>mode</code> Removed, Explicit Limits","text":"<p>We've streamlined how chunking constraints are managed, moving towards more explicit control and introducing new options for granular segmentation.</p> <p>What's changed? - Removed <code>mode</code> argument: The <code>mode</code> argument (e.g., \"sentence\", \"token\", \"hybrid\") has been removed from the CLI and <code>PlainTextChunker</code>'s <code>chunk</code> and <code>batch_chunk</code> methods. The chunking strategy is now implicitly determined by the combination of constraint flags you provide (<code>max_tokens</code>, <code>max_sentences</code>, <code>max_section_breaks</code>). - No More Default Values: <code>max_tokens</code> and <code>max_sentences</code> no longer have implicit default values. You must now explicitly set these if you wish to use them. - New Constraint Flags: We've introduced <code>max_section_breaks</code> (for <code>PlainTextChunker</code> and <code>DocumentChunker</code>) and <code>max_lines</code> (for <code>CodeChunker</code>) to provide even more granular control over your chunking strategy.</p> <p>Why the change? This approach provides clearer, more explicit control over your chunking strategy, preventing unexpected behavior from implicit defaults and allowing for more precise customization. It simplifies the API by removing a redundant argument and provides more direct control over how chunks are formed.</p> <p>How to adapt your code: - Instead of specifying a <code>mode</code>, simply provide the desired constraint flags. For example, to chunk by sentences, provide <code>max_sentences</code>. To chunk by tokens, provide <code>max_tokens</code>. - Explicitly set <code>max_tokens</code> or <code>max_sentences</code> if you were relying on their previous defaults. - Utilize the new <code>max_section_breaks</code> and <code>max_lines</code> flags for advanced chunking control.</p> Before (v1.4.0)After (v2.x.x) <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, mode=\"sentence\", max_sentences=5) # Implicit max_tokens=512\n</code></pre> <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, max_sentences=5, max_tokens=512) # Mode is implicit, max_tokens explicit\n</code></pre>"},{"location":"migration/#language-detection-logic-integrated","title":"Language Detection Logic Integrated","text":"<p>The standalone language detection utility has found a new home!</p> <p>Where's the language expert? Our old <code>detect_text_language.py</code> has hung up its hat (or rather, its file path). Its brilliant brainpower is now living directly inside <code>src/chunklet/sentence_splitter/sentence_splitter.py</code>, making language detection a super-integrated part of the splitting process!</p> <p>Why the internal move? We wanted to simplify the internal magic and put our language detective right on the front lines with the sentence-splitting squad! Optimized, integrated, and ready to roll!</p> <p>Need to find your language? If you were directly importing <code>detect_text_language</code>, you'll need to update those imports. But good news for most: if you're interacting with Chunklet via the <code>PlainTextChunker</code>, all this magic happens behind the scenes! Need to explicitly detect language? <code>SentenceSplitter</code>'s got a fresh <code>detected_top_language</code> method just for you:</p> Before (v1.4.0)After (v2.x.x) <pre><code>from chunklet.utils.detect_text_language import detect_text_language\nlang, confidence = detect_text_language(text)\n</code></pre> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\nsplitter = SentenceSplitter()\nlang, confidence = splitter.detected_top_language(text)\n</code></pre>"},{"location":"migration/#custom-sentence-splitters-your-rules-our-game","title":"Custom Sentence Splitters: Your Rules, Our Game!","text":"<p>Have a unique way you prefer your sentences split? We've made it even simpler to integrate your own custom sentence splitting logic!</p> <p>Say goodbye to <code>custom_splitters</code> parameter! The <code>custom_splitters</code> parameter in our (now named <code>PlainTextChunker</code>) constructor has gracefully retired. Custom splitters now reside in a super handy, centralized registry (<code>src/chunklet/sentence_splitter/registry.py</code>)!</p> <p>Why the registry revamp? We've crafted a more robust, organized, and flexible hub for your custom splitting logic! This change thoughtfully decouples splitter registration from the <code>PlainTextChunker</code>, enabling global registration and effortless reuse of your fantastic custom splitters across all instances. It's like giving your custom splitters the VIP treatment they deserve!</p> <p>Time for a quick code update: If you were using that old <code>custom_splitters</code> parameter, it's time to embrace our new, more elegant registry system. We're confident you'll find it a breeze! For more details on how to create and register your own splitters, see the Custom Sentence Splitter documentation.</p> Before (v1.4.0)After (v2.x.x) <pre><code>import re\nfrom chunklet import Chunklet\nfrom typing import List\n\n# Define a simple custom sentence splitter\ndef my_custom_splitter(text: str) -&gt; List[str]:\n    # This is a very basic splitter for demonstration\n    # In a real scenario, this would be a more sophisticated function\n    return [s.strip() for s in re.split(r'(?&lt;=\\.)\\\\s+', text) if s.strip()]\n\n# Initialize Chunklet with the custom splitter\nchunker = Chunklet(\n    custom_splitters=[\n        {\n            \"name\": \"MyCustomEnglishSplitter\",\n            \"languages\": \"en\",\n            \"callback\": my_custom_splitter,\n        }\n    ]\n)\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences, warnings = chunker.preview_sentences(text=text, lang=\"en\")\n\nprint(\"---\" + \" Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings:\n    print(\"\\n---\" + \" Warnings ---\")\n    for warning in warnings:\n        print(warning)\n</code></pre> <pre><code>from chunklet.sentence_splitter.registry import registered_splitter\nfrom chunklet import PlainTextChunker # Updated class name\nimport re\n\n# If 'name' is not provided, the function's name ('my_awesome_splitter' in this case) will be used.\n# When using the decorator, the decorated function itself is automatically registered as the callback;\n@registered_splitter(\"en\", name=\"MyAwesomeEnglishSplitter\")\ndef my_awesome_splitter(text: str) -&gt; list[str]:\n    # Your super-duper custom splitting logic here!\n    return [s.strip() for s in re.split(r'[.!?]\\s+', text) if s.strip()]\n\n# If you prefer not to use decorators, you can use the 'register_splitter' function instead.\n# from chunklet.sentence_splitter.registry import register_splitter\n# register_splitter(\"en\", callback=my_awesome_splitter, name=\"MyAwesomeEnglishSplitter\")\n\n# Now, when you use PlainTextChunker with lang=\"en\", it will use your splitter!\nchunker = PlainTextChunker()\ntext = \"Hello world! How are you? I am fine.\"\nsentences = chunker.chunk(text, lang=\"en\", max_sentences=1) # Use chunk method\nprint(sentences)\n</code></pre>"},{"location":"migration/#exception-renames-and-changes","title":"Exception Renames and Changes","text":"<p>We've refined our exception handling to provide more clarity and specificity.   -   <code>TokenNotProvidedError</code> is now <code>MissingTokenCounterError</code>: This exception is raised when a <code>token_counter</code> is required but not provided.   -   <code>CallbackError</code> for Token Counter Failures: Previously, issues within user-provided token counters might have raised a generic <code>ChunkletError</code>. Now, a more specific <code>CallbackError</code> is raised, making debugging easier.  </p>"},{"location":"migration/#cli-usage-changes","title":"CLI Usage Changes","text":"<p>In v1.4.0, the <code>chunklet</code> CLI had a simpler structure, primarily focused on plain text. In v2.x.x, the CLI has been reorganized for clarity and to support different chunkers.</p> <p>New <code>chunk</code> command and chunker selection! A new <code>chunk</code> command is now the main entrypoint for all chunking operations. - When you provide text directly as an argument, <code>PlainTextChunker</code> is used. - When you use <code>--source</code> to provide a file path, <code>DocumentChunker</code> is used by default to handle a variety of document types. - You can use flags like <code>--code</code> to explicitly select the <code>CodeChunker</code>.</p> <p>Why the new structure? This change provides a clearer and more extensible command-line interface, making it easier to select the right chunker for your content.</p> <p>For more details, see the CLI Usage documentation.</p> Before (v1.4.0)After (v2.x.x) <pre><code>chunklet \"Your text here.\" --mode sentence --max-sentences 5\n</code></pre> <pre><code># Chunking a string uses PlainTextChunker\nchunklet chunk \"Your text here.\" --max-sentences 5\n\n# Chunking a file from a path uses DocumentChunker by default\nchunklet chunk --source your_text.txt --max-sentences 5\n</code></pre> <p>That's all for this migration guide! We truly hope these updates enhance your Chunklet-py experience and make your chunking tasks even more enjoyable. Happy chunking! \ud83c\udf89</p>"},{"location":"supported-languages/","title":"Supported Languages: A World Tour","text":"<p>Curious about the languages Chunklet-py supports? You're in the right place! We've built Chunklet-py to be quite the language expert, thanks to some fantastic third-party libraries. When we talk about language codes, we're usually using the ISO 639-1 standard (those handy two-letter codes). If you're ever wondering about other language codes, Wikipedia's List of ISO 639 language codes is a great resource.</p>"},{"location":"supported-languages/#the-all-stars-officially-supported-languages","title":"\u2b50 The All-Stars: Officially Supported Languages","text":"<p>Let's dive into the languages where Chunklet-py truly shines! Through wonderful collaborations with various libraries, we're proud to offer dedicated, high-quality splitters for over 50 languages. And if your language isn't in this impressive lineup, don't you worry \u2013 our dependable Fallback Splitter is always ready to lend a hand. Below, you'll discover the specific libraries that make this extensive language support possible.</p>"},{"location":"supported-languages/#headliner-pysbd","title":"Headliner: <code>pysbd</code>","text":"<p>Meet <code>pysbd</code>, one of our primary tools for accurate sentence boundary detection. This library is highly effective at identifying sentence endings, even in complex linguistic contexts.</p> Language Code Language Name Flag en English \ud83c\uddec\ud83c\udde7 mr Marathi \ud83c\uddee\ud83c\uddf3 hi Hindi \ud83c\uddee\ud83c\uddf3 bg Bulgarian \ud83c\udde7\ud83c\uddec es Spanish \ud83c\uddea\ud83c\uddf8 ru Russian \ud83c\uddf7\ud83c\uddfa ar Arabic \ud83c\uddf8\ud83c\udde6 am Amharic \ud83c\uddea\ud83c\uddf9 hy Armenian \ud83c\udde6\ud83c\uddf2 fa Persian (Farsi) \ud83c\uddee\ud83c\uddf7 ur Urdu \ud83c\uddf5\ud83c\uddf0 pl Polish \ud83c\uddf5\ud83c\uddf1 zh Chinese (Mandarin) \ud83c\udde8\ud83c\uddf3 nl Dutch \ud83c\uddf3\ud83c\uddf1 da Danish \ud83c\udde9\ud83c\uddf0 fr French \ud83c\uddeb\ud83c\uddf7 it Italian \ud83c\uddee\ud83c\uddf9 el Greek \ud83c\uddec\ud83c\uddf7 my Burmese (Myanmar) \ud83c\uddf2\ud83c\uddf2 ja Japanese \ud83c\uddef\ud83c\uddf5 de German \ud83c\udde9\ud83c\uddea kk Kazakh \ud83c\uddf0\ud83c\uddff sk Slovak \ud83c\uddf8\ud83c\uddf0"},{"location":"supported-languages/#special-guest-sentsplit","title":"Special Guest: <code>sentsplit</code>","text":"<p><code>sentsplit</code> complements our primary tools by providing support for additional languages. It effectively extends our coverage for diverse linguistic needs.</p> Language Code Language Name Flag ko Korean \ud83c\uddf0\ud83c\uddf7 lt Lithuanian \ud83c\uddf1\ud83c\uddf9 pt Portuguese \ud83c\uddf5\ud83c\uddf9 tr Turkish \ud83c\uddf9\ud83c\uddf7"},{"location":"supported-languages/#the-dance-troupe-indic-nlp-library","title":"The Dance Troupe: <code>Indic NLP Library</code>","text":"<p>The <code>Indic NLP Library</code> is crucial for supporting the rich and diverse languages of the Indian subcontinent. It provides comprehensive linguistic support for these languages.</p> Language Code Language Name Flag as Assamese \ud83c\uddee\ud83c\uddf3 bn Bengali \ud83c\uddee\ud83c\uddf3 gu Gujarati \ud83c\uddee\ud83c\uddf3 kn Kannada \ud83c\uddee\ud83c\uddf3 ml Malayalam \ud83c\uddee\ud83c\uddf3 ne Nepali \ud83c\uddf3\ud83c\uddf5 or Odia \ud83c\uddee\ud83c\uddf3 pa Punjabi \ud83c\uddee\ud83c\uddf3 sa Sanskrit \ud83c\uddee\ud83c\uddf3 ta Tamil \ud83c\uddee\ud83c\uddf3 te Telugu \ud83c\uddee\ud83c\uddf3"},{"location":"supported-languages/#the-versatile-voice-sentencex","title":"The Versatile Voice: <code>Sentencex</code>","text":"<p><code>Sentencex</code> significantly expands Chunklet's language capabilities. This library contributes a substantial collection of languages, ensuring broad and comprehensive coverage.</p> <p>Note</p> <p><code>Sentencex</code> is a powerful library that uses a fallback system to support a vast number of languages.  It uses a fallback system to support a vast number of languages. Many languages are mapped to fallbacks of more common languages. The list below is a curated selection of the more reliable and unique languages from <code>Sentencex</code>. It has been filtered to: *   Include only languages with an ISO 639-1 code. *   Exclude languages that are already covered by <code>pysbd</code>, <code>sentsplit</code>, or <code>Indic NLP Library</code>. *   Exclude languages that are fallbacks to other languages in the list but are not reliable enough.</p> Language Code Language Name Flag an Aragonese \ud83c\uddea\ud83c\uddf8 ca Catalan \ud83c\uddea\ud83c\uddf8 co Corsican \ud83c\uddeb\ud83c\uddf7 cs Czech \ud83c\udde8\ud83c\uddff fi Finnish \ud83c\uddeb\ud83c\uddee gl Galician \ud83c\uddea\ud83c\uddf8 io Ido \ud83c\udff3\ufe0f jv Javanese \ud83c\uddee\ud83c\udde9 li Limburgish \ud83c\uddf3\ud83c\uddf1 mo Moldovan \ud83c\uddf2\ud83c\udde9 nds Low German \ud83c\udde9\ud83c\uddea nn Norwegian Nynorsk \ud83c\uddf3\ud83c\uddf4 oc Occitan \ud83c\uddeb\ud83c\uddf7 su Sundanese \ud83c\uddee\ud83c\udde9 wa Walloon \ud83c\udde7\ud83c\uddea"},{"location":"supported-languages/#the-universal-translator-fallback-splitter","title":"The Universal Translator: Fallback Splitter","text":"<p>API Reference</p> <p>The API documentation for the universal fallback splitter can be found in the <code>FallbackSplitter</code> API docs file.</p> <p>For languages not covered by our specialized libraries, the Fallback Splitter steps in. Consider it Chunklet's adaptable solution, a rule-based regex splitter designed to provide a reasonable attempt at sentence segmentation for any language. While it may not offer the nuanced precision of language-specific tools, it's a dependable option to ensure no language is left unaddressed.</p>"},{"location":"supported-languages/#teaching-chunklet-new-tricks-custom-splitters","title":"Teaching Chunklet New Tricks: Custom Splitters","text":"<p>What if your specific language or domain requires a unique approach to sentence splitting? Or perhaps you have a very particular method in mind? No need to worry! Chunklet-py is designed to be flexible, allowing you to implement and integrate your own Custom Splitter.</p> <p>You can integrate your own sentence splitting logic in two ways:</p> <p>a) The Function Call Method (A Direct Approach):</p> <pre><code>from chunklet.sentence_splitter.registry import register_splitter\n\ndef my_custom_splitter(text: str) -&gt; list[str]:\n    # Your brilliant, custom splitting logic here\n    return text.split('.')\n\n# Teach Chunklet your new trick for English\nregister_splitter('en', callback=my_custom_splitter, name='MyCustomSplitter')\n</code></pre> <p>b) The Decorator Method (An Elegant Approach):</p> <pre><code>from chunklet.sentence_splitter.registry import registered_splitter\n\n@registered_splitter('fr', name='MyFrenchSplitter')\ndef my_french_splitter(text: str) -&gt; list[str]:\n    # Your magnifique splitting logic for French\n    return text.split('!')\n</code></pre> <p>Global Splitter Magic</p> <p>Feeling extra global? You can register a splitter with the special language code <code>xx</code>. This makes it a universal fallback that you can explicitly call by setting <code>lang='xx'</code> in your chunking operations. Pretty neat, huh?</p>"},{"location":"troubleshooting/","title":"Troubleshooting: Your Chunklet-py Adventure Guide \ud83d\udee0\ufe0f","text":"<p>Welcome to the troubleshooting guide! Here you'll find solutions to common issues you might encounter while using <code>chunklet-py</code>. Think of this as your friendly roadmap when things don't go quite as planned.</p>"},{"location":"troubleshooting/#batch-processing-hangs-or-fails-on-exit","title":"Batch Processing Hangs or Fails on Exit","text":"Why does <code>batch_chunk</code> hang, and then show a <code>TypeError</code> when I try to exit with <code>Ctrl+C</code>? <p>This happens when you use <code>batch_chunk</code> but don't fully iterate through all the results. For example, using a <code>break</code> statement to stop early.</p> <p>The <code>batch_chunk</code> method uses a multiprocessing pool in the background. If you exit the loop early, the generator gets abandoned without proper cleanup. Those background processes can get stuck in limbo, and when you hit <code>Ctrl+C</code>, Python tries to clean up but fails with a <code>TypeError: 'NoneType' object is not callable</code>.</p> <p>Solution:</p> <p>The fix is to make sure the generator always gets fully consumed or explicitly closed. Think of it as making sure you finish your meal before leaving the table!</p> <p>Option 1: Explicitly Close the Generator (Recommended)</p> <p>The most reliable approach is to wrap your loop in a <code>try...finally</code> block and call the <code>close()</code> method on the generator. This ensures proper cleanup even if you bail out early with a break.</p> <p>Here is an example:</p> <pre><code>from chunklet import DocumentChunker\n\npaths = [\"path/to/your/doc1.pdf\", \"path/to/your/doc2.txt\"]\nchunker = DocumentChunker()\nchunks_generator = chunker.batch_chunk(paths)\n\ntry:\n    for i, chunk in enumerate(chunks_generator):\n        if i &gt;= 10:  # Example: Stop after 10 chunks\n            break\n        print(chunk.content)\nfinally:\n    chunks_generator.close()\n</code></pre> <pre><code>By explicitly closing the generator, you ensure all background processes get properly cleaned up, preventing the hang and letting your program exit gracefully.\n</code></pre> <p>Option 2: Convert to a List</p> <p>If you don't need chunks as they're generated and prefer having everything ready at once, you can convert the generator to a list. This forces full consumption and ensures the multiprocessing pool shuts down properly.</p> <pre><code>from chunklet import DocumentChunker\n\npaths = [\"path/to/your/doc1.pdf\", \"path/to/your/doc2.txt\"]\nchunker = DocumentChunker()\nall_chunks = list(chunker.batch_chunk(paths))\n\nfor i, chunk in enumerate(all_chunks):\n    if i &gt;= 10:  # Example: You can still break, but the pool is already closed\n        break\n    print(chunk.content)\n</code></pre> <pre><code>This approach is simpler if memory isn't a concern and you need all chunks ready before moving on.\n</code></pre> <p>Related Reading: *   mpire Issue #141: Fork-mode processes hanging *   Why your multiprocessing Pool is stuck</p>"},{"location":"whats-new/","title":"What's New","text":"<p>What's on This Page</p> <p>This page highlights the big features and major changes for each version. For all the nitty-gritty details, bug fixes, and technical improvements, check out our full changelog.</p>"},{"location":"whats-new/#whats-new-in-chunklet-v211","title":"What's New in Chunklet v2.1.1! \ud83d\udc1b","text":""},{"location":"whats-new/#critical-bug-fix-in-v211","title":"\ud83d\udc1b Critical Bug Fix in v2.1.1","text":"<ul> <li>Visualizer Static Files Issue: \ud83d\udea8 CRITICAL - Fixed a breaking bug where the Chunk Visualizer static files (CSS, JS, HTML) were missing from the PyPI package distribution. This caused <code>RuntimeError: Directory does not exist</code> when running <code>chunklet visualize</code>. The visualizer now works correctly after installation!</li> </ul>"},{"location":"whats-new/#whats-new-in-chunklet-v210","title":"What's New in Chunklet v2.1.0! \ud83c\udf89","text":""},{"location":"whats-new/#major-features-in-v210","title":"\u2728 Major Features in v2.1.0","text":"<ul> <li>Interactive Chunk Visualizer: \ud83c\udf10 Launch a web-based interface for real-time chunk visualization, parameter tuning, and exploring your chunking results interactively!</li> <li>CLI Visualize Command: \ud83d\udcbb Use <code>chunklet visualize</code> to start the web interface with customizable host, port, and tokenizer options.</li> <li>Expanded File Format Support: \ud83d\udcc1 Added support for ODT files (.odt) and tabular files (.csv and .xlsx) to handle even more document types.</li> </ul>"},{"location":"whats-new/#bug-fixes-in-v210","title":"\ud83d\udc1b Bug Fixes in v2.1.0","text":"<ul> <li>Code Chunker Issues: \ud83d\udd27 Fixed multiple bugs in CodeChunker including line skipping in oversized blocks, decorator separation, path detection errors, and redundant processing logic.</li> <li>CLI Path Validation Bug: Resolved TypeError where len() was called on PosixPath object. Thanks to @arnoldfranz for reporting this issue.</li> <li>Hidden Bugs Uncovered: \ud83d\udd75\ufe0f\u200d\u2642\ufe0f Adding comprehensive test coverage revealed and fixed multiple hidden bugs in document chunker batch processing error handling that were previously undetected.</li> </ul>"},{"location":"whats-new/#whats-new-in-chunklet-v201","title":"What's New in Chunklet v2.0.1! \ud83c\udf89","text":""},{"location":"whats-new/#patch-fixes-in-v201","title":"\u2728 Patch Fixes in v2.0.1","text":"<ul> <li>CLI Bug Fix: Fixed a tricky unpacking bug in the <code>split</code> command that was causing incorrect results. The fix properly separates language detection from sentence splitting for accurate output.</li> </ul>"},{"location":"whats-new/#whats-new-in-chunklet-v203","title":"What's New in Chunklet v2.0.3! \ud83c\udf89","text":""},{"location":"whats-new/#improvements-in-v203","title":"\u2728 Improvements in v2.0.3","text":"<ul> <li>Enhanced Span Detection: \ud83e\udded Fixed some hardcoded limits and added adaptive calculations for better span detection across different text lengths.</li> <li>Improved Regex Performance: \u26a1 Switched from fuzzysearch to optimized regex for faster and more precise span finding.</li> <li>Dependency Cleanup: \ud83e\uddf9 Removed the fuzzysearch dependency to keep things lighter and simpler.</li> </ul>"},{"location":"whats-new/#whats-new-in-chunklet-v202","title":"What's New in Chunklet v2.0.2! \ud83c\udf89","text":""},{"location":"whats-new/#refinements-in-v202","title":"\u2728 Refinements in v2.0.2","text":"<ul> <li>Code Cleanup: \ud83e\uddf9 Removed some debug print statements from the <code>SentenceSplitter</code> for cleaner production code.</li> </ul>"},{"location":"whats-new/#whats-new-in-chunklet-v200","title":"What's New in Chunklet v2.0.0! \ud83c\udf89","text":""},{"location":"whats-new/#highlights-of-v200","title":"\u2728 Highlights of v2.0.0","text":"<ul> <li>Class Renaming: The <code>Chunklet</code> class has been renamed to <code>PlainTextChunker</code> for clearer naming. Don't worry about updating your code - our Migration Guide has you covered!</li> <li>Continuation marker: \ud83d\udcd1 Improved the continuation marker logic and exposed its value so you can define your own or disable it entirely.</li> <li>Code Chunker Introduction: We're excited to introduce <code>CodeChunker</code>! \ud83e\uddd1\u200d\ud83d\udcbb This new rule-based, language-agnostic chunker provides smart syntax-aware code splitting - perfect for code-related RAG applications.</li> <li>Document Chunker Introduction: We're pleased to introduce <code>DocumentChunker</code>! \ud83d\udcc4 This robust tool handles a wide variety of file formats including PDF, DOCX, TXT, MD, RST, RTF, TEX, HTML, and EPUB files.</li> <li>Expanded Language Support: \u00a1Hola! Bonjour! Namaste! \ud83d\udde3\ufe0f We've expanded from 36+ to over 50 languages thanks to our library integrations and smart fallback mechanisms.</li> <li>New Constraint Flags: Added <code>max_section_breaks</code> for PlainTextChunker and DocumentChunker, plus <code>max_lines</code> for CodeChunker - giving you more precise control over chunking.</li> <li>Improved Error Handling: Added more specific exception types (like <code>FileProcessingError</code> and <code>CallbackError</code>) and centralized batch error handling for clearer feedback and better control.</li> <li>Flexible Batch Error Handling: The new <code>on_errors</code> parameter lets you control what happens when errors occur in batches - you can <code>raise</code>, <code>skip</code>, or <code>break</code> as needed.</li> <li>CLI Refactoring: Streamlined the command-line interface with simplified flags and improved batch processing capabilities for a smoother experience.</li> <li>Modularity &amp; Extensibility: Made the library more modular with a dedicated <code>SentenceSplitter</code> and flexible custom splitter registry for easier customization.</li> <li>Performance &amp; Memory Optimization: Significant refactoring with generators for batch methods to drastically reduce memory usage, especially for large documents.</li> <li>Caching Strategy Refined: We've gone lean and mean! \u267b\ufe0f Removed most in-memory caching to prioritize performance, keeping only <code>count_tokens</code> cached.</li> <li>Python 3.8/3.9 Support Dropped: Time marches on, and so do we! \ud83d\udd70\ufe0f Dropped official support for Python 3.8 and 3.9 - minimum version is now 3.10.</li> <li>CLI Flags Deprecation (--no-cache, --batch, --mode): Cleaned up the CLI by removing redundant flags for a simpler interface.</li> </ul>"},{"location":"whats-new/#curious-about-our-journey","title":"\ud83d\uddfa\ufe0f Curious About Our Journey?","text":"<p>For a complete list of all changes, fixes, and improvements across versions, check out our detailed Changelog - it's got all the technical details!</p>"},{"location":"getting-started/cli/","title":"Chunklet Command Line Interface (CLI): Your Chunking Powerhouse! \ud83d\ude80","text":"<p>Meet <code>chunklet</code>, your versatile CLI companion for all things text processing! From precise sentence splitting and smart chunking of any content to interactive visualization in your browser - we've got the tools to make your LLM workflows flow effortlessly. Whether you're processing documents, code, or plain text, chunklet adapts to your needs with RAG-ready precision.</p> <p><code>chunklet</code> vs <code>chunklet-py</code></p> <p>The CLI command is <code>chunklet</code> (kept for backward compatibility), while the Python package is named <code>chunklet-py</code> to avoid naming conflicts with other packages.</p> <p>Before we dive into the fun stuff, you can always check your <code>chunklet</code> version or get a quick help guide.:</p> <pre><code>chunklet --version\nchunklet --help\n</code></pre> <p>You can also get specific help for each command</p> <pre><code>chunklet split --help\nchunklet chunk --help\nchunklet visualize --help\n</code></pre>"},{"location":"getting-started/cli/#the-split-command-precision-sentence-segmentation","title":"The <code>split</code> Command: Precision Sentence Segmentation \u2702\ufe0f","text":"<p>Need to break down text into individual sentences with surgical precision? The <code>split</code> command is your go-to! It leverages <code>chunklet</code>'s powerful <code>SentenceSplitter</code> to give you clean, segmented sentences.</p>"},{"location":"getting-started/cli/#quick-facts-for-split","title":"Quick Facts for <code>split</code>","text":"<ul> <li>Operates on a raw text string or a single file (<code>--source</code>).</li> <li>Outputs sentences separated by newline characters.</li> <li>Perfect for preprocessing text before more complex chunking.</li> </ul> Flag Description Default <code>&lt;TEXT&gt;</code> The input text to split. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path to a single file to read input from. Cannot be a directory. None <code>--destination, -d &lt;PATH&gt;</code> Path to a single file to write the segmented sentences. If not provided, output goes to STDOUT. STDOUT <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). Use 'auto' for automatic detection. auto <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#scenarios-splitting-like-a-pro","title":"Scenarios: Splitting Like a Pro!","text":""},{"location":"getting-started/cli/#scenario-1-splitting-text-directly-and-multilingually","title":"Scenario 1: Splitting Text Directly (and Multilingually!)","text":"<p>Segment a direct text input containing multiple languages into individual sentences, leveraging automatic language detection.</p> <pre><code>chunklet split \"This is the first sentence. Here is the second sentence, in French. C'est la vie! \u00bfC\u00f3mo est\u00e1s?\" --lang auto\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-splitting-a-file-and-saving-the-output","title":"Scenario 2: Splitting a File and Saving the Output","text":"<p>Process a document and save its segmented sentences to a new file. Easy peasy!</p> <pre><code>chunklet split --source my_novel_chapter.txt --destination sentences.txt --lang en\n</code></pre>"},{"location":"getting-started/cli/#the-chunk-command-your-intelligent-chunking-workhorse","title":"The <code>chunk</code> Command: Your Intelligent Chunking Workhorse!","text":"<p>The <code>chunk</code> command is where the real magic happens! It's your versatile tool for breaking down text, documents, and even code into RAG-ready chunks. The \"flavor\" of chunking (plain text, document, or code) is determined by the flags you provide.</p>"},{"location":"getting-started/cli/#key-flags-for-chunk-the-essentials","title":"Key Flags for <code>chunk</code> (The Essentials!)","text":"Flag Description Default <code>&lt;TEXT&gt;</code> The input text to chunk. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path(s) to one or more files or directories to read input from. Repeat for multiple sources (e.g., <code>-s file1.txt -s dir/</code>). None <code>--destination, -d &lt;PATH&gt;</code> Path to a file (writes JSON for <code>.json</code> extensions or existing files) or directory (writes separate files) to write the chunks. If a non-JSON file exists, a warning is shown and JSON is written. If not provided, output goes to STDOUT. STDOUT <code>--max-tokens</code> Maximum number of tokens per chunk. Applies to all chunking strategies. (Must be &gt;= 12) None <code>--max-sentences</code> Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (Must be &gt;= 1) None <code>--max-section-breaks</code> Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (Must be &gt;= 1) None <code>--overlap-percent</code> Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. 20.0 <code>--offset</code> Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. 0 <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto) auto <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#general-text-document-chunking-default-or-with-doc","title":"General Text &amp; Document Chunking (Default or with <code>--doc</code>) \ud83d\udcc4","text":"<p>This is your bread-and-butter chunking for everyday text and diverse document types.</p> <ul> <li>Default Behavior: If neither <code>--doc</code> nor <code>--code</code> is specified, <code>chunklet</code> uses the PlainTextChunker for direct text input. The <code>PlainTextChunker</code> is designed to transform unruly text into perfectly sized, context-aware chunks.</li> <li>Document Power-Up: Activate the DocumentChunker with the <code>--doc</code> flag to process <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, and <code>.rtf</code> files! It intelligently extracts text and then applies the same robust chunking logic.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-document-power-up","title":"Key Flags for Document Power-Up","text":"Flag Description Default <code>--doc</code> Activate the <code>DocumentChunker</code> for multi-format file processing. False <code>--n-jobs</code> Number of parallel jobs for batch processing. (None uses all available cores) None <code>--on-errors</code> How to handle errors during batch processing: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise"},{"location":"getting-started/cli/#scenarios-text-document-chunking-in-action","title":"Scenarios: Text &amp; Document Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-basic-text-chunking-with-token-limits-and-overlap","title":"Scenario 1: Basic Text Chunking with Token Limits and Overlap","text":"<p>Chunk a long text string into segments, ensuring no chunk exceeds 200 tokens, with a healthy 15% overlap for context.</p> <pre><code>chunklet chunk \"The quick brown fox jumps over the lazy dog. This is the first sentence. The second sentence is a bit longer. And this is the third one. Finally, the fourth sentence concludes our example. The last sentence is here to finish the text.\"\n  --max-tokens 200 \\\n  --overlap-percent 15\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-chunking-a-pdf-document-with-sentence-and-section-break-limits","title":"Scenario 2: Chunking a PDF Document with Sentence and Section Break Limits","text":"<p>Process a PDF document, ensuring chunks are no more than 10 sentences or 2 section breaks, and save the output to a file.</p> <pre><code>chunklet chunk --doc --source my_report.pdf \\\n  --max-sentences 10 \\\n  --max-section-breaks 2 \\\n  --destination processed_report_chunks.txt\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-batch-processing-a-directory-of-documents-with-error-handling","title":"Scenario 3: Batch Processing a Directory of Documents (with Error Handling!)","text":"<p>Process all supported documents within a directory, saving the chunks to a new folder. If any file causes an error, <code>chunklet</code> will gracefully skip it and continue!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/my/project_docs \\\n  --destination ./processed_chunks \\\n  --n-jobs 4 \\\n  --on-errors skip \\\n  --max-tokens 1024 \\\n  --metadata # Don't forget your metadata!\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-a-text-file-with-a-specific-language-and-metadata","title":"Scenario 4: Chunking a Text File with a Specific Language and Metadata","text":"<p>Chunk a French text file, limiting by tokens, and include all the juicy metadata for later analysis.</p> <pre><code>chunklet chunk --source french_article.txt \\\n  --lang fr \\\n  --max-tokens 300 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#code-chunking-with-code","title":"Code Chunking (with <code>--code</code>) \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>For the developers, by the developers! The CodeChunker is a language-agnostic wizard that breaks your source code into semantically meaningful blocks (functions, classes, etc.). Activate it with the <code>--code</code> flag.</p> <ul> <li>Heads Up! This mode is primarily token-based. <code>--max-sentences</code>, <code>--max-section-breaks</code>, and <code>--overlap-percent</code> are generally ignored here, as code structure takes precedence.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-code-chunking","title":"Key Flags for Code Chunking","text":"Flag Description Default <code>--code</code> Activate the <code>CodeChunker</code> for structurally-aware code segmentation. False <code>--max-lines</code> Maximum number of lines per chunk. (Must be &gt;= 5) None <code>--max-functions</code> Maximum number of functions per chunk. (Must be &gt;= 1) None <code>--docstring-mode</code> Docstring processing strategy: <code>summary</code> (first line), <code>all</code>, or <code>excluded</code>. all <code>--strict</code> If <code>True</code>, raise an error when structural blocks exceed <code>--max-tokens</code>. If <code>False</code>, split oversized blocks. True <code>--include-comments</code> Include comments in output chunks. True"},{"location":"getting-started/cli/#scenarios-code-chunking-in-action","title":"Scenarios: Code Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-chunking-a-single-python-file-excluding-comments","title":"Scenario 1: Chunking a Single Python File, Excluding Comments","text":"<p>Get a clean, comment-free view of your code's structure. Perfect for quick reviews!</p> <pre><code>chunklet chunk --code --source my_script.py \\\n  --max-tokens 512 \\\n  --include-comments False\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-chunking-a-codebase-allowing-oversized-blocks","title":"Scenario 2: Batch Chunking a Codebase, Allowing Oversized Blocks","text":"<p>Process an entire code repository, letting <code>chunklet</code> split any functions or classes that are just too long, and save everything to a dedicated folder.</p> <pre><code>chunklet chunk --code \\\n  --source ./my_awesome_repo \\\n  --destination ./code_chunks \\\n  --max-tokens 1024 \\\n  --strict False \\\n  --n-jobs 8 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-extracting-function-summaries-docstring-mode-summary","title":"Scenario 3: Extracting Function Summaries (Docstring Mode: Summary)","text":"<p>Focus on the \"what\" of your functions by only including the first line of their docstrings.</p> <pre><code>chunklet chunk --code --source utils.py \\\n  --docstring-mode summary \\\n  --max-functions 1 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-by-lines-and-functions-for-granular-control","title":"Scenario 4: Chunking by Lines and Functions for Granular Control","text":"<p>For super-fine-grained control, chunk a file by both maximum lines and maximum functions per chunk.</p> <pre><code>chunklet chunk --code --source main.go \\\n  --max-lines 100 \\\n  --max-functions 2 \\\n  --max-tokens 700\n</code></pre>"},{"location":"getting-started/cli/#advanced-system-hooks","title":"\ud83d\udee0\ufe0f Advanced System Hooks","text":"<p>These flags are your secret weapons for scaling up operations, integrating with external tools, and getting the most out of your chunked data. They apply to the <code>chunk</code> command.</p>"},{"location":"getting-started/cli/#system-hook-flags","title":"System Hook Flags","text":"Flag Description Default <code>--tokenizer-command</code> A shell command string for token counting. It must take text via STDIN and output the integer count via STDOUT. None <code>--n-jobs</code> Number of parallel processes to use during batch operations. (None uses all available CPU cores) None <code>--on-errors</code> Defines batch error handling: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for debugging or process detail. False"},{"location":"getting-started/cli/#scenarios-unleashing-advanced-power","title":"Scenarios: Unleashing Advanced Power!","text":""},{"location":"getting-started/cli/#scenario-1-verbose-debugging-for-a-single-file","title":"Scenario 1: Verbose Debugging for a Single File","text":"<p>When things get tricky, crank up the verbosity to see exactly what <code>chunklet</code> is doing under the hood while chunking a specific file.</p> <pre><code>chunklet chunk --source problematic_file.txt \\\n  --max-tokens 100 \\\n  --verbose\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-processing-with-parallelism-and-error-skipping","title":"Scenario 2: Batch Processing with Parallelism and Error Skipping","text":"<p>Process a large collection of diverse documents, leveraging all your CPU cores, and gracefully skip any problematic files without halting the entire operation. Plus, get all the metadata!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/massive_document_archive \\\n  --destination ./final_chunks \\\n  --n-jobs -1 # Use all available cores!\n  --on-errors skip \\\n  --max-tokens 512 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-processing-multiple-specific-files-with-advanced-hooks","title":"Scenario 3: Processing Multiple Specific Files with Advanced Hooks","text":"<p>Process a selection of individual files, explicitly listing each one, and apply advanced chunking parameters. This demonstrates how to handle a non-directory batch of files, ensuring each is processed with metadata and error handling.</p> <pre><code>chunklet chunk --doc \\\n  --source my_document.pdf \\\n  --source another_report.docx \\\n  --source plain_text_notes.txt \\\n  --destination ./processed_specific_files \\\n  --max-tokens 700 \\\n  --metadata \\\n  --on-errors skip\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-custom-token-counting-with-an-external-script","title":"Scenario 4: Custom Token Counting with an External Script","text":"<p>Align <code>chunklet</code>'s chunk sizes perfectly with your LLM's token limits using any external tokenizer you can imagine!</p> <p>Create your external script (e.g., <code>my_llm_tokenizer.py</code>):</p> <pre><code># my_llm_tokenizer.py\nimport sys\nimport tiktoken # Or your LLM's specific tokenizer library\n\n# Read text from stdin\ntext = sys.stdin.read()\n\n# Replace with your actual token counting logic (e.g., for OpenAI's GPT models)\nencoding = tiktoken.encoding_for_model(\"gpt-4\")\ntoken_count = len(encoding.encode(text))\n\nprint(token_count) # Must print only the integer count\n</code></pre> <p>Now, run <code>chunklet</code> with your custom tokenizer:</p> <pre><code>chunklet chunk \\\n  --text \"This is a super important piece of text that needs precise token counting for my large language model.\"\n  --max-tokens 50 \\\n  --tokenizer-command \"python ./my_llm_tokenizer.py\" \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#scenario-5-saving-chunks-as-json-with-metadata","title":"Scenario 5: Saving Chunks as JSON with Metadata","text":"<p>Save processed chunks directly as a JSON file for easy parsing and integration:</p> <pre><code>chunklet chunk --source document.pdf --doc --destination chunks.json --metadata\n</code></pre>"},{"location":"getting-started/cli/#diving-deeper-into-metadata","title":"Diving Deeper into Metadata","text":"<p>Want to know exactly what kind of rich context <code>chunklet</code> attaches to your chunks? From source paths and character spans to document-specific properties and code AST details.</p> <p>\ud83d\udc49 Head over to the Metadata in Chunklet-py guide to unlock all its secrets!</p>"},{"location":"getting-started/cli/#the-visualize-command-your-interactive-chunk-playground","title":"The <code>visualize</code> Command: Your Interactive Chunk Playground! \ud83c\udfae","text":"<p>Ready to see your chunking in action with a beautiful web interface? The <code>visualize</code> command launches Chunklet's interactive web visualizer - perfect for experimenting with parameters, seeing real-time results, and fine-tuning your chunking strategies!</p> <p>Want programmatic control?</p> <p>For code-based usage and detailed technical information, check out the Text Chunk Visualizer documentation.</p> <p>This command starts a local web server that gives you: - Live parameter tuning - Adjust chunking settings and see results instantly - Visual chunk exploration - See exactly how your text gets divided - Multiple chunking modes - Try plain text, document, and code chunking all in one place - Custom tokenizers - Plug in your own token counting for precise control</p>"},{"location":"getting-started/cli/#key-flags-for-visualize","title":"Key Flags for <code>visualize</code>","text":"Flag Description Default <code>--host</code> Host IP to bind the server (use <code>0.0.0.0</code> for network access) 127.0.0.1 <code>--port, -p</code> Port number for the server 8000 <code>--tokenizer-command</code> Shell command for custom token counting None <code>--headless</code> Run without opening browser automatically False"},{"location":"getting-started/cli/#getting-started-with-visualization","title":"Getting Started with Visualization! \ud83d\udda5","text":""},{"location":"getting-started/cli/#scenario-1-basic-visualizer-launch","title":"Scenario 1: Basic Visualizer Launch","text":"<p>Fire up the visualizer on the default port and let it open your browser automatically:</p> <pre><code>chunklet visualize\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-custom-port-and-host","title":"Scenario 2: Custom Port and Host","text":"<p>Run on a specific port and host (great for accessing from other devices):</p> <pre><code>chunklet visualize --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"getting-started/cli/#headless-cli","title":"Scenario 3: Headless Mode with Custom Tokenizer","text":"<p>Run in the background with your own token counting script:</p> <pre><code>chunklet visualize --headless --tokenizer-command \"python my_tokenizer.py\"\n</code></pre> <p>The visualizer will show you the URL to access it in your browser. Press <code>Ctrl+C</code> to stop the server when you're done!</p> <p>REST API for Headless Automation! \ud83e\udd16</p> <p>When running in headless mode, you can use the visualizer's REST API to programmatically upload files, chunk content, and retrieve results without any web interface! Perfect for automation scripts, CI/CD pipelines, or integrating chunking into your applications.</p> <p>See the Headless/REST API Usage section for complete examples of programmatic file processing.</p> <p>Pro Visualization Tips</p> <ul> <li>Use <code>--headless false</code> (or just omit it) to auto-open your browser</li> <li>Try different ports if 8000 is already in use</li> <li>Experiment with different chunking modes - text, document, and code all in one interface!</li> </ul> API Reference <p>For a deep dive into the <code>chunklet</code> CLI, its commands, and all the nitty-gritty details, check out the full API documentation</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Ready to get Chunklet-py up and running? Fantastic! This guide will walk you through the installation process, making it as smooth as possible.</p> <p>Requirements</p> <p>Chunklet-py requires Python 3.10 or newer. We recommend using Python 3.11+ for the best experience.</p> <p>Package Name Change</p> <p>Chunklet-py was previously named <code>chunklet</code>. The old <code>chunklet</code> package is no longer maintained. When installing, make sure to use <code>chunklet-py</code> (with the hyphen) to get the latest version.</p>"},{"location":"getting-started/installation/#the-easy-way","title":"The Easy Way","text":"<p>The most straightforward method to install Chunklet-py is by using <code>pip</code>:</p> <pre><code># Install and verify version\npip install chunklet-py\nchunklet --version\n</code></pre> <p>And that's all there is to it! You're now ready to start using Chunklet-py.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Chunklet-py offers optional dependencies to unlock additional functionalities, such as document processing or code chunking. You can install these extras using the following syntax:</p> <ul> <li>Document Processing: For handling <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, and other document formats:     <pre><code>pip install \"chunklet-py[document]\"\n</code></pre></li> <li>Code Chunking: For advanced code analysis and chunking features:     <pre><code>pip install \"chunklet-py[code]\"\n</code></pre></li> <li>Visualization: For the interactive web-based chunk visualizer:     <pre><code>pip install \"chunklet-py[visualization]\"\n</code></pre></li> <li>All Extras: To install all optional dependencies:     <pre><code>pip install \"chunklet-py[all]\"\n</code></pre></li> </ul>"},{"location":"getting-started/installation/#the-alternative-way","title":"The Alternative Way","text":"<p>For those who prefer to build from source, you can clone the repository and install it manually. This method allows for direct modification of the source code and installation of all optional features:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install .[all]\n</code></pre> <p>But why would you want to do that? The easy way is so much easier.</p>"},{"location":"getting-started/installation/#contributing-to-chunklet-py","title":"Contributing to Chunklet-py","text":"<p>Interested in helping make Chunklet-py even better? That's fantastic! Before you dive in, please take a moment to review our Contributing Guide. Here's how you can set up your development environment:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\n# For basic development (testing, linting)\npip install -e \".[dev]\"\n# For documentation development\npip install -e \".[docs]\"\n# For comprehensive development (including all optional features)\npip install -e \".[dev-all]\"\n</code></pre> <p>These commands install Chunklet-py in \"editable\" mode, ensuring that any changes you make to the source code are immediately reflected. The <code>[dev]</code>, <code>[docs]</code>, and <code>[dev-all]</code> options include the necessary dependencies for specific development tasks.</p> <p>Now, go forth and code! And remember, good developers always write tests. (Even in a Python project, we appreciate all forms of excellent code examples!)</p>"},{"location":"getting-started/metadata/","title":"Metadata in Chunklet-py: Your Chunk's Story \ud83d\udcd6","text":"<p>Ever wondered where your chunks come from and what makes them tick? \ud83e\udd14 Chunklet-py's metadata system tells the whole story! Each chunk comes with rich contextual information about its origin, location, and characteristics. Think of metadata as your chunk's detailed biography - the who, what, when, and where of your text.</p> <p>Every chunk is wrapped in a handy <code>Box</code> object with a <code>metadata</code> attribute. This metadata dictionary is your treasure trove of chunk insights. Access it easily with dot notation (<code>chunk.metadata</code>) or dictionary-style (<code>chunk[\"metadata\"]</code>) - your choice!</p>"},{"location":"getting-started/metadata/#common-metadata","title":"Common Metadata: The Essentials \ud83d\udccb","text":"<p>No matter which chunker you use, every chunk includes these fundamental metadata fields. Think of them as your chunk's basic information - the essentials you need to know.</p> <ul> <li><code>chunk_num</code> (int): Your chunk's sequential ID number within each source - perfect for keeping things organized</li> <li><code>span</code> (tuple[int, int]): Character position coordinates showing exactly where this chunk sits in the original text</li> <li><code>source</code> (str): Where did this chunk come from? (The origin story!)<ul> <li>File processing: Absolute path to the file (for DocumentChunker or CodeChunker)</li> <li>CLI text input: <code>\"stdin\"</code> (because it came from standard input)</li> <li>PlainTextChunker strings: Only included if you provide it via <code>base_metadata</code> parameter</li> <li>CodeChunker edge cases: Might be <code>\"N/A\"</code> if the source can't be determined</li> </ul> </li> </ul>"},{"location":"getting-started/metadata/#plaintextchunker-metadata","title":"PlainTextChunker Metadata: Simple &amp; Clean \ud83d\udcc4","text":"<p>The <code>PlainTextChunker</code> keeps things straightforward and clean. Your chunks include the essential Common Metadata fields (<code>chunk_num</code> and <code>span</code>). The <code>source</code> field only shows up if you explicitly provide it via the <code>base_metadata</code> parameter.</p> <p>No frills, just the basics - perfect when you want clean, minimal metadata without any extra baggage.</p>"},{"location":"getting-started/metadata/#documentchunker-metadata","title":"DocumentChunker Metadata: Rich &amp; Detailed \ud83d\udcda","text":"<p>The <code>DocumentChunker</code> provides comprehensive metadata beyond the basics. It extracts rich, file-specific information from your documents - revealing detailed insights about each file's properties and history.</p> <p>Universal Fields (for multi-section docs): *   <code>section_count</code> (int): Total number of sections in the document (pages, chapters, etc.) *   <code>curr_section</code> (int): Which section this chunk belongs to</p> <p>File-Type Specific Information:</p> <ul> <li>PDF Files: Includes <code>title</code>, <code>author</code>, <code>creator</code>, <code>producer</code>, <code>publisher</code>, <code>created</code>, <code>modified</code>, and <code>page_count</code> fields (powered by pdfminer.six)</li> <li>EPUB Files: Dublin Core metadata including <code>title</code>, <code>creator</code>, <code>contributor</code>, <code>publisher</code>, <code>date</code>, and <code>rights</code></li> <li>DOCX Files: Core properties like <code>title</code>, <code>author</code>, <code>publisher</code>, <code>last_modified_by</code>, <code>created</code>, <code>modified</code>, <code>rights</code>, and <code>version</code></li> </ul> <p>Safety First with Optional Fields!</p> <p>These metadata fields are optional - not every document fills them out. Use <code>chunk.metadata.get(\"author\")</code> instead of <code>chunk.metadata[\"author\"]</code> to avoid <code>KeyError</code>s when a field is missing.</p>"},{"location":"getting-started/metadata/#codechunker-metadata","title":"CodeChunker Metadata: Code Intelligence \ud83d\udcbb","text":"<p>The <code>CodeChunker</code> provides code-specific insights beyond basic metadata. It helps you understand the structural context of each chunk - perfect for tracking where your code elements originated! \ud83d\udd0d</p> <p>Code-Specific Information: *   <code>tree</code> (str): Abstract syntax tree representation showing structural relationships within the chunk *   <code>start_line</code> (int): Line number where this chunk begins in the original file *   <code>end_line</code> (int): Line number where this chunk ends in the original file</p> <p>Automatically included in every <code>Box</code> object when chunking code, helping you understand which functions, classes, or code blocks are in each chunk.</p>"},{"location":"getting-started/metadata/#cli-metadata-output-command-line-insights","title":"CLI Metadata Output: Command Line Insights \ud83d\udda5\ufe0f","text":"<p>The <code>chunklet</code> CLI adapts metadata output based on your input type and flags. Think of it as your CLI's helpful companion that provides just the right context!</p> <p>Metadata Control: The <code>--metadata</code> flag gives you control over what gets included. *   With <code>--metadata</code>: Your chunks come with their full context - metadata appears alongside content, either printed to stdout or saved in <code>.json</code> files with <code>--destination</code> *   Without <code>--metadata</code>: Just the chunk content - clean and simple when you want to focus purely on the text</p> <p>Metadata by Input Type:</p> <ul> <li>Direct Text Input (<code>chunklet chunk \"Your text...\"</code>): Uses <code>PlainTextChunker</code> with essential Common Metadata fields (<code>chunk_num</code>, <code>span</code>) and <code>source</code> set to <code>\"stdin\"</code></li> <li>Document Processing (<code>chunklet chunk --doc --source document.pdf</code>): <code>DocumentChunker</code> provides rich document metadata including Common Metadata plus file-specific details (PDF titles, EPUB creators, DOCX authors) as detailed in DocumentChunker Metadata</li> <li>Code Processing (<code>chunklet chunk --code --source code.py</code>): <code>CodeChunker</code> includes structural information with Common Metadata and code-specific fields like <code>tree</code>, <code>start_line</code>, <code>end_line</code> as described in CodeChunker Metadata</li> </ul> <p>The CLI automatically provides the most relevant metadata for your use case - making chunk analysis both powerful and intuitive. Smart and simple! \ud83c\udfaf</p>"},{"location":"getting-started/programmatic/","title":"Overview","text":"<p>Welcome to the programmatic interface! This is where you integrate Chunklet-py's intelligent text and code chunking capabilities directly into your Python applications. Whether you're building RAG pipelines, data processing workflows, or custom AI solutions, our flexible API has everything you need.</p> <ul> <li> <p> Sentence Splitter</p> <p>Precisely splits text into semantically meaningful sentences across 50+ languages with intelligent detection and complex structure handling.</p> <p>Essential for preparing clean text data for NLP tasks, LLMs, and any application that needs accurate sentence boundaries.</p> <p> Learn More</p> </li> <li> <p> Plain Text Chunker</p> <p>Transforms plain text into perfectly sized, context-aware chunks with flexible constraint-based chunking and intelligent overlap for optimal LLM and embedding performance.</p> <p>Perfect for RAG systems, document analysis, and any workflow that needs smart text segmentation with full control over chunk sizes.</p> <p> Learn More</p> </li> <li> <p> Document Chunker</p> <p>Work with diverse document formats including <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, <code>.rtf</code>, <code>.odt</code>, <code>.csv</code>, and <code>.xlsx</code>, transforming them into structured, chunked outputs.</p> <p>Handles the complexity of different file types so you can focus on building great applications without worrying about format compatibility.</p> <p> Learn More</p> </li> <li> <p> Code Chunker</p> <p>Intelligently chunks source code while preserving logical structure and context and maintaining code semantics across functions, classes, and modules.</p> <p>Language-agnostic and lightweight - ideal for code understanding and generation tasks, analysis, documentation, and AI model training.</p> <p> Learn More</p> </li> <li> <p> Text Chunk Visualizer</p> <p>Interactive web interface for real-time chunk visualization, parameter tuning, and exploring chunking results with live feedback.</p> <p>Perfect for experimenting with chunking strategies, comparing different settings, and understanding how your text gets processed.</p> <p> Learn More</p> </li> </ul> <p>Ready to dive in? Click any card above to explore the chunker that matches your mission! Let's turn that text into perfectly chunked gold! \u2728</p>"},{"location":"getting-started/programmatic/code_chunker/","title":"Code Chunker","text":""},{"location":"getting-started/programmatic/code_chunker/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py[code]\n</code></pre> <p>This installs all the code processing dependencies needed for language-agnostic code chunking! \ud83d\udcbb</p>"},{"location":"getting-started/programmatic/code_chunker/#code-chunker-your-code-intelligence-sidekick","title":"Code Chunker: Your Code Intelligence Sidekick!","text":"<p>Ever stared at a massive codebase feeling like you're decoding ancient hieroglyphs? The <code>CodeChunker</code> is your trusty code companion that transforms tangled functions and classes into clean, understandable chunks that actually make sense!</p> <p>Forget basic regex hacks! This language-agnostic wizard uses clever patterns to identify functions, classes, and logical blocks across Python, JavaScript, Java, C++, and more - no PhD required.</p> <p>Language-agnostic and lightweight - ideal for code understanding and generation tasks, analysis, documentation, and AI model training.</p>"},{"location":"getting-started/programmatic/code_chunker/#code-chunker-superpowers","title":"Code Chunker Superpowers! \u26a1","text":"<p>The <code>CodeChunker</code> comes packed with smart features for your coding adventures:</p> <ul> <li>Rule-Based and Language-Agnostic: Uses universal patterns to spot code blocks, working with tons of languages out of the box - Python, C++, Java, JavaScript, and more!</li> <li>Convention-Aware: Assumes your code follows standard formatting - no full language parsers needed for surprisingly accurate results!</li> <li>Structurally Neutral: Handles mixed-language code like a pro - SQL in Python? JavaScript in HTML? No problem, it treats them as part of the block.</li> <li>Flexible Constraint-Based Chunking: Ultimate control over code segmentation! Mix and match limits based on tokens, lines, or functions for perfect chunks.</li> <li>Annotation-Aware: Smart about comments and docstrings - uses them to better understand your code's structure.</li> <li>Flexible Source Input: Feed it code as strings, file paths, or <code>pathlib.Path</code> objects. File paths? It'll read them automatically!</li> <li>Strict Mode Control: By default protects structural blocks from being split (even if they exceed limits), throwing a <code>TokenLimitError</code>. Want more flexibility? Set <code>strict=False</code>.</li> </ul>"},{"location":"getting-started/programmatic/code_chunker/#code-constraints-your-chunking-control-panel","title":"Code Constraints: Your Chunking Control Panel! \ud83c\udf9b\ufe0f","text":"<p><code>CodeChunker</code> works primarily in structural mode, letting you set chunk boundaries based on code structure. Fine-tune your chunks with these constraint options:</p> Constraint Value Requirement Description <code>max_tokens</code> <code>int &gt;= 12</code> Token budget master! Code blocks exceeding this limit get split at smart structural boundaries. <code>max_lines</code> <code>int &gt;= 5</code> Line count commander! Perfect for managing chunks where line numbers often match logical code units. <code>max_functions</code> <code>int &gt;= 1</code> Function group guru! Keeps related functions together or splits them when you hit the limit. <p>Constraint Must-Have!</p> <p>You must specify at least one limit (<code>max_tokens</code>, <code>max_lines</code>, or <code>max_functions</code>) when using <code>chunk</code> or <code>batch_chunk</code>. Skip this and you'll get an <code>InvalidInputError</code> - rules are rules!</p> <p>The <code>CodeChunker</code> has two main methods: <code>chunk</code> for single code inputs and <code>batch_chunk</code> for processing multiple codes. <code>chunk</code> returns a list of <code>Box</code> objects, while <code>batch_chunk</code> returns a generator that yields a <code>Box</code> object for each chunk. Each <code>Box</code> has <code>content</code> (str) and <code>metadata</code> (dict). For detailed information about metadata structure and usage, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#single-run","title":"Single Run:","text":"<p>Let's see <code>CodeChunker</code> in action with a single code input. The flexible <code>source</code> parameter accepts:</p> <ul> <li>Raw code as a string</li> <li>File path as a string</li> <li><code>pathlib.Path</code> object</li> </ul> <p>When you provide a file path, <code>CodeChunker</code> automatically handles reading the file for you!</p> <pre><code>from pathlib import Path\n\n# All of the following are valid:\nchunks_from_string = chunker.chunk(\"def my_func():\\n  return 1\")\nchunks_from_path_str = chunker.chunk(\"/path/to/your/code.py\")\nchunks_from_path_obj = chunker.chunk(Path(\"/path/to/your/code.py\"))\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-lines-line-count-control","title":"Chunking by Lines: Line Count Control! \ud83d\udccf","text":"<p>Ready to chunk code by line count? This gives you predictable, size-based chunks:</p> <pre><code>from chunklet.experimental.code_chunker import CodeChunker\n\nPYTHON_CODE = '''\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n'''\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk(\n    PYTHON_CODE,                \n    max_lines=10,               # (1)!\n    include_comments=True,      # (2)!\n    docstring_mode=\"all\",       # (3)!\n    strict=False,               # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> <ol> <li>Sets the maximum number of lines per chunk. If a code block exceeds this limit, it will be split.</li> <li>Set to True to include comments in the output chunks. Defaults to True.</li> <li><code>docstring_mode=\"all\"</code> ensures that complete docstrings, with all their multi-line details, are preserved in the code chunks. Other options are <code>\"summary\"</code> to include only the first line, or <code>\"excluded\"</code> to remove them entirely. Default is \"all\".</li> <li>When <code>strict=False</code>, structural blocks (like functions or classes) that exceed the limit set will be split into smaller chunks. If <code>strict=True</code> (default), a <code>TokenLimitError</code> would be raised instead.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\n\nMetadata:\n    chunk_num: 1\n    tree: global\n    start_line: 1\n    end_line: 7\n    span: (0, 38)\n    source: N/A\n\n--- Chunk 2 ---\nContent:\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n\nMetadata:\n    chunk_num: 2\n    tree: global\n    \u2514\u2500 class Calculator\n    start_line: 8\n    end_line: 14\n    span: (38, 192)\n    source: N/A\n\n--- Chunk 3 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n\nMetadata:\n    chunk_num: 3\n    tree: global\n    \u2514\u2500 class Calculator\n       \u2514\u2500 def add(\n    start_line: 15\n    end_line: 23\n    span: (192, 444)\n    source: N/A\n\n--- Chunk 4 ---\nContent:\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\n\nMetadata:\n    chunk_num: 4\n    tree: global\n    \u251c\u2500 class Calculator\n    \u2502  \u2514\u2500 def multiply(\n    \u2514\u2500 def standalone_function(\n    start_line: 24\n    end_line: 30\n    span: (444, 603)\n    source: N/A\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>CodeChunker</code>: <pre><code>chunker = CodeChunker(verbose=True)\n</code></pre></p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-tokens-token-budget-master","title":"Chunking by Tokens: Token Budget Master! \ud83e\ude99","text":"<p>Here's how you can use <code>CodeChunker</code> to chunk code by the number of tokens:</p> <pre><code># Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk(\n    PYTHON_CODE,                \n    max_tokens=50,                        \n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n\nstart_line: 1\nend_line: 14\nspan: (0, 192)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u251c\u2500 def add(\n   \u2514\u2500 def multiply(\n\nstart_line: 15\nend_line: 27\nspan: (192, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-functions-function-group-guru","title":"Chunking by Functions: Function Group Guru! \ud83d\udc65","text":"<p>This constraint is useful when you want to ensure that each chunk contains a specific number of functions, helping to maintain logical code units.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_functions=1,\n    include_comments=False,\n)\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def add(\n\nstart_line: 1\nend_line: 23\nspan: (0, 444)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def multiply(self, x, y):\n\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def multiply(\n\nstart_line: 24\nend_line: 27\nspan: (444, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#combining-multiple-constraints-mix-and-match-magic","title":"Combining Multiple Constraints: Mix and Match Magic! \ud83c\udfad","text":"<p>The real power of <code>CodeChunker</code> comes from combining multiple constraints. This allows for highly specific and granular control over how your code is chunked. Here are a few examples of how you can combine different constraints.</p>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-and-tokens","title":"By Lines and Tokens","text":"<p>This is useful when you want to limit by both the number of lines and the overall token count, whichever is reached first.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=5,\n    max_tokens=50\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-and-functions","title":"By Lines and Functions","text":"<p>This combination is great for ensuring that chunks don't span across too many functions while also keeping the line count in check.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=10,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-tokens-and-functions","title":"By Tokens and Functions","text":"<p>A powerful combination for structured code where you want to respect function boundaries while adhering to a strict token budget.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_tokens=100,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-tokens-and-functions","title":"By Lines, Tokens, and Functions","text":"<p>For the ultimate level of control, you can combine all three constraints. The chunking will stop as soon as any of the three limits is reached.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=8,\n    max_tokens=150,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#batch-run-processing-multiple-code-files-like-a-pro","title":"Batch Run: Processing Multiple Code Files Like a Pro! \ud83d\udcda","text":"<p>While <code>chunk</code> is perfect for single code inputs, <code>batch_chunk</code> is your power player for processing multiple code files in parallel. It uses a memory-friendly generator so you can handle massive codebases with ease.</p> <p>Given we have the following code snippets saved as individual files in a <code>code_examples</code> directory:</p>"},{"location":"getting-started/programmatic/code_chunker/#cpp_calculatorcpp","title":"cpp_calculator.cpp","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n\n// Function 1: Simple greeting\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\n// Function 2: Logic block\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1; // Error code\n    }\n    int result = a + b;\n    return result;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#javadataprocessorjava","title":"JavaDataProcessor.java","text":"<pre><code>package com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n    // Constructor\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n    // Method 1: Getter\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n    // Method 2: Core processing logic\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n        // Assume processing logic here\n        return true;\n    }\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#js_utilsjs","title":"js_utils.js","text":"<pre><code>// Utility function\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n// Main function with control flow\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n    // Loop structure\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#go_configgo","title":"go_config.go","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\n// Struct definition\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n// Function 1: Factory function\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n// Function 2: Method on the struct\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\\\n\", c.Timeout, c.Retries)\n}\n</code></pre> <p>We can process them all at once by providing a list of paths to the <code>batch_chunk</code> method. Assuming these files are saved in a <code>code_examples</code> directory:</p> <pre><code>from chunklet.code_chunker import CodeChunker\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\n# Initialize the chunker\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nsources = [\n    \"code_examples/cpp_calculator.cpp\",\n    \"code_examples/JavaDataProcessor.java\",\n    \"code_examples/js_utils.js\",\n    \"code_examples/go_config.go\",\n]\n\nchunks = chunker.batch_chunk(\n    sources=sources,\n    max_tokens=50,\n    include_comments=False,\n    n_jobs=2,               # (1)!\n    on_errors=\"raise\",      # (2)!\n    show_progress=True,     # (3)!\n)\n\n# Output the results\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content.strip()}\\n\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"  {k}: {v}\")\n    print()\n</code></pre> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to view output <pre><code>Chunking ...:   0%|          | 0/4 [00:00, ?it/s]\n--- Chunk 1 ---\nContent:\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1;\n    }\n    int result = a + b;\n    return result;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n  start_line: 1\n  end_line: 17\n  span: (0, 329)\n  source: code_examples/cpp_calculator.cpp\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 2 ---\nContent:\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u2514\u2500 function processArray(\n  start_line: 1\n  end_line: 19\n  span: (0, 372)\n  source: code_examples/js_utils.js\n\n--- Chunk 3 ---\nContent:\npackage com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n\n        return true;\n    }\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package com\n\u2514\u2500 public class DataProcessor\n   \u251c\u2500 public DataProcessor(\n   \u251c\u2500 public String getPath(\n   \u2514\u2500 public boolean process(\n\n  start_line: 1\n  end_line: 25\n  span: (0, 500)\n  source: code_examples/JavaDataProcessor.java\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 4 ---\nContent:\npackage main\n\nimport (\n    \"fmt\"\n)\n\n\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\n\", c.Timeout, c.Retries)\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package main\n\u251c\u2500 type Config\n\u2514\u2500 func NewConfig(\n\n  start_line: 1\n  end_line: 26\n  span: (0, 382)\n  source: code_examples/go_config.go\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00, 19.71it/s]\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#separator-keeping-your-code-batches-organized","title":"Separator: Keeping Your Code Batches Organized! \ud83d\udccb","text":"<p>The <code>separator</code> parameter lets you add a custom marker that gets yielded after all chunks from a single code file are processed. Super handy for batch processing when you want to clearly separate chunks from different source files.</p> <p>note</p> <p><code>None</code> cannot be used as a separator.</p> <pre><code>from chunklet.code_chunker import CodeChunker\nfrom more_itertools import split_at\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nSIMPLE_SOURCES = [\n    # Python: Simple Function Definition Boundary\n    '''\ndef greet_user(name):\n    \"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n''',\n    # C#: Simple Method and Class Boundary\n    '''\npublic class Utility\n{\n    // C# Method\n    public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n'''\n]\n\nchunker = CodeChunker(token_counter=simple_token_counter)\ncustom_separator = \"---END_OF_SOURCE---\"\n\nchunks_with_separators = chunker.batch_chunk(\n    sources=SIMPLE_SOURCES,\n    max_tokens=20,\n    separator=custom_separator,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, code_chunks in enumerate(chunk_groups):\n    if code_chunks: # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in code_chunks:\n            print(f\"Content:\\n {chunk.content}\\n\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>Chunking ...:   0%|          | 0/2 [00:00, ?it/s]\n--- Chunks for Document 1 ---\nContent:\ndef greet_user(name):\n\"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 def greet_user(\\n', 'start_line': 1, 'end_line': 5, 'span': (0, 124), 'source': 'N/A'}\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 1/2 [00:00,  9.48it/s]\n--- Chunks for Document 2 ---\nContent:\npublic class Utility\n{\n    // C# Method\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 public class Utility\\n', 'start_line': 1, 'end_line': 4, 'span': (0, 41), 'source': 'N/A'}\nContent:\n     public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n\nMetadata: {'chunk_num': 2, 'tree': 'global\\n\u2514\u2500 public class Utility\\n   \u2514\u2500 public int Add(\\n', 'start_line': 5, 'end_line': 10, 'span': (41, 133), 'source': 'N/A'}\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00,  1.92it/s]\n</code></pre> <p>What are the limitations of CodeChunker?</p> <p>While powerful, <code>CodeChunker</code> isn't magic! It assumes your code is reasonably well-behaved (syntactically conventional). Highly obfuscated, minified, or macro-generated sources might give it a headache. Also, nested docstrings or comment blocks can be a bit tricky for it to handle perfectly.</p>"},{"location":"getting-started/programmatic/code_chunker/#inspiration-the-code-behind-the-magic","title":"Inspiration: The Code Behind the Magic! \u2728","text":"<p>The <code>CodeChunker</code> draws inspiration from various projects and concepts in the field of code analysis and segmentation. These influences have shaped its design principles and capabilities:</p> <ul> <li>code_chunker by Camel AI</li> <li>code_chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> API Reference <p>For complete technical details on the <code>CodeChunker</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/document_chunker/","title":"Document Chunker","text":"<p>Psst... Read <code>PlainTextChunker</code> First!</p> <p>Think of <code>DocumentChunker</code> as <code>PlainTextChunker</code>'s upgrade that adds document processing superpowers! While <code>DocumentChunker</code> handles all sorts of fancy file formats, the core chunking intelligence (splitting by sentences, tokens, and sections) still lives in <code>PlainTextChunker</code>.</p> <p>Before diving in here, get cozy with the PlainTextChunker documentation first. It's the foundation for everything! We'll focus on the document-specific upgrades here.</p>"},{"location":"getting-started/programmatic/document_chunker/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py[document]\n</code></pre> <p>This installs all the document processing dependencies needed to handle PDFs, DOCX, EPUB, ODT, Excel, and more! \ud83d\udcda</p>"},{"location":"getting-started/programmatic/document_chunker/#taming-your-documents-format-freedom-unleashed","title":"Taming Your Documents: Format Freedom Unleashed! \ud83d\udcc4","text":"<p>Tired of juggling different tools for every file type you encounter? The <code>DocumentChunker</code> is your universal document wrangler that speaks every format under the sun. From corporate DOCX files to academic PDFs, from EPUB novels to everything in between - it handles the complexity of different file types so you can focus on building great applications without worrying about format compatibility.</p> <p>Think of <code>DocumentChunker</code> as the master conductor of your document orchestra! It smartly detects file types, calls in the right specialists to extract and convert text (often to clean Markdown), and then hands off the baton to <code>PlainTextChunker</code> for the final segmentation symphony.</p> <p>Ready to liberate your documents from format chaos? Let's make some magic happen!</p>"},{"location":"getting-started/programmatic/document_chunker/#whats-in-the-magic-bag","title":"What's in the Magic Bag?","text":"<p>The <code>DocumentChunker</code> comes packed with superpowers that make document processing feel like child's play:</p> <ul> <li>Multi-Format Maestro: From corporate DOCX boardrooms to academic PDF libraries, this chunker speaks every file language fluently! Handles <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, <code>.rtf</code>, <code>.odt</code>, <code>.csv</code>, and <code>.xlsx</code> files like a pro. \ud83c\udf0d</li> <li>Metadata Magician: Not just text - it automatically enriches your chunks with valuable metadata like source file paths and PDF page numbers. Your chunks come with bonus context! \ud83d\udcca</li> <li>Bulk Processing Beast: Got a mountain of documents to conquer? No problem! This beast efficiently processes multiple documents in parallel. \ud83d\udcda\u26a1</li> <li>Pluggable Processor Power: Have a mysterious file format that's one-of-a-kind? Plug in your own custom processors - <code>DocumentChunker</code> is ready for any challenge you throw at it! \ud83d\udd0c\ud83d\udee0\ufe0f</li> </ul> <p>The <code>DocumentChunker</code> has two main methods: <code>chunk</code> for single file adventures and <code>batch_chunk</code> for processing multiple files like a boss. <code>chunk</code> returns a list of handy <code>Box</code> objects, while <code>batch_chunk</code> is a memory-friendly generator that yields chunks one by one. Each <code>Box</code> comes with <code>content</code> (the actual text) and <code>metadata</code> (all the juicy details about your document).</p> <p>Special Handling for Streaming Processors</p> <p>Some processors work differently due to their streaming nature - they yield content page by page or in blocks rather than all at once. This means they require special care:</p> <p>Streaming processors (PDF, EPUB, DOCX, ODT): These beauties process content as they go, so they're designed for <code>batch_chunk</code> method. Using them with the regular <code>chunk</code> method will throw a <code>FileProcessingError</code> since <code>chunk</code> expects all content upfront.</p> <p>Regular processors work fine with both <code>chunk</code> and <code>batch_chunk</code> methods.</p> <p>These processors also add extra metadata magic! Check the Metadata guide for details.</p>"},{"location":"getting-started/programmatic/document_chunker/#single-file-showdown-lets-process-one-document","title":"Single File Showdown: Let's Process One Document! \ud83d\udcc4","text":"<p>The <code>DocumentChunker</code>'s <code>chunk</code> method shares most arguments with <code>PlainTextChunker.chunk</code>, but with a couple of key twists:</p> <ul> <li>First argument is <code>path</code> (file path string or <code>pathlib.Path</code> object) instead of raw <code>text</code></li> <li>No <code>base_metadata</code> parameter - document processors handle metadata automatically</li> </ul> <p>Just like its sibling, you must specify at least one limit (<code>max_sentences</code>, <code>max_tokens</code>, or <code>max_section_breaks</code>) or you'll get an <code>InvalidInputError</code>. Rules are rules! \ud83d\udccf</p> <p>Let's grab some sample content to play with:</p> <pre><code>The quick brown fox jumps over the lazy dog. This is the first sentence, and it's a classic.\nHere is the second sentence, which is a bit longer and more descriptive. And this is the third one, short and sweet.\nThe fourth sentence concludes our initial example, but we need more text to demonstrate the chunking effectively.\nLet's add a fifth sentence to make the text a bit more substantial. The sixth sentence will provide even more content for our test.\nThis is the seventh sentence, and we are still going. The eighth sentence is here to make the text even longer.\nFinally, the ninth sentence will be the last one for this example, making sure we have enough content to create multiple chunks.\n</code></pre> <p>Pop this into a file called <code>sample_text.txt</code>. Now let's see <code>DocumentChunker</code> in action:</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\n# Assuming sample_text.txt is in the same directory as your script\nfile_path = \"sample_text.txt\"\n\nchunker = DocumentChunker()\n\nchunks = chunker.chunk(\n    path=file_path,\n    lang=\"auto\",             # (1)!\n    max_sentences=4,         # (2)!\n    overlap_percent=0,       # (3)!\n    offset=0                 # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li><code>lang=\"auto\"</code> lets us detect language automatically. Super handy, but specifying a known language like <code>lang=\"en\"</code> can boost accuracy and speed.</li> <li><code>max_sentences=4</code> caps each chunk at 4 sentences max. Your content gets neatly portioned!</li> <li><code>overlap_percent=0</code> means zero overlap between chunks. By default, we add 20% overlap to keep context flowing smoothly.</li> <li><code>offset=0</code> starts us from the very beginning. (Zero-based indexing - because programmers love starting from zero!)</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 1, 'span': (0, 209)}\nContent: The quick brown fox jumps over the lazy dog.\nThis is the first sentence, and it's a classic.\nHere is the second sentence, which is a bit longer and more descriptive.\nAnd this is the third one, short and sweet.\n\n--- Chunk 2 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 2, 'span': (210, 509)}\nContent: The fourth sentence concludes our initial example, but we need more text to demonstrate the chunking effectively.\nLet's add a fifth sentence to make the text a bit more substantial.\nThe sixth sentence will provide even more content for our test.\nThis is the seventh sentence, and we are still going.\n\n--- Chunk 3 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 3, 'span': (510, 696)}\nContent: The eighth sentence is here to make the text even longer.\nFinally, the ninth sentence will be the last one for this example, making sure we have enough content to create multiple chunks.\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>DocumentChunker</code>: <pre><code>chunker = DocumentChunker(verbose=True)\n</code></pre></p> <p>Customizing the Continuation Marker</p> <p>You can customize the continuation marker, which is prepended to clauses that don't fit in the previous chunk. To do this, pass the <code>continuation_marker</code> parameter to the chunker's constructor.</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"[...]\")\n</code></pre> <p>If you don't want any continuation marker, you can set it to an empty string:</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"\")\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#batch-run-processing-multiple-documents-like-a-boss","title":"Batch Run: Processing Multiple Documents Like a Boss! \ud83d\udcda","text":"<p>While <code>chunk</code> is perfect for single documents, <code>batch_chunk</code> is your power tool for processing multiple documents in parallel. It returns a memory-friendly generator so you can handle massive document collections with ease.</p> <p>The <code>batch_chunk</code> method shares most arguments with <code>PlainTextChunker.batch_chunk</code> (like <code>lang</code>, <code>max_tokens</code>, <code>max_sentences</code>, etc.). The key differences:</p> <ul> <li>First argument is <code>paths</code> (list of file paths as strings or <code>pathlib.Path</code> objects) instead of raw <code>texts</code></li> <li>No <code>base_metadata</code> parameter - document processors handle metadata automatically</li> </ul> <p>For our example, we'll grab some sample files from the samples directory in the repo.</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\npaths = [\n    \"samples/Lorem Ipsum.docx\",\n    \"samples/What_is_rst.rst\",\n    \"samples/minimal.epub\",\n    \"samples/sample-pdf-a4-size.pdf\",\n]                                     # (1)!\n\nchunker = DocumentChunker(token_counter=word_counter) # (2)!\n\nchunks_generator = chunker.batch_chunk(\n    paths=paths,\n    overlap_percent=30,\n    max_sentences=12,\n    max_tokens=256,\n    max_section_breaks=1,\n    n_jobs=2,                    # (3)!\n    on_errors=\"raise\",           # (4)!\n    show_progress=False,         # (5)!\n)\n\nfor i, chunk in enumerate(chunks_generator):\n    if i == 10:                      # (6)!\n        break\n\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content}\\n\")\n    print(f\"Metadata:\")\n    for k, v in chunk.metadata.items():\n        print(f\" | {k}: {v}\")\n\n    print()\n\nchunks_generator.close()          # (7)!\nprint(\"\\nAnd so on...\")\n</code></pre> <ol> <li>If your files are saved elsewhere make sure to update that accoordingly.</li> <li>Initializes the <code>DocumentChunker</code> with a <code>token_counter</code> function. This is crucial when using <code>max_tokens</code> for chunking.</li> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> <li>We break the loop early to demonstrate the cleanup mechanism.</li> <li>Explicitly close the generator to ensure proper cleanup.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nContent:\nQuantum Aristoxeni ingenium consumptum videmus in musicis?\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nQuid nunc honeste dicit?\nTum Torquatus: Prorsus, inquit, assentior; Duo Reges: constructio interrete.\nIam in altera philosophiae parte.\nSed haec omittamus; Haec para/doca illi, nos admirabilia dicamus.\nNihil sane.\n**Expressa vero in iis aetatibus, quae iam confirmatae sunt.**\nSit sane ista voluptas.\nNon quam nostram quidem, inquit Pomponius iocans; An tu me de L.\nSed haec omittamus; Cave putes quicquam esse verius.\n[Image - 1]\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/Lorem Ipsum.docx\n author: train11\n last_modified_by: Microsoft Office User\n created: 2012-08-07 08:50:00+00:00\n modified: 2019-12-05 23:29:00+00:00\n section_count: 1\n curr_section: 1\n\n--- Chunk 2 ---\nContent:\nxml version=\"1.0\" encoding=\"utf-8\"?\nReStructuredText (rst): plain text markup\nReStructuredText (rst): plain text markup=====\n[The tiny table of contents](#top)\n* [1   What is reStructuredText?](#what-is-restructuredtext)\n* [2   What is it good for?](#what-is-it-good-for)\n* [3   Show me some formatting examples](#show-me-some-formatting-examples)\n* [4   Where can I learn more?](#where-can-i-learn-more)\n* [5   Show me some more stuff, please](#show-me-some-more-stuff-please)\n[1   What is reStructuredText?](#toc-entry-1)=====\nAn easy-to-read, what-you-see-is-what-you-get plaintext markup syntax\nand parser system, abbreviated *rst*.\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 1\n\n--- Chunk 3 ---\nContent:\nAn easy-to-read,\nwhat-you-see-is-what-you-get plaintext markup syntax\nand parser system,\nabbreviated *rst*.\nIn other words, using a simple\ntext editor, documents can be created which\n* are easy to read in text editor and\n* can be *automatically* converted to\n+ html and\n+ latex (and therefore pdf)\n[2   What is it good for?](#toc-entry-2)=====\nreStructuredText can be used, for example, to\n\nMetadata:\n chunk_num: 2\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 2\n\n--- Chunk 4 ---\nContent:\n... + latex (and therefore pdf)\n[2   What is it good for?](#toc-entry-2)=====\nreStructuredText can be used,\nfor example,\nto\n* write technical documentation (so that it can easily be offered as a\npdf file or a web page)\n* create html webpages without knowing html\n* to document source code\n[3   Show me some formatting examples](#toc-entry-3)=====\nYou can highlight text in *italics* or, to provide even more emphasis\nin **bold**.\n\nMetadata:\n chunk_num: 3\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 3\n\n--- Chunk 5 ---\nContent:\n... [3   Show me some formatting examples](#toc-entry-3)=====\nYou can highlight text in *italics* or,\nto provide even more emphasis\nin **bold**.\nOften, when describing computer code, we like to use a\nfixed space font to quote code snippets.\nWe can also include footnotes [[1]](#footnote-1).\nWe could include source code files\n(by specifying their name) which is useful when documenting code.\nWe\ncan also copy source code verbatim (i.e. include it in the rst\ndocument) like this:```\n\nMetadata:\n chunk_num: 4\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 4\n\n--- Chunk 6 ---\nContent:\n... which is useful when documenting code.\nWe\ncan also copy source code verbatim (i.e. include it in the rst\ndocument)\nlike this:```\nint main ( int argc, char *argv[] ) {\nprintf(\"Hello World\\n\");\nreturn 0;}```\nWe have already seen at itemised list in section [What is it good\nfor?\n](#what-is-it-good-for).\nEnumerated list and descriptive lists are supported as\n\nMetadata:\n chunk_num: 5\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 5\n\n--- Chunk 7 ---\nContent:\nWe have already seen at itemised list in section [What is it good\nfor?\n](#what-is-it-good-for).\nEnumerated list and descriptive lists are supported as\nwell.\nIt provides very good support for including html-links in a\nvariety of ways.\nAny section and subsections defined can be linked to,\nas well.\n[4   Where can I learn more?](#toc-entry-4)=====\nreStructuredText is described at\n&lt;http://docutils.sourceforge.net/rst.html&gt;.\n\nMetadata:\n chunk_num: 6\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 6\n\n--- Chunk 8 ---\nContent:\n... as well.\n[4   Where can I learn more?](#toc-entry-4)=====\nreStructuredText is described at\n&lt;http://docutils.sourceforge.net/rst.html&gt;.\nWe provide some geeky small\nprint in this footnote [[2]](#footnote-2).\n[5   Show me some more stuff, please](#toc-entry-5)=====\nWe can also include figures:\n![image.png](image.png)\nThe magnetisation in a small ferromagnetic disk.\nThe diametre is of the order of 120 nanometers and the material is Ni20Fe\n80.\nPng is a file format that is both acceptable for html pages as well as fo\nr (pdf)latex.\n\nMetadata:\n chunk_num: 7\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 7\n\n--- Chunk 9 ---\nContent:\n... ![image.png](image.png)\nThe magnetisation in a small ferromagnetic disk.\nThe diametre is of the order of 120 nanometers and the material is Ni20Fe\n80.\nPng is a file format that is both acceptable for html pages as well as fo\nr (pdf)latex.\n\n---\n|  |  |\n| --- | --- |\n| [[1]](#footnote-reference-1) | although there isn't much point of using\n a footnote here.|\n|  |  |\n| --- | --- |\n| [[2]](#footnote-reference-2) | Random facts:   * Emacs provides an rst\n mode * when converting rst to html, a style sheet can be provided (there\n is a similar feature for latex) * rst can also be converted into XML * th\ne recommended file extension for rst is .txt |\n\nMetadata:\n chunk_num: 8\n span: (2499, 3150)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 8\n\n--- Chunk 10 ---\nContent:\nTable of Contents=====\n1. [Chapter 1](chapter_1.xhtml)\n2. [Chapter 2](chapter_2.xhtml)\n3. [Copyright](copyright.xhtml)\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/minimal.epub\n title: Sample .epub Book\n creator: Thomas Hansen\n section_count: 4\n curr_section: 1\n\nAnd so on...\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p> <p>Token Counter Requirement</p> <p>When using the <code>max_tokens</code> constraint, a <code>token_counter</code> function is essential. This function, which you provide, should accept a string and return an integer representing its token count. Failing to provide a <code>token_counter</code> will result in a <code>MissingTokenCounterError</code>.</p> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/document_chunker/#separator-keeping-your-document-batches-organized","title":"Separator: Keeping Your Document Batches Organized! \ud83d\udccb","text":"<p>The <code>separator</code> parameter works the same way here as in <code>PlainTextChunker</code> - it yields a custom value after each document's chunks, perfect for keeping your batch processing tidy.</p> <p>For detailed examples and code, check out the PlainTextChunker separator docs - it's all the same functionality!</p>"},{"location":"getting-started/programmatic/document_chunker/#custom-processors-build-your-own-document-wizards","title":"Custom Processors: Build Your Own Document Wizards! \ud83d\udee0\ufe0f\ud83d\udd2e","text":"<p>Want to handle exotic file formats that <code>DocumentChunker</code> doesn't know about? Create your own custom processors! This lets you add specialized processing for any file type and prioritize your custom processors over the built-in ones.</p> <p>Global Registry Alert!</p> <p>Custom processors get registered globally - once you add one, it's available everywhere in your app. Watch out for side effects if you're registering processors across different parts of your codebase, especially in multi-threaded or long-running applications!</p> <p>To use a custom processor, you leverage the <code>@registry.register</code> decorator. This decorator allows you to register your function for one or more file extensions directly. Your custom processor function must accept a single <code>file_path</code> parameter (str) and return a <code>tuple[str | list[str], dict]</code> containing extracted text (or list of texts for multi-section documents) and a metadata dictionary.</p> <p>Custom Processor Rules</p> <ul> <li>Your function must accept exactly one required parameter (the file path)</li> <li>Optional parameters with defaults are totally fine</li> <li>File extensions must start with a dot (like <code>.json</code>, <code>.custom</code>)</li> <li>Lambda functions are not supported unless you provide a <code>name</code> parameter</li> <li>The metadata dictionary will be merged with common metadata (chunk_num, span, source)</li> <li>For multi-section documents, return a list of strings - each will be processed as a separate section</li> <li>If an error occurs during the document processing (e.g., an issue with the custom processor function), a <code>CallbackError</code> will be raised</li> </ul> <pre><code>import os\nimport re\nimport json\nimport tempfile\nfrom chunklet.document_chunker import DocumentChunker, CustomProcessorRegistry\n\n\nregistry = CustomProcessorRegistry()\n\n# Define a simple custom processor for .json files\n@registry.register(\".json\", name=\"MyJSONProcessor\")\ndef my_json_processor(file_path: str) -&gt; tuple[str, dict]:\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Assuming the json has a \"text\" field with paragraphs\n    text_content = \"\\n\".join(data.get(\"text\", []))\n    metadata = data.get(\"metadata\", {})\n    metadata[\"source\"] = file_path\n    return text_content, metadata\n\n# A longer and more complex JSON sample\njson_data = {\n    \"metadata\": {\n        \"document_id\": \"doc-12345\",\n        \"created_at\": \"2025-11-05\"\n    },\n    \"text\": [\n        \"This is the first paragraph of our longer JSON sample. It contains multiple sentences to test the chunking process.\",\n        \"The second paragraph introduces a new topic. We are exploring the capabilities of custom processors in the chunklet library.\",\n        \"Finally, the third paragraph concludes our sample. We hope this demonstrates the flexibility of the system in handling various data formats.\"\n    ]\n}\n\nchunker = DocumentChunker()\n\n# Use a temporary file\nwith tempfile.NamedTemporaryFile(mode='w+', suffix=\".json\") as tmp:\n    json.dump(json_data, tmp)\n    tmp.seek(0)\n    tmp_path = tmp.name\n\n    chunks = chunker.chunk(\n        path=tmp_path,\n        max_sentences=5,\n    )\n\n    for i, chunk in enumerate(chunks):\n        print(f\"--- Chunk {i+1} ---\")\n        print(f\"Content:\\n{chunk.content}\\n\")\n        print(f\"Metadata:\\n{chunk.metadata}\")\n        print()\n\n# Optionally unregister\nregistry.unregister(\".json\")\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\nThis is the first paragraph of our longer JSON sample.\nIt contains multiple sentences to test the chunking process.\nThe second paragraph introduces a new topic.\nWe are exploring the capabilities of custom processors in the chunklet library.\nFinally, the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 1, 'span': (0, 292)}\n\n--- Chunk 2 ---\nContent:\n... the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 2, 'span': (250, 292)}\n</code></pre> <p>Registering Without the Decorator</p> <p>If you prefer not to use decorators, you can directly use the <code>registry.register()</code> method. This is particularly useful when registering processors dynamically.</p> <pre><code>from chunklet.document_chunker import CustomProcessorRegistry\n\nregistry = CustomProcessorRegistry()\n\ndef my_other_processor(file_path: str) -&gt; tuple[str, dict]:\n    # ... your logic ...\n    return \"some text\", {\"source\": file_path}\n\nregistry.register(my_other_processor, \".custom\", name=\"MyOtherProcessor\")\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#customprocessorregistry-methods-summary","title":"<code>CustomProcessorRegistry</code> Methods Summary","text":"<ul> <li><code>processors</code>: Returns a shallow copy of the dictionary of registered processors.</li> <li><code>is_registered(ext: str)</code>: Checks if a processor is registered for the given file extension, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], ReturnType] | None = None, *exts: str, name: str | None = None)</code>: Registers a processor callback for one or more file extensions.</li> <li><code>unregister(*exts: str)</code>: Removes processor(s) from the registry.</li> <li><code>clear()</code>: Clears all registered processors from the registry.</li> <li><code>extract_data(file_path: str, ext: str)</code>: Processes a file using a registered processor, returning the extracted data and the name of the processor used.</li> </ul> API Reference <p>For complete technical details on the <code>DocumentChunker</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/","title":"Plain Text Chunker","text":""},{"location":"getting-started/programmatic/plain_text_chunker/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py\n</code></pre> <p>No extra dependencies needed - <code>PlainTextChunker</code> is ready to roll right out of the box! \ud83d\ude80</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#taming-your-text-with-precision","title":"Taming Your Text with Precision","text":"<p>Got a wall of text that's feeling a bit... overwhelming? The <code>PlainTextChunker</code> is your friendly neighborhood text organizer that transforms unruly paragraphs into perfectly sized, context-aware chunks. Perfect for RAG systems, document analysis, and any workflow that needs smart text segmentation with full control over chunk sizes.</p> <p>Forget dumb splitting - we're talking intelligent segmentation that actually understands context! The <code>PlainTextChunker</code> works hard to preserve meaning and flow, so your chunks don't end up as confusing puzzle pieces.</p> <p>Ready to bring some order to the chaos? Let's dive in and make your text behave!</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#where-plaintextchunker-really-shines","title":"Where <code>PlainTextChunker</code> Really Shines","text":"<p>The <code>PlainTextChunker</code> comes packed with smart features that make it your go-to text wrangling sidekick:</p> <ul> <li>Flexible Constraint-Based Chunking: Ultimate control over your chunks! Mix and match limits based on sentences, tokens, or Markdown section breaks. Craft exactly the chunk size you need with precision control! \ud83c\udfaf</li> <li>Intelligent Overlap for Context Preservation: Adds smart overlaps between chunks so your text flows smoothly. No more jarring transitions that leave readers scratching their heads!</li> <li>Extensive Multilingual Support: Speaks over 50 languages fluently, thanks to our trusty sentence splitter. Global domination through better text chunking! \ud83c\udf0d</li> <li>Customizable Token Counting: Plug in your own token counter for perfect alignment with different LLMs. Because one size definitely doesn't fit all models!</li> <li>Optimized Parallel Processing: Turbocharges through large texts using multiple processors. Speed demon mode activated! \u26a1</li> <li>Memory-Conscious Operation: Handles massive documents efficiently by yielding chunks one at a time. Your RAM will thank you later! \ud83d\udcbe</li> </ul>"},{"location":"getting-started/programmatic/plain_text_chunker/#constraint-based-chunking-your-text-your-rules","title":"Constraint-Based Chunking: Your Text, Your Rules!","text":"<p><code>PlainTextChunker</code> lets you call the shots with constraint-based chunking. Mix and match limits to craft the perfect chunk size for your needs. Here's the constraint menu:</p> Constraint Value Requirement Description <code>max_sentences</code> <code>int &gt;= 1</code> Sentence power mode! Tell us how many sentences per chunk, and we'll group them thoughtfully so your ideas flow like a well-written story. <code>max_tokens</code> <code>int &gt;= 12</code> Token budget watcher! We'll carefully pack sentences into chunks while respecting your token limits. If a sentence gets too chatty, we'll politely split it at clause boundaries. \ud83e\udd10 <code>max_section_breaks</code> <code>int &gt;= 1</code> Structure superhero! Limits Markdown section breaks per chunk (headings <code>##</code>, rules <code>---</code>) to keep your document's organization intact. Your headings stay where they belong! <p>The <code>PlainTextChunker</code> has two main methods: <code>chunk</code> for single texts and <code>batch_chunk</code> for processing multiple texts at once. <code>chunk</code> returns a list of handy <code>Box</code> objects, while <code>batch_chunk</code> is a memory-friendly generator that yields chunks one by one. Each <code>Box</code> has <code>content</code> (the actual text) and <code>metadata</code> (all the juicy details). Check the Metadata guide for the full scoop!</p> <p>Quick Note: Constraints Required!</p> <p>You must specify at least one limit (like <code>max_sentences</code>, <code>max_tokens</code>, or <code>max_section_breaks</code>) when using <code>chunk</code> or <code>batch_chunk</code>. Forget to add one? You'll get an <code>InvalidInputError</code> - but don't worry, it's an easy fix!</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#single-run","title":"Single Run:","text":"<p>For our examples, we'll use this sample text:</p> <pre><code>text = \"\"\"\n# Introduction to Chunking\n\nThis is the first paragraph of our document. It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization. We aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information. It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings. Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques: Level Up Your Skills!\n\nReady to go beyond the basics? Let's explore some pro-level techniques that make your chunks even smarter!\n\n### Overlap Considerations: Keeping Things Connected\n\nWant your chunks to flow smoothly like a well-told story? Overlap is your secret weapon! It includes a bit of the previous chunk at the start of the next one, ensuring your text doesn't feel choppy or disconnected. Continuity is key!\n\n---\n\n# Conclusion\n\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data. Experiment with different constraints to find the optimal strategy for your needs.\n\"\"\"\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-sentences-sentence-power-mode","title":"Chunking by Sentences: Sentence Power Mode! \ud83d\udcdd","text":"<p>Ready to chunk by sentence count? This is perfect when you want predictable, idea-focused chunks. Let's see it in action:</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\nchunker = PlainTextChunker()  # (1)!\n\nchunks = chunker.chunk(\n    text=text,\n    lang=\"auto\",             # (2)!\n    max_sentences=2,\n    overlap_percent=0,       # (3)!\n    offset=0                 # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li><code>verbose=True</code> turns on the chatty mode - you'll see detailed logging of internal processes like language detection. (Default is quiet mode!)</li> <li><code>lang=\"auto\"</code> lets us detect the language automatically. Super convenient, but specifying a known language like <code>lang=\"en\"</code> can boost accuracy and speed.</li> <li><code>overlap_percent=0</code> means no overlap between chunks. By default, we add 20% overlap to keep your text flowing smoothly across chunks.</li> <li><code>offset=0</code> starts us from the very beginning of the text. (Zero-based indexing - because programmers love starting from zero!)</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 73)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (74, 259)}\nContent: It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (260, 370)}\nContent: ## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (371, 529)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (531, 748)}\nContent: There are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 6, 'span': (749, 786)}\nContent: ---\n\n## Advanced Chunking Techniques\n\n--- Chunk 7 ---\nMetadata: {'chunk_num': 7, 'span': (788, 995)}\nContent: Beyond basic splitting, advanced techniques involve understanding the document's structure.\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\n\n--- Chunk 8 ---\nMetadata: {'chunk_num': 8, 'span': (996, 1066)}\nContent: This section will delve into such methods.\n\n### Overlap Considerations\n\n--- Chunk 9 ---\nMetadata: {'chunk_num': 9, 'span': (1068, 1264)}\nContent: To ensure smooth transitions between chunks, an overlap mechanism is often employed.\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n--- Chunk 10 ---\nMetadata: {'chunk_num': 10, 'span': (749, 763)}\nContent: ---\n\n# Conclusion\n\n--- Chunk 11 ---\nMetadata: {'chunk_num': 11, 'span': (1285, 1459)}\nContent: In conclusion, mastering chunking is key to unlocking the full potential of your text data.\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>DocumentChunker</code>: <pre><code>chunker = PlainTextChunker(verbose=True)\n</code></pre></p>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-tokens-token-budget-master","title":"Chunking by Tokens: Token Budget Master! \ud83e\ude99","text":"<p>Token Counter Requirement</p> <p>When using the <code>max_tokens</code> constraint, a <code>token_counter</code> function is essential. This function, which you provide, should accept a string and return an integer representing its token count. Failing to provide a <code>token_counter</code> will result in a <code>MissingTokenCounterError</code>.</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\n# Simple counter for demonstration purpose\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nchunker = PlainTextChunker(token_counter=word_counter)         # (1)!\n\nchunks = chunker.chunk(\n    text=text,\n    lang=\"auto\",\n    max_tokens=12,\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Initializes <code>PlainTextChunker</code> with a custom <code>word_counter</code> function. This function will be used to count tokens when <code>max_tokens</code> is used.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 291)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (256, 573)}\nContent: ...\n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking,\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (531, 880)}\nContent: There are several strategies for chunking,\nincluding splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nBeyond basic splitting, advanced techniques involve understanding the document's structure.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (808, 1153)}\nContent: ... advanced techniques involve understanding the document's structure.\n\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n### Overlap Considerations\nTo ensure smooth transitions between chunks, an overlap mechanism is often employed.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1109, 1377)}\nContent: ... an overlap mechanism is often employed.\n\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n---\n\n# Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 6, 'span': (1296, 1459)}\nContent: ... mastering chunking is key to unlocking the full potential of your text data.\n\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-section-breaks-structure-superhero","title":"Chunking by Section Breaks: Structure Superhero! \ud83e\uddb8\u200d\u2640\ufe0f","text":"<p>This constraint is useful for documents structured with Markdown headings or thematic breaks.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_section_breaks=2\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 503)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (371, 753)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own,\nwhich is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (678, 1038)}\nContent: Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nBeyond basic splitting, advanced techniques involve understanding the document's structure.\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (890, 1269)}\nContent: ... preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n### Overlap Considerations\nTo ensure smooth transitions between chunks, an overlap mechanism is often employed.\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n---\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1239, 1459)}\nContent: ... providing continuity.\n\n---\n\n# Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Adding Base Metadata</p> <p>You can pass a <code>base_metadata</code> dictionary to the <code>chunk</code> method. This metadata will be included in the <code>metadata</code> of each chunk. For example: <code>chunker.chunk(..., base_metadata={\"source\": \"my_document.txt\"})</code>. For more details on metadata structure and available fields, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#combining-multiple-constraints-mix-and-match-magic","title":"Combining Multiple Constraints: Mix and Match Magic! \ud83c\udfad","text":"<p>The real power of <code>PlainTextChunker</code> comes from combining multiple constraints. This allows for highly specific and granular control over how your text is chunked. Here are a few examples of how you can combine different constraints.</p> <p>Token Counter Requirement</p> <p>Remember, whenever you use the <code>max_tokens</code> constraint, you must provide a <code>token_counter</code> function.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-and-tokens","title":"By Sentences and Tokens","text":"<p>This is useful when you want to limit by both the number of sentences and the overall token count, whichever is reached first.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=5,\n    max_tokens=100\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-and-section-breaks","title":"By Sentences and Section Breaks","text":"<p>This combination is great for ensuring that chunks don't span across too many sections while also keeping the sentence count in check.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=10,\n    max_section_breaks=2\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-tokens-and-section-breaks","title":"By Tokens and Section Breaks","text":"<p>A powerful combination for structured documents where you want to respect section boundaries while adhering to a strict token budget.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_tokens=256,\n    max_section_breaks=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-tokens-and-section-breaks","title":"By Sentences, Tokens, and Section Breaks","text":"<p>For the ultimate level of control, you can combine all three constraints. The chunking will stop as soon as any of the three limits is reached.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=8,\n    max_tokens=200,\n    max_section_breaks=2\n)\n</code></pre> <p>Customizing the Continuation Marker</p> <p>You can customize the continuation marker, which is prepended to clauses that don't fit in the previous chunk. To do this, pass the <code>continuation_marker</code> parameter to the chunker's constructor.</p> <pre><code>chunker = PlainTextChunker(continuation_marker=\"[...]\")\n</code></pre> <p>If you don't want any continuation marker, you can set it to an empty string:</p> <pre><code>chunker = PlainTextChunker(continuation_marker=\"\")\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#batch-run-processing-multiple-texts-like-a-pro","title":"Batch Run: Processing Multiple Texts Like a Pro! \ud83d\udcda","text":"<p>While <code>chunk</code> is perfect for single texts, <code>batch_chunk</code> is your power player for processing multiple texts in parallel. It uses a memory-friendly generator so you can handle massive text collections with ease. It shares most arguments with <code>chunk</code> (like <code>max_sentences</code>, <code>max_tokens</code>, <code>lang</code>, etc.), plus some extra parameters for batch management.</p> <p>Here's an example of how to use <code>batch_chunk</code>:</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nEN_TEXT = \"This is the first document. It has multiple sentences for chunking. Here is the second document. It is a bit longer to test batch processing effectively. And this is the third document. Short and sweet, but still part of the batch. The fourth document. Another one to add to the collection for testing purposes.\"\nES_TEXT = \"Este es el primer documento. Contiene varias frases para la segmentaci\u00f3n de texto. El segundo ejemplo es m\u00e1s extenso. Queremos probar el procesamiento en diferentes idiomas.\"\nFR_TEXT = \"Ceci est le premier document. Il est essentiel pour l'\u00e9valuation multilingue. Le deuxi\u00e8me document est court mais important. La variation est la cl\u00e9.\"\n\n# Initialize PlainTextChunker\nchunker = PlainTextChunker(token_counter=word_counter)\n\nchunks = chunker.batch_chunk(\n    texts=[EN_TEXT, ES_TEXT, FR_TEXT],\n    max_sentences=5,\n    max_tokens=20,\n    n_jobs=2,                    # (1)!\n    on_errors=\"raise\",           # (2)!\n    show_progress=True,          # (3)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to show output <pre><code>  0%|                                              | 0/3 [00:00&lt;?, ?it/s]\n--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 97)}\nContent: This is the first document.\nIt has multiple sentences for chunking.\nHere is the second document.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (96, 202)}\nContent: It is a bit longer to test batch processing effectively.\nAnd this is the third document.\nShort and sweet,\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (186, 253)}\nContent: Short and sweet,\nbut still part of the batch.\nThe fourth document.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (252, 311)}\nContent: Another one to add to the collection for testing purposes.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 1, 'span': (0, 118)}\nContent: Este es el primer documento.\nContiene varias frases para la segmentaci\u00f3n de texto.\nEl segundo ejemplo es m\u00e1s extenso.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 2, 'span': (117, 173)}\nContent: Queremos probar el procesamiento en diferentes idiomas.\n\nChunking ...: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e            | 2/3 [00:00, 10.09it/s]\n--- Chunk 7 ---\nMetadata: {'chunk_num': 1, 'span': (0, 125)}\nContent: Ceci est le premier document.\nIl est essentiel pour l'\u00e9valuation multilingue.\nLe deuxi\u00e8me document est court mais important.\n\n--- Chunk 8 ---\nMetadata: {'chunk_num': 2, 'span': (125, 149)}\nContent: La variation est la cl\u00e9.\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00, 19.88it/s]\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p> <p>Adding Base Metadata to Batches</p> <p>Just like with the <code>chunk</code> method, you can pass a <code>base_metadata</code> dictionary to <code>batch_chunk</code>. This is useful for adding common information, like a source filename, to all chunks processed in the batch. For more details on metadata structure and available fields, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#separator","title":"Separator: Keeping Your Batches Organized! \ud83d\udccb","text":"<p>The <code>separator</code> parameter lets you add a custom marker that gets yielded after all chunks from a single text are processed. Super handy for batch processing when you want to clearly separate chunks from different source texts.</p> <p>Quick Note</p> <p><code>None</code> won't work as a separator - you'll need something more substantial!</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\nfrom more_itertools import split_at\n\nchunker = PlainTextChunker()\ntexts = [\n    \"This is the first document. It has two sentences.\",\n    \"This is the second document. It also has two sentences.\"\n]\ncustom_separator = \"---END_OF_DOCUMENT---\"\n\nchunks_with_separators = chunker.batch_chunk(\n    texts,\n    max_sentences=1,\n    separator=custom_separator,\n    show_progress=False,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, doc_chunks in enumerate(chunk_groups):\n    if doc_chunks: # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in doc_chunks:\n            print(f\"Content: {chunk.content}\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>--- Chunks for Document 1 ---\nContent: This is the first document.\nMetadata: {'chunk_num': 1, 'span': (0, 27)}\nContent: It has two sentences.\nMetadata: {'chunk_num': 2, 'span': (28, 49)}\n\n--- Chunks for Document 2 ---\nContent: This is the second document.\nMetadata: {'chunk_num': 1, 'span': (0, 28)}\nContent: It also has two sentences.\nMetadata: {'chunk_num': 2, 'span': (29, 55)}\n</code></pre> API Reference <p>For complete technical details on the <code>PlainTextChunker</code> class, check out the API documentation.````</p>"},{"location":"getting-started/programmatic/sentence_splitter/","title":"Sentence Splitter","text":""},{"location":"getting-started/programmatic/sentence_splitter/#the-art-of-precise-sentence-splitting","title":"The Art of Precise Sentence Splitting \u2702\ufe0f","text":"<p>Let's be honest, simply splitting text by periods can be a bit like trying to perform delicate surgery with a butter knife \u2013 it often leads to more problems than solutions! This approach can result in sentences being cut mid-thought, abbreviations being misinterpreted, and a general lack of clarity that can leave your NLP models scratching their heads.</p> <p>This common challenge in NLP, known as Sentence Boundary Disambiguation, is precisely what the <code>SentenceSplitter</code> is designed to address.</p> <p>Imagine the <code>SentenceSplitter</code> as a skilled linguistic surgeon. It applies its understanding of grammar and context to make precise cuts, cleanly separating sentences while preserving their original meaning. It's intelligent, multilingual, and essential for preparing clean text data for NLP tasks, LLMs, and any application that needs accurate sentence boundaries.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#whats-under-the-hood","title":"What's Under the Hood? \u2699\ufe0f","text":"<p>The <code>SentenceSplitter</code> is more than just a basic rule-based tool; it's a sophisticated system packed with powerful features:</p> <ul> <li>Multilingual Support \ud83c\udf0d: Handles over 50 languages with intelligent detection and language-specific splitting methods. Check our supported languages for the full list.</li> <li>Custom Splitters \ud83d\udd27: Easily integrate your own custom sentence splitting functions for specialized languages or domains.</li> <li>Reliable Fallback \ud83d\udee1\ufe0f: For unsupported languages, a robust fallback mechanism ensures effective sentence splitting.</li> <li>Error Monitoring \ud83d\udd0d: Actively monitors for issues and provides clear feedback on custom splitter problems.</li> <li>Output Refinement \u2728: Meticulously cleans the output, removing empty sentences and fixing punctuation issues.</li> </ul>"},{"location":"getting-started/programmatic/sentence_splitter/#example-usage","title":"Example Usage","text":"<p>Here's a quick example of how you can use the <code>SentenceSplitter</code> to split a block of text into sentences:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nTEXT = \"\"\"\nShe loves cooking. He studies AI. \"You are a Dr.\", she said. The weather is great. We play chess. Books are fun, aren't they?\n\nThe Playlist contains:\n  - two videos\n  - one image\n  - one music\n\nRobots are learning. It's raining. Let's code. Mars is red. Sr. sleep is rare. Consider item 1. This is a test. The year is 2025. This is a good year since N.A.S.A. reached 123.4 light year more.\n\"\"\"\n\nsplitter = SentenceSplitter(verbose=True)\nsentences = splitter.split(TEXT, lang=\"auto\") #(1)!\n\nfor sentence in sentences:\n    print(sentence)\n</code></pre> <ol> <li>Auto language detection: Let the splitter automatically detect the language of your text. For best results, specify a language code like <code>\"en\"</code> or <code>\"fr\"</code> directly.</li> </ol> Click to show output <pre><code>2025-11-02 16:27:29.277 | WARNING  | chunklet.sentence_splitter.sentence_splitter:split:136 - The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\n2025-11-02 16:27:29.316 | INFO     | chunklet.sentence_splitter.sentence_splitter:detected_top_language:109 - Language detection: 'en' with confidence 10/10.\n2025-11-02 16:27:29.447 | INFO     | chunklet.sentence_splitter.sentence_splitter:split:167 - Text splitted into sentences. Total sentences detected: 19\nShe loves cooking.\nHe studies AI.\n\"You are a Dr.\", she said.\nThe weather is great.\nWe play chess.\nBooks are fun, aren't they?\nThe Playlist contains:\n- two videos\n- one image\n- one music\nRobots are learning.\nIt's raining.\nLet's code.\nMars is red.\nSr. sleep is rare.\nConsider item 1.\nThis is a test.\nThe year is 2025.\nThis is a good year since N.A.S.A. reached 123.4 light year more.\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#detecting-top-languages","title":"Detecting Top Languages \ud83c\udfaf","text":"<p>Here's how you can detect the top language of a given text using the <code>SentenceSplitter</code>:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nlang_texts = {\n    \"en\": \"This is a sentence. This is another sentence. Mr. Smith went to Washington. He said 'Hello World!'. The quick brown fox jumps over the lazy dog.\",\n    \"fr\": \"Ceci est une phrase. Voici une autre phrase. M. Smith est all\u00e9 \u00e0 Washington. Il a dit 'Bonjour le monde!'. Le renard brun et rapide saute par-dessus le chien paresseux.\",\n    \"es\": \"Esta es una oraci\u00f3n. Aqu\u00ed hay otra oraci\u00f3n. El Sr. Smith fue a Washington. Dijo '\u00a1Hola Mundo!'. El r\u00e1pido zorro marr\u00f3n salta sobre el perro perezoso.\",\n    \"de\": \"Dies ist ein Satz. Hier ist ein weiterer Satz. Herr Smith ging nach Washington. Er sagte 'Hallo Welt!'. Der schnelle braune Fuchs springt \u00fcber den faulen Hund.\",\n    \"hi\": \"\u092f\u0939 \u090f\u0915 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u092f\u0939 \u090f\u0915 \u0914\u0930 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u0936\u094d\u0930\u0940 \u0938\u094d\u092e\u093f\u0925 \u0935\u093e\u0936\u093f\u0902\u0917\u091f\u0928 \u0917\u090f\u0964 \u0909\u0938\u0928\u0947 \u0915\u0939\u093e '\u0928\u092e\u0938\u094d\u0924\u0947 \u0926\u0941\u0928\u093f\u092f\u093e!'\u0964 \u0924\u0947\u091c \u092d\u0942\u0930\u093e \u0932\u094b\u092e\u0921\u093c\u0940 \u0906\u0932\u0938\u0940 \u0915\u0941\u0924\u094d\u0924\u0947 \u092a\u0930 \u0915\u0942\u0926\u0924\u093e \u0939\u0948\u0964\"\n}\n\nsplitter = SentenceSplitter()\n\nfor lang, text in lang_texts.items():\n    detected_lang, confidence = splitter.detected_top_language(text)\n    print(f\"Original language: {lang}\")\n    print(f\"Detected language: {detected_lang} with confidence {confidence:.2f}\")\n    print(\"-\" * 20)\n</code></pre> Click to show output <pre><code>Original language: en\nDetected language: en with confidence 1.00\n--------------------\nOriginal language: fr\nDetected language: fr with confidence 1.00\n--------------------\nOriginal language: es\nDetected language: es with confidence 1.00\n--------------------\nOriginal language: de\nDetected language: de with confidence 1.00\n--------------------\nOriginal language: hi\nDetected language: hi with confidence 1.00\n--------------------\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#custom-sentence-splitter","title":"Custom Sentence Splitter: Your Sentence Splitting Playground \ud83c\udfa8","text":"<p>Want to bring your own sentence splitting magic? You can plug in your custom splitter functions to Chunklet! Perfect for specialized languages or domains where you want to prioritize your custom logic over our built-in splitters.</p> <p>Global Registry Alert!</p> <p>Custom splitters get registered globally - once you add one, it's available everywhere in your app. Watch out for side effects if you're registering splitters across different parts of your codebase, especially in multi-threaded or long-running applications!</p> <p>To use a custom splitter, you leverage the <code>@registry.register</code> decorator. This decorator allows you to register your function for one or more languages directly. Your custom splitter function must accept a single <code>text</code> parameter (str) and return a <code>list[str]</code> of sentences.</p> <p>Custom Splitter Rules</p> <ul> <li>Your function must accept exactly one required parameter (the text)</li> <li>Optional parameters with defaults are totally fine</li> <li>Must return a list of strings</li> <li>Empty strings get filtered out automatically</li> <li>Lambda functions work if you provide a <code>name</code> parameter</li> <li>Errors during splitting will raise a <code>CallbackError</code></li> </ul>"},{"location":"getting-started/programmatic/sentence_splitter/#basic-custom-splitter","title":"Basic Custom Splitter","text":"<pre><code>import re\nfrom chunklet.sentence_splitter import SentenceSplitter, CustomSplitterRegistry\n\nsplitter = SentenceSplitter(verbose=False)\nregistry = CustomSplitterRegistry()\n\n@registry.register(\"en\", name=\"MyCustomEnglishSplitter\")\ndef english_sent_splitter(text: str) -&gt; list[str]:\n    \"\"\"A simple custom sentence splitter\"\"\"\n    return [s.strip() for s in re.split(r'(?&lt;=\\\\.)\\s+', text) if s.strip()]\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences = splitter.split(text=text, lang=\"en\")\n\nprint(\"--- Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n</code></pre> Click to show output <pre><code>--- Sentences using Custom Splitter ---\nSentence 1: This is the first sentence.\nSentence 2: This is the second sentence.\nSentence 3: And the third.\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#multi-language-custom-splitter","title":"Multi-Language Custom Splitter","text":"<pre><code>@registry.register(\"fr\", \"es\", name=\"MultiLangExclamationSplitter\")  #(1)!\ndef multi_lang_splitter(text: str) -&gt; list[str]:\n    return [s.strip() for s in re.split(r'(?&lt;=!)\\s+', text) if s.strip()]\n</code></pre> <ol> <li>This registers the same custom splitter for both French (\"fr\") and Spanish (\"es\") languages.</li> </ol>"},{"location":"getting-started/programmatic/sentence_splitter/#unregistering-custom-splitters","title":"Unregistering Custom Splitters","text":"<pre><code>registry.unregister(\"en\")  # (1)!\n</code></pre> <ol> <li>This will remove the custom splitter associated with the \"en\" language code. Note that you can unregister multiple languages if you had registered them with the same function: <code>registry.unregister(\"fr\", \"es\")</code></li> </ol> <p>Skip the Decorator?</p> <p>Not a fan of decorators? No worries - you can directly use the <code>registry.register()</code> method. Super handy for dynamic registration or when your callback function isn't in the global scope.</p> <pre><code>from chunklet.sentence_splitter import CustomSplitterRegistry\n\nregistry = CustomSplitterRegistry()\n\ndef my_other_splitter(text: str) -&gt; list[str]:\n    return text.split(' ')\n\nregistry.register(my_other_splitter, \"jp\", name=\"MyOtherSplitter\")\n</code></pre> <p>Want to Build from Scratch?</p> <p>Going full custom? Inherit from the <code>BaseSplitter</code> abstract class! It gives you a clear interface (<code>def split(self, text: str, lang: str) -&gt; list[str]</code>) to implement. Your custom splitter will then work seamlessly with <code>PlainTextChunker</code> (docs) or <code>DocumentChunker</code> (docs).</p>"},{"location":"getting-started/programmatic/sentence_splitter/#customsplitterregistry-methods-summary","title":"<code>CustomSplitterRegistry</code> Methods Summary","text":"<ul> <li><code>splitters</code>: Returns a shallow copy of the dictionary of registered splitters.</li> <li><code>is_registered(lang: str)</code>: Checks if a splitter is registered for the given language, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], list[str]] | None = None, *langs: str, name: str | None = None)</code>: Registers a splitter callback for one or more languages.</li> <li><code>unregister(*langs: str)</code>: Removes splitter(s) from the registry.</li> <li><code>clear()</code>: Clears all registered splitters from the registry.</li> <li><code>split(text: str, lang: str)</code>: Processes a text using a splitter registered for the given language, returning a list of sentences and the name of the splitter used.</li> </ul> API Reference <p>For complete technical details on the <code>SentenceSplitter</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/visualizer/","title":"Text Chunk Visualizer","text":"<p> Interactive chunking in action - upload, process, and explore! </p>"},{"location":"getting-started/programmatic/visualizer/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py[visualization]\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#text-chunk-visualizer-your-window-into-the-chunking-abyss","title":"Text Chunk Visualizer: Your Window into the Chunking Abyss","text":"<p>This installs the web interface dependencies (FastAPI + Uvicorn) for interactive chunk visualization! \ud83c\udf10</p> <p>Ever wondered what your text or code looks like after being chopped up by a chunking algorithm? The Text Chunk Visualizer demystifies text segmentation with a clean web interface - WYSIWYG (what you see is what you get).</p> <p>No more guessing games - see your chunking results in real-time!</p>"},{"location":"getting-started/programmatic/visualizer/#so-how-do-i-get-this-running","title":"So How Do I Get This Running?","text":"<p>First, make sure you have the visualization dependencies:</p> <pre><code>pip install \"chunklet-py[visualization]\"\n</code></pre> <p>Here's the basic code to get it running:</p> <pre><code>from chunklet.visualizer import Visualizer\n\n# Optional: Define a custom token counter\n# def my_token_counter(text: str) -&gt; int:\n#     return len(text.split())  # Simple word-based counting\n\nvisualizer = Visualizer(\n    host=\"127.0.0.1\",    #(1)!\n    port=8000,          #(2)!\n    # token_counter=my_token_counter  # Uncomment if using custom counter\n)\n\nvisualizer.serve()  # Blocks until Ctrl+C\n</code></pre> <ol> <li>Host: The IP address where the server will listen. Use <code>\"127.0.0.1\"</code> for localhost or <code>\"0.0.0.0\"</code> to allow access from other devices on your network.</li> <li>Port: The port number for the web server. The visualizer will be accessible at <code>http://host:port</code>.  </li> </ol> Click to show output <pre><code>Starting Chunklet Visualizer...\nURL: http://127.0.0.1:8000\nPress Ctrl+C to stop the server\nOpened in default browser\n = = = = = = = = = = = = = = = = = = = = = = = = = =\n\nTEXT CHUNK VISUALIZER\n= = = = = = = = = = = = = = = = = = = = = = = = = =\nURL: http://127.0.0.1:8000\nINFO:     Started server process [30999]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     127.0.0.1:45482 - \"GET / HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45490 - \"GET /static/js/app.js HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45482 - \"GET /static/css/style.css HTTP/1.1\" 304 Not Modified\nINFO:     127.0.0.1:45490 - \"GET /api/token_counter_status HTTP/1.1\" 200 OK\n</code></pre> <p>Run this and you'll see the server start up with the URL where your visualizer is ready!</p> <p>(But honestly, the CLI <code>chunklet visualize</code> command is way easier for most use cases!)</p> <p>Prefer command line?</p> <p>For quick access without writing code, check out the CLI visualize command.</p>"},{"location":"getting-started/programmatic/visualizer/#whats-the-web-interface-like","title":"What's the Web Interface Like?","text":"<p>Open your browser to the URL shown in the terminal output. You'll find a clean interface designed for quick chunking experiments.</p> How do I upload files? <p>Simple: drag and drop your text files (<code>.txt</code>, <code>.md</code>, <code>.py</code>, etc.) onto the upload area, or click \"Browse Files\" to select them manually. The visualizer accepts any text-based file.</p> What's the difference between Document and Code mode? <p>Choose your chunking strategy after upload: - Document Mode: For general text, articles, and documents - focuses on sentences and sections - Code Mode: For source code - understands functions, classes, and code structure</p> <p>Each mode has its own parameter controls because text and code need different chunking approaches.</p> How Do I Process My Content? <p>Select your mode and parameters, then click \"Process Document\" or \"Process Code\". The visualizer applies your settings and shows exactly how your content gets chunked.</p> What About the Interactive Features? <p>The interface gives you great visibility:</p> <ul> <li>Click to Highlight: Click text to see which chunk(s) contain it</li> <li>Double-Click for Details: Get full metadata popups with span, chunk number, and source info</li> <li>Overlap Toggle: Use \"Reveal Overlaps\" to see where chunks share content</li> </ul> Can I Export My Results? <p>Absolutely! Click \"Download Chunks\" to get a JSON file with all chunks, their content, and complete metadata - perfect for further processing or analysis.</p> <p>The exported JSON follows this structure:</p> <pre><code>{\n    \"chunks\": [\n     {\n        \"content\": \"The actual text content of this chunk...\",\n        \"metadata\": {\n            \"source\": \"path/to/source/file.txt\",\n            \"chunk_num\": 1,\n            \"span\": [0, 150],\n            // ... other metadata fields\n        }\n    },\n    // ... more chunks\n    ],\n     \"stats\": {\n    \"chunk_count\": 3,\n    \"overlap_count\": 2,\n    \"text_length\": 696,\n    \"mode\": \"document\",\n    \"generated\": \"2025-12-18T15:16:11.379Z\"\n  }\n}\n</code></pre> <p>Quick Tips for Better Results</p> <ul> <li>Start with small files to get familiar with the interface</li> <li>Experiment with different parameter combinations to see their effects</li> <li>Use the metadata views to understand chunk boundaries</li> <li>The visualizer is perfect for comparing chunking strategies side-by-side</li> </ul> <p>Go experiment! The visualizer makes it easy to see exactly what your settings produce, so you can fine-tune for optimal chunking.</p>"},{"location":"getting-started/programmatic/visualizer/#headlessrest-api-usage","title":"Headless/REST API Usage","text":"<p>The <code>Visualizer</code> isn't just a web interface - it also provides a complete REST API for headless chunking operations. This means you can use Chunklet's interactive features programmatically without the web UI!</p>"},{"location":"getting-started/programmatic/visualizer/#available-endpoints","title":"Available Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/health</code> Health check endpoint <code>GET</code> <code>/api/token_counter_status</code> Check if token counter is configured <code>POST</code> <code>/api/chunk</code> Upload and chunk a file"},{"location":"getting-started/programmatic/visualizer/#chunking-files-programmatically","title":"Chunking Files Programmatically","text":""},{"location":"getting-started/programmatic/visualizer/#option-1-cli-headless-server-recommended","title":"Option 1: CLI Headless Server (Recommended)","text":"<p>The easiest way to start a headless visualizer server is with the CLI:</p> <pre><code>chunklet visualize --headless --port 8000\n</code></pre> <p>CLI Headless Mode</p> <p>See the Scenario 3: Headless Mode in the CLI documentation for more details on headless CLI usage with custom tokenizers.</p>"},{"location":"getting-started/programmatic/visualizer/#option-2-python-server-script","title":"Option 2: Python Server Script","text":"<p>For more programmatic control, create a custom server script (<code>server.py</code>):</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Headless visualizer server for programmatic chunking.\"\"\"\n\nfrom chunklet.visualizer import Visualizer\n\n# Configure your visualizer\nvisualizer = Visualizer(\n    host=\"127.0.0.1\",\n    port=8000,\n    # token_counter=lambda text: len(text.split())  # Optional custom tokenizer\n)\n\nprint(\"Starting headless visualizer server...\")\nprint(\"Press Ctrl+C to stop\")\nvisualizer.serve()\n</code></pre> <p>Run the server:</p> <pre><code>python server.py\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#using-the-rest-api-client","title":"Using the REST API Client","text":"<p>Use this Python client to chunk files programmatically:</p> <pre><code>import requests\n\n# Connect to your running server\nbase_url = \"http://127.0.0.1:8000\"\n\n# Check if token counter is available\nresponse = requests.get(f\"{base_url}/api/token_counter_status\")\nprint(response.json())  # {\"token_counter_available\": false}\n\n# Chunk a file\nwith open(\"my_document.txt\", \"rb\") as f:\n    files = {\"file\": (\"my_document.txt\", f, \"text/plain\")}\n    data = {\n        \"mode\": \"document\",  # or \"code\"\n        \"params\": '{\"max_sentences\": 3, \"overlap_percent\": 20}'\n    }\n\n    response = requests.post(f\"{base_url}/api/chunk\", files=files, data=data)\n\nif response.status_code == 200:\n    result = response.json()\n    print(f\"Created {result['stats']['chunk_count']} chunks\")\n\n    # Access chunks\n    for chunk in result[\"chunks\"]:\n        print(f\"Chunk content: {chunk['content']}\")\n        print(f\"Metadata: {chunk['metadata']}\")\nelse:\n    print(f\"Error: {response.status_code} - {response.text}\")\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#response-format","title":"Response Format","text":"<p>The <code>/api/chunk</code> endpoint returns:</p> <pre><code>{\n  \"text\": \"Original file content...\",\n  \"chunks\": [\n    {\n      \"content\": \"Chunk text content...\",\n      \"metadata\": {\n        \"source\": \"filename.txt\",\n        \"chunk_num\": 1,\n        \"span\": [0, 150],\n        // ... additional metadata\n      }\n    }\n  ],\n  \"stats\": {\n    \"text_length\": 696,\n    \"chunk_count\": 3,\n    \"mode\": \"document\"\n  }\n}\n</code></pre> <p>Perfect for Integration</p> <p>Use the REST API to integrate Chunklet's visualizer capabilities into your own applications, automation scripts, or testing pipelines!</p> API Reference <p>For complete technical details on the <code>Visualizer</code> class, check out the API documentation.</p>"},{"location":"reference/chunklet/","title":"chunklet","text":""},{"location":"reference/chunklet/#chunklet","title":"chunklet","text":"<p>Chunklet: Advanced Text, Code, and Document Chunking for LLM Applications</p> <p>A comprehensive library for semantic text segmentation, interactive chunk visualization, and multi-format document processing. Split content intelligently across 50+ languages, visualize chunks in real-time, and handle various file types with flexible, context-aware chunking strategies.</p> <p>Key Features: - Sentence splitting: Multilingual text segmentation across 50+ languages - Semantic chunking: PlainTextChunker, DocumentChunker, and CodeChunker - Interactive visualization: Web-based chunk exploration and parameter tuning - Multi-format support: Text, code, PDF, DOCX, EPUB, and more - Batch processing: Memory-optimized generators with flexible error handling</p> <p>Modules:</p> <ul> <li> <code>base_chunker</code>           \u2013            <p>Base Chunker Abstract Class</p> </li> <li> <code>cli</code>           \u2013            </li> <li> <code>code_chunker</code>           \u2013            </li> <li> <code>common</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>exceptions</code>           \u2013            </li> <li> <code>plain_text_chunker</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> <li> <code>visualizer</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/#chunklet.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/#chunklet.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/#chunklet.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/#chunklet.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/#chunklet.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the `chunk` method, like `chunker.chunk(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/#chunklet.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/#chunklet.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p>"},{"location":"reference/chunklet/base_chunker/","title":"base_chunker","text":""},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker","title":"chunklet.base_chunker","text":"<p>Base Chunker Abstract Class</p> <p>Defines the interface for chunkers.</p> <p>Classes:</p> <ul> <li> <code>BaseChunker</code>           \u2013            <p>Abstract base class for chunkers.</p> </li> </ul>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker","title":"BaseChunker","text":"<pre><code>BaseChunker(verbose: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for chunkers.</p> <p>Defines the standard interface for chunking content into units.</p> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Process multiple items in parallel.</p> </li> <li> <code>chunk</code>             \u2013              <p>Extract chunks.</p> </li> <li> <code>log_info</code>             \u2013              <p>Log an info message if verbose is enabled.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>def __init__(self, verbose: bool = False):\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.batch_chunk","title":"batch_chunk  <code>abstractmethod</code>","text":"<pre><code>batch_chunk(*args, **kwargs) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple items in parallel.</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef batch_chunk(self, *args, **kwargs) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple items in parallel.\n\n    Yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.chunk","title":"chunk  <code>abstractmethod</code>","text":"<pre><code>chunk(*args, **kwargs) -&gt; list[Box]\n</code></pre> <p>Extract chunks.</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of chunks with content and metadata.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk(self, *args, **kwargs) -&gt; list[Box]:\n    \"\"\"\n    Extract chunks.\n\n    Returns:\n        list[Box]: List of chunks with content and metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.log_info","title":"log_info","text":"<pre><code>log_info(*args, **kwargs) -&gt; None\n</code></pre> <p>Log an info message if verbose is enabled.</p> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>def log_info(self, *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message if verbose is enabled.\"\"\"\n    if self.verbose:\n        logger.info(*args, **kwargs)\n</code></pre>"},{"location":"reference/chunklet/cli/","title":"cli","text":""},{"location":"reference/chunklet/cli/#chunklet.cli","title":"chunklet.cli","text":"<p>Functions:</p> <ul> <li> <code>chunk_command</code>             \u2013              <p>Chunk text or files based on specified parameters.</p> </li> <li> <code>split_command</code>             \u2013              <p>Split text or a single file into sentences</p> </li> <li> <code>visualize_command</code>             \u2013              <p>Start the web-based chunk visualizer interface for interactive text and code chunking.</p> </li> </ul>"},{"location":"reference/chunklet/cli/#chunklet.cli.chunk_command","title":"chunk_command","text":"<pre><code>chunk_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to chunk. If not provided, --source must be used.\",\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    code: bool = typer.Option(\n        False,\n        \"--code\",\n        help=\"Use CodeChunker for code files.\",\n    ),\n    doc: bool = typer.Option(\n        False,\n        \"--doc\",\n        help=\"Use DocumentChunker for document files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto)\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. (default: 20)\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. (default: 0)\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=\"A shell command to use for token counting. The command should take text as stdin and output the token count as a number.\",\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        help=\"Include metadata in the output. If --destination is a directory, metadata is saved as separate .json files; otherwise, it's included inline in the output.\",\n    ),\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking. (default: None, uses all available cores)\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n)\n</code></pre> <p>Chunk text or files based on specified parameters.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"chunk\", help=\"Chunk text or files based on specified parameters.\")\ndef chunk_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to chunk. If not provided, --source must be used.\"\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    # flags for chunker type\n    code: bool = typer.Option(False, \"--code\", help=\"Use CodeChunker for code files.\"),\n    doc: bool = typer.Option(\n        False, \"--doc\", help=\"Use DocumentChunker for document files.\"\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto)\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. (default: 20)\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. (default: 0)\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=(\n            \"A shell command to use for token counting. \"\n            \"The command should take text as stdin and output the token count as a number.\"\n        ),\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        help=(\n            \"Include metadata in the output. If --destination is a directory, \"\n            \"metadata is saved as separate .json files; otherwise, it's \"\n            \"included inline in the output.\"\n        ),\n    ),\n    # for Batching\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking. (default: None, uses all available cores)\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    # CodeChunker specific arguments\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n):\n    \"\"\"Chunk text or files based on specified parameters.\"\"\"\n    # --- Input validation logic ---\n    provided_inputs = [arg for arg in [text, source] if arg]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please provide a text, or use the --source option.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Please provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if doc and code:\n        typer.echo(\n            \"Error: Please specify either '--doc' or '--code', but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # --- Tokenizer setup ---\n    token_counter = None\n    if tokenizer_command:\n        token_counter = _create_external_tokenizer(tokenizer_command)\n\n    # Construct chunk_kwargs dynamically\n    chunk_kwargs = {\n        \"max_tokens\": max_tokens,\n        \"token_counter\": token_counter,\n    }\n\n    if code:\n        if CodeChunker is None:\n            typer.echo(\n                \"Error: CodeChunker dependencies not available.\\n\"\n                \"Please install with: pip install chunklet-py[code]\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        chunker_instance = CodeChunker(\n            verbose=verbose,\n            token_counter=token_counter,\n        )\n        chunk_kwargs.update(\n            {\n                \"max_lines\": max_lines,\n                \"max_functions\": max_functions,\n                \"docstring_mode\": docstring_mode,\n                \"strict\": strict,\n                \"include_comments\": include_comments,\n            }\n        )\n    else:\n        if text:\n            chunker_instance = PlainTextChunker(\n                verbose=verbose,\n                token_counter=token_counter,\n            )\n        else:\n            if DocumentChunker is None:\n                typer.echo(\n                    \"Error: DocumentChunker dependencies not available.\\n\"\n                    \"Please install with: pip install chunklet-py[document]\",\n                    err=True,\n                )\n                raise typer.Exit(code=1)\n\n            chunker_instance = DocumentChunker(\n                verbose=verbose,\n                token_counter=token_counter,\n            )\n        chunk_kwargs.update(\n            {\n                \"lang\": lang,\n                \"max_sentences\": max_sentences,\n                \"max_section_breaks\": max_section_breaks,\n                \"overlap_percent\": overlap_percent,\n                \"offset\": offset,\n            }\n        )\n\n    # --- Chunking logic ---\n    if text:\n        chunks = chunker_instance.chunk(\n            text=text,\n            **chunk_kwargs,\n        )\n    else:\n        file_paths = _extract_files(source)\n\n        if len(file_paths) == 1 and file_paths[0].suffix not in {\n            \".docx\",\n            \".epub\",\n            \".pdf\",\n            \".odt\",\n        }:\n            single_file = file_paths[0]\n            chunks = chunker_instance.chunk(\n                path=single_file,\n                **chunk_kwargs,\n            )\n        else:\n            # Batch input logic\n            chunks = chunker_instance.batch_chunk(\n                paths=file_paths,\n                n_jobs=n_jobs,\n                show_progress=True,\n                on_errors=on_errors,\n                **chunk_kwargs,\n            )\n\n    if not chunks:\n        typer.echo(\n            \"Warning: No chunks were generated. \"\n            \"This might be because the input was empty or did not contain any processable content.\",\n            err=True,\n        )\n        raise typer.Exit(code=0)\n\n    if destination:\n        _write_chunks(chunks, destination, metadata)\n    else:\n        _print_chunks(chunks, destination, metadata)\n</code></pre>"},{"location":"reference/chunklet/cli/#chunklet.cli.split_command","title":"split_command","text":"<pre><code>split_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to split. If not provided, --source must be used.\",\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n)\n</code></pre> <p>Split text or a single file into sentences</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"split\", help=\"Splits text or a single file into sentences.\")\ndef split_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to split. If not provided, --source must be used.\"\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n):\n    \"\"\"Split text or a single file into sentences\"\"\"\n    # Validation and Input Acquisition\n    provided_inputs = [arg for arg in [text, source] if arg is not None]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please use a text argument or the --source option.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if source:\n        # --- Source Constraints ---\n        if source.is_dir():\n            typer.echo(\n                f\"Error: Source path '{source}' cannot be a directory for the 'split' command.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n        if not source.is_file():\n            typer.echo(\n                f\"Error: Source path '{source}' not found or is not a file.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        try:\n            input_text = source.read_text(encoding=\"utf-8\")\n        except Exception as e:\n            typer.echo(f\"Error reading source file: {e}\", err=True)\n            raise typer.Exit(code=1)\n    else:\n        input_text = text\n\n    # --- Destination Constraint ---\n    if destination and destination.is_dir():\n        typer.echo(\n            f\"Error: Destination path '{destination}' cannot be a directory for the 'split' command.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Split Logic\n    splitter = SentenceSplitter(verbose=verbose)\n    lang_detected, confidence = splitter.detected_top_language(input_text)\n    sentences = splitter.split(input_text, lang=lang or lang_detected)\n\n    # Output Handling\n    if destination:\n        output_str = \"\\n\".join(sentences)\n        source_display = f\"from {source.name}\" if source else \"(from stdin)\"\n\n        try:\n            destination.write_text(output_str, encoding=\"utf-8\")\n            typer.echo(\n                f\"Successfully split and wrote {len(sentences)} sentences \"\n                f\"{source_display} to {destination} (Confidence: {confidence})\",\n                err=True,\n            )\n        except Exception as e:\n            typer.echo(f\"Error writing to destination file: {e}\", err=True)\n            raise typer.Exit(code=1)\n    else:\n        source_display = f\"Source: {source.name}\" if source else \"Source: stdin\"\n\n        typer.echo(\n            f\"--- Sentences ({len(sentences)}): \"\n            f\" [{source_display} | Lang: {lang.upper()} | Confidence: {confidence}] ---\"\n        )\n\n        for sentence in sentences:\n            typer.echo(sentence)\n</code></pre>"},{"location":"reference/chunklet/cli/#chunklet.cli.visualize_command","title":"visualize_command","text":"<pre><code>visualize_command(\n    host: str = typer.Option(\n        \"127.0.0.1\",\n        \"--host\",\n        help=\"Host IP to bind the visualizer server. (default: 127.0.0.1)\",\n    ),\n    port: int = typer.Option(\n        8000,\n        \"--port\",\n        \"-p\",\n        help=\"Port number to run the visualizer server. (default: 8000)\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=\"A shell command to use for token counting in the visualizer. The command should take text as stdin and output the token count as a number.\",\n    ),\n    headless: bool = typer.Option(\n        False,\n        \"--headless\",\n        help=\"Run visualizer in headless mode (don't open browser automatically).\",\n    ),\n)\n</code></pre> <p>Start the web-based chunk visualizer interface for interactive text and code chunking.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(\n    name=\"visualize\",\n    help=\"Start the web-based chunk visualizer interface for interactive text and code chunking.\",\n)\ndef visualize_command(\n    host: str = typer.Option(\n        \"127.0.0.1\",\n        \"--host\",\n        help=\"Host IP to bind the visualizer server. (default: 127.0.0.1)\",\n    ),\n    port: int = typer.Option(\n        8000,\n        \"--port\",\n        \"-p\",\n        help=\"Port number to run the visualizer server. (default: 8000)\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=(\n            \"A shell command to use for token counting in the visualizer. \"\n            \"The command should take text as stdin and output the token count as a number.\"\n        ),\n    ),\n    headless: bool = typer.Option(\n        False,\n        \"--headless\",\n        help=\"Run visualizer in headless mode (don't open browser automatically).\",\n    ),\n):\n    \"\"\"\n    Start the web-based chunk visualizer interface for interactive text and code chunking.\n    \"\"\"\n    if Visualizer is None:\n        typer.echo(\n            \"Error: Visualization dependencies not available.\\n\"\n            \"Please install with: pip install chunklet-py[visualization]\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Check if port is available\n    url = f\"http://{host}:{port}\"\n    if not _check_port_available(host, port):\n        typer.echo(f\"Error: Port {port} is already in use on {host}\", err=True)\n        typer.echo(\"Options:\", err=True)\n        typer.echo(f\"  1. Stop the process currently occupying {url}\", err=True)\n        typer.echo(\n            \"  2. Use a different port: chunklet visualize --port &lt;different_port&gt;\",\n            err=True,\n        )\n        typer.echo(\n            \"  3. Find the PID:\\n\"\n            f\"     - Linux: 'ss -tunlp | grep :{port}' or 'fuser {port}/tcp'\\n\"\n            f\"     - Windows: 'netstat -ano | findstr :{port}'\\n\"\n            f\"     - Mac: 'lsof -i :{port}'\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Create token counter if tokenizer command provided\n    token_counter = None\n    if tokenizer_command:\n        token_counter = _create_external_tokenizer(tokenizer_command)\n\n    # Start the visualizer\n    visualizer = Visualizer(host=host, port=port, token_counter=token_counter)\n\n    typer.echo(\"Starting Chunklet Visualizer...\")\n    typer.echo(f\"URL: {url}\")\n    typer.echo(\"Press Ctrl+C to stop the server\")\n\n    if not headless:\n        import webbrowser\n\n        try:\n            webbrowser.open(url)\n            typer.echo(\"Opened in default browser\")\n        except Exception as e:\n            typer.echo(f\"Could not open browser: {e}\", err=True)\n\n    try:\n        visualizer.serve()\n    except KeyboardInterrupt:\n        typer.echo(\"\\nVisualizer stopped.\")\n    except Exception as e:\n        typer.echo(f\"Error starting visualizer: {e}\", err=True)\n        raise typer.Exit(code=1)\n</code></pre>"},{"location":"reference/chunklet/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/#chunklet.code_chunker","title":"chunklet.code_chunker","text":"<p>Modules:</p> <ul> <li> <code>code_chunker</code>           \u2013            <p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> </li> <li> <code>helpers</code>           \u2013            </li> <li> <code>patterns</code>           \u2013            <p>regex_patterns.py</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/","title":"_code_structure_extractor","text":""},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor","title":"chunklet.code_chunker._code_structure_extractor","text":"<p>Code Structure Extractor</p> <p>Internal module for extracting code structures from source code. Split from CodeChunker for modularity.</p> <p>Classes:</p> <ul> <li> <code>CodeStructureExtractor</code>           \u2013            <p>Internal class for extracting structural units from source code.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor","title":"CodeStructureExtractor","text":"<pre><code>CodeStructureExtractor(verbose: bool = False)\n</code></pre> <p>Internal class for extracting structural units from source code.</p> <p>Methods:</p> <ul> <li> <code>extract_code_structure</code>             \u2013              <p>Preprocess and parse source into individual snippet boxes.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/_code_structure_extractor.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure","title":"extract_code_structure","text":"<pre><code>extract_code_structure(\n    source: str | Path,\n    include_comments: bool,\n    docstring_mode: str,\n) -&gt; tuple[list[dict], tuple[int, ...]]\n</code></pre> <p>Preprocess and parse source into individual snippet boxes.</p> <p>This function-first extraction identifies functions as primary units while implicitly handling other structures within the function context.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[dict], tuple[int, ...]]</code>           \u2013            <p>tuple[list[dict], tuple[int, ...]]: A tuple containing the list of extracted code structure boxes and the line lengths.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/_code_structure_extractor.py</code> <pre><code>def extract_code_structure(\n    self,\n    source: str | Path,\n    include_comments: bool,\n    docstring_mode: str,\n) -&gt; tuple[list[dict], tuple[int, ...]]:\n    \"\"\"\n    Preprocess and parse source into individual snippet boxes.\n\n    This function-first extraction identifies functions as primary units\n    while implicitly handling other structures within the function context.\n\n    Args:\n        source (str | Path): Raw code string or Path to source file.\n        include_comments (bool): Whether to include comments in output.\n        docstring_mode (Literal[\"summary\", \"all\", \"excluded\"]): How to handle docstrings.\n\n    Returns:\n        tuple[list[dict], tuple[int, ...]]: A tuple containing the list of extracted code structure boxes and the line lengths.\n    \"\"\"\n    source_code = self._read_source(source)\n    if not source_code:\n        return [], ()\n\n    source_code, cumulative_lengths = self._preprocess(\n        source_code, include_comments, docstring_mode\n    )\n\n    state = {\n        \"curr_struct\": [],\n        \"last_indent\": 0,\n        \"inside_func\": False,\n        \"snippet_dicts\": [],\n    }\n    buffer = defaultdict(list)\n\n    for line_no, line in enumerate(source_code.splitlines(), start=1):\n        indent_level = len(line) - len(line.lstrip())\n\n        # Detect annotated lines\n        matched = re.search(r\"\\(-- ([A-Z]+) --&gt;\\) \", line)\n        if matched:\n            self._handle_annotated_line(\n                line=line,\n                line_no=line_no,\n                indent_level=indent_level,\n                matched=matched,\n                buffer=buffer,\n                state=state,\n            )\n            continue\n\n        # Manage block accumulation\n\n        func_start = FUNCTION_DECLARATION.match(line)\n        func_start = func_start.group(0) if func_start else None\n\n        self._handle_block_start(\n            line=line,\n            indent_level=indent_level,\n            buffer=buffer,\n            state=state,\n            source=source,\n            func_start=func_start,\n        )\n\n        if not state[\"curr_struct\"]:  # Fresh block\n            state[\"curr_struct\"] = [\n                CodeLine(\n                    line_no,\n                    line,\n                    indent_level,\n                    func_start,\n                )\n            ]\n            continue\n\n        if (\n            line.strip()\n            and indent_level &lt;= state[\"last_indent\"]\n            and not (OPENER.match(line) or CLOSURE.match(line))\n        ):  # Block end\n            self._flush_snippet(\n                state[\"curr_struct\"], state[\"snippet_dicts\"], buffer\n            )\n            state[\"last_indent\"] = 0\n            state[\"inside_func\"] = False\n\n        state[\"curr_struct\"].append(\n            CodeLine(line_no, line, indent_level, func_start)\n        )\n\n    # Append last snippet\n    if state[\"curr_struct\"]:\n        self._flush_snippet(state[\"curr_struct\"], state[\"snippet_dicts\"], buffer)\n\n    snippet_dicts = self._post_processing(state[\"snippet_dicts\"])\n    if self.verbose:\n        logger.info(\n            \"Extracted {} structural blocks from source\", len(snippet_dicts)\n        )\n\n    return snippet_dicts, cumulative_lengths\n</code></pre>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>Raw code string or Path to source file.</p>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to include comments in output.</p>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal[summary, all, excluded]</code>)           \u2013            <p>How to handle docstrings.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker","title":"chunklet.code_chunker.code_chunker","text":"<p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> <p>Language-Agnostic Code Chunking Utility</p> <p>This module provides a robust, convention-aware engine for segmenting source code into semantic units (\"chunks\") such as functions, classes, namespaces, and logical blocks. Unlike purely heuristic or grammar-dependent parsers, the <code>CodeChunker</code> relies on anchored, multi-language regex patterns and indentation rules to identify structures consistently across a variety of programming languages.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker--limitations","title":"Limitations","text":"<p><code>CodeChunker</code> assumes syntactically conventional code. Highly obfuscated, minified, or macro-generated sources may not fully respect its boundary patterns, though such cases fall outside its intended domain.</p> Inspired by <ul> <li>Camel.utils.chunker.CodeChunker (@ CAMEL-AI.org)</li> <li>code-chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> <p>Classes:</p> <ul> <li> <code>CodeChunker</code>           \u2013            <p>Language-agnostic code chunking utility for semantic code segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker","title":"CodeChunker","text":"<pre><code>CodeChunker(\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseChunker</code></p> <p>Language-agnostic code chunking utility for semantic code segmentation.</p> <p>Extracts structural units (functions, classes, namespaces) from source code across multiple programming languages using pattern-based detection and token-aware segmentation.</p> Key Features <ul> <li>Cross-language support (Python, C/C++, Java, C#, JavaScript, Go, etc.)</li> <li>Structural analysis with namespace hierarchy tracking</li> <li>Configurable token limits with strict/lenient overflow handling</li> <li>Flexible docstring and comment processing modes</li> <li>Accurate line number preservation and source tracking</li> <li>Parallel batch processing for multiple files</li> <li>Comprehensive logging and progress tracking</li> </ul> <p>Initialize the CodeChunker with optional token counter and verbosity control.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Process multiple source files or code strings in parallel.</p> </li> <li> <code>chunk</code>             \u2013              <p>Extract semantic code chunks from source using multi-dimensional analysis.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbose setting.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize the CodeChunker with optional token counter and verbosity control.\n\n    Args:\n        verbose (bool): Enable verbose logging.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n    \"\"\"\n    self.token_counter = token_counter\n    self._verbose = verbose\n    self.extractor = CodeStructureExtractor(verbose=self._verbose)\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbose setting.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    sources: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple source files or code strings in parallel.</p> <p>Leverages multiprocessing to efficiently chunk multiple code sources, applying consistent chunking rules across all inputs.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata. Includes: - content (str): Code content - tree (str): Namespace hierarchy - start_line (int): Starting line in original source - end_line (int): Ending line in original source - span (tuple[int, int]): Character-level span (start and end offsets) in the original source. - source_path (str): Source file path or \"N/A\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid input parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    sources: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple source files or code strings in parallel.\n\n    Leverages multiprocessing to efficiently chunk multiple code sources,\n    applying consistent chunking rules across all inputs.\n\n    Args:\n        sources (restricted_iterable[str | Path]): A restricted iterable of file paths or raw code strings to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable | None): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n        n_jobs (int | None): Number of parallel workers. Uses all available CPUs if None.\n        show_progress (bool): Display progress bar during processing. Defaults to True.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to 'raise'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n            Includes:\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path or \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid input parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=sources,\n        iterable_name=\"sources\",\n        separator=separator,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(sources)","title":"<code>sources</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of file paths or raw code strings to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy: - \"summary\": Include only first line of docstrings - \"all\": Include complete docstrings - \"excluded\": Remove all docstrings Defaults to \"all\"</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers. Uses all available CPUs if None.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Display progress bar during processing. Defaults to True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True\n) -&gt; list[Box]\n</code></pre> <p>Extract semantic code chunks from source using multi-dimensional analysis.</p> <p>Processes source code by identifying structural boundaries (functions, classes, namespaces) and grouping content based on multiple constraints including tokens, lines, and logical units while preserving semantic coherence.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of code chunks with metadata. Each Box contains: - content (str): Code content - tree (str): Namespace hierarchy - start_line (int): Starting line in original source - end_line (int): Ending line in original source - span (tuple[int, int]): Character-level span (start and end offsets) in the original source. - source_path (str): Source file path or \"N/A\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid configuration parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n) -&gt; list[Box]:\n    \"\"\"\n    Extract semantic code chunks from source using multi-dimensional analysis.\n\n    Processes source code by identifying structural boundaries (functions, classes,\n    namespaces) and grouping content based on multiple constraints including\n    tokens, lines, and logical units while preserving semantic coherence.\n\n    Args:\n        source (str | Path): Raw code string or file path to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable, optional): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n\n    Returns:\n        list[Box]: List of code chunks with metadata. Each Box contains:\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path or \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid configuration parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    self._validate_constraints(max_tokens, max_lines, max_functions, token_counter)\n\n    # Adjust limits for internal use\n    if max_tokens is None:\n        max_tokens = sys.maxsize\n    if max_lines is None:\n        max_lines = sys.maxsize\n    if max_functions is None:\n        max_functions = sys.maxsize\n\n    token_counter = token_counter or self.token_counter\n\n    if isinstance(source, str) and not source.strip():\n        self.log_info(\"Input source is empty. Returning empty list.\")\n        return []\n\n    self.log_info(\n        \"Starting chunk processing for {}\",\n        (\n            f\"source: {source}\"\n            if isinstance(source, Path)\n            or (isinstance(source, str) and is_path_like(source))\n            else f\"code starting with:\\n```\\n{source[:100]}...\\n```\\n\"\n        ),\n    )\n\n    snippet_dicts, cumulative_lengths = self.extractor.extract_code_structure(\n        source, include_comments, docstring_mode\n    )\n\n    result_chunks = self._group_by_chunk(\n        snippet_dicts=snippet_dicts,\n        cumulative_lengths=cumulative_lengths,\n        token_counter=token_counter,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        strict=strict,\n        source=source,\n    )\n\n    self.log_info(\n        \"Generated {} chunk(s) for the {}\",\n        len(result_chunks),\n        (\n            f\"source: {source}\"\n            if isinstance(source, Path)\n            or (isinstance(source, str) and is_path_like(source))\n            else f\"code starting with:\\n```\\n{source[:100]}...\\n```\\n\"\n        ),\n    )\n\n    return result_chunks\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>Raw code string or file path to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy: - \"summary\": Include only first line of docstrings - \"all\": Include complete docstrings - \"excluded\": Remove all docstrings Defaults to \"all\"</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/helpers/","title":"helpers","text":""},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers","title":"chunklet.code_chunker.helpers","text":"<p>Functions:</p> <ul> <li> <code>is_binary_file</code>             \u2013              <p>Determine whether a file is binary or text.</p> </li> <li> <code>is_python_code</code>             \u2013              <p>Check if a source is written in Python.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_binary_file","title":"is_binary_file","text":"<pre><code>is_binary_file(file_path: str | Path) -&gt; bool\n</code></pre> <p>Determine whether a file is binary or text.</p> <p>First tries to guess the file type based on its MIME type derived from the file extension. If MIME type is unavailable or ambiguous, reads the first 1024 bytes of the file and checks for null bytes (<code>b'\u0000'</code>), which indicate binary content.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the file is likely binary, False if text.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/helpers.py</code> <pre><code>@validate_input\ndef is_binary_file(file_path: str | Path) -&gt; bool:\n    \"\"\"\n    Determine whether a file is binary or text.\n\n    First tries to guess the file type based on its MIME type derived from\n    the file extension. If MIME type is unavailable or ambiguous, reads the\n    first 1024 bytes of the file and checks for null bytes (`b'\\0'`), which\n    indicate binary content.\n\n    Args:\n        file_path (str | Path): Path to the file.\n\n    Returns:\n        bool: True if the file is likely binary, False if text.\n    \"\"\"\n    file_path = Path(file_path)\n    mime_type, _ = mimetypes.guess_type(file_path)\n    if mime_type:\n        return not mime_type.startswith(\"text\")\n\n    with open(file_path, \"rb\") as f:\n        chunk = f.read(1024)\n        return b\"\\0\" in chunk\n</code></pre>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_binary_file(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the file.</p>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_python_code","title":"is_python_code","text":"<pre><code>is_python_code(source: str | Path) -&gt; bool\n</code></pre> <p>Check if a source is written in Python.</p> <p>This function uses multiple indicators, prioritizing syntactic validity via the Abstract Syntax Tree (AST) parser for maximum confidence.</p> Indicators used <ul> <li>File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).</li> <li>Shebang line detection (e.g., \"#!/usr/bin/python\").</li> <li>Definitive syntax check using Python's <code>ast.parse()</code>.</li> <li>Fallback heuristic via Pygments lexer guessing.</li> </ul> Note <p>The function is definitive for complete, syntactically correct code blocks. It falls back to a Pygments heuristic only for short, incomplete, or ambiguous code snippets that fail AST parsing.</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the source is written in Python.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/helpers.py</code> <pre><code>@validate_input\ndef is_python_code(source: str | Path) -&gt; bool:\n    \"\"\"\n    Check if a source is written in Python.\n\n    This function uses multiple indicators, prioritizing syntactic validity\n    via the Abstract Syntax Tree (AST) parser for maximum confidence.\n\n    Indicators used:\n      - File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).\n      - Shebang line detection (e.g., \"#!/usr/bin/python\").\n      - Definitive syntax check using Python's `ast.parse()`.\n      - Fallback heuristic via Pygments lexer guessing.\n\n    Note:\n        The function is definitive for complete, syntactically correct code blocks.\n        It falls back to a Pygments heuristic only for short, incomplete, or\n        ambiguous code snippets that fail AST parsing.\n\n    Args:\n        source (str | Path): raw code string or Path to source file to check.\n\n    Returns:\n        bool: True if the source is written in Python.\n    \"\"\"\n    # Path-based check\n    if isinstance(source, Path) or (isinstance(source, str) and is_path_like(source)):\n        path = Path(source)\n        return path.suffix.lower() in {\".py\", \".pyi\", \".pyx\", \".pyw\"}\n\n    if isinstance(source, str):\n        # Shebang line check\n        if re.match(r\"#!/usr/bin/(env\\s+)?python\", source.strip()):\n            return True\n\n        # Definitive syntactic check (Highest confidence)\n        try:\n            ast.parse(source)\n            # If parsing succeeds, it's definitely Python code\n            return True\n        except Exception:  # noqa: S110\n            # If fails, it might still be Python code (e.g., incomplete snippet), so continue with heuristics\n            pass\n\n    # Pygments heuristic (Lowest confidence, last resort)\n    try:\n        lexer = guess_lexer(source)\n        return lexer.name.lower() == \"python\"\n    except ClassNotFound:\n        return False\n</code></pre>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_python_code(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>raw code string or Path to source file to check.</p>"},{"location":"reference/chunklet/code_chunker/patterns/","title":"patterns","text":""},{"location":"reference/chunklet/code_chunker/patterns/#chunklet.code_chunker.patterns","title":"chunklet.code_chunker.patterns","text":"<p>regex_patterns.py</p> <p>Written by: Speedyk-005 Copyright 2025 License: MIT</p> <p>This module contains regular expressions for chunking and parsing source code across multiple programming languages. The patterns are designed to match:</p> <ul> <li>Single-line comments (Python, C/C++, Java, JavaScript, Lisp, etc.)</li> <li>Multi-line comments / docstrings (Python, C-style, Ruby, Lisp, etc.)</li> <li>Function or method definitions across various languages</li> <li>Namespaces, classes, modules, and interfaces</li> <li>Annotations / decorators (Python, C#, Java)</li> <li>Block-ending indicators ('}' or 'end')</li> </ul> <p>These regexes can be imported into a chunker or parser to identify logical sections of code for semantic analysis, tokenization, or processing.</p> Note <ul> <li>re.M = multiline (^,$ match each line)</li> <li>re.S = DOTALL (. matches newline)</li> </ul>"},{"location":"reference/chunklet/common/","title":"common","text":""},{"location":"reference/chunklet/common/#chunklet.common","title":"chunklet.common","text":"<p>Modules:</p> <ul> <li> <code>batch_runner</code>           \u2013            </li> <li> <code>path_utils</code>           \u2013            </li> <li> <code>token_utils</code>           \u2013            </li> <li> <code>validation</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/","title":"batch_runner","text":""},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner","title":"chunklet.common.batch_runner","text":"<p>Functions:</p> <ul> <li> <code>capture_result_and_exception</code>             \u2013              <p>Decorator to capture result and exception from a function call.</p> </li> <li> <code>run_in_batch</code>             \u2013              <p>Processes a batch of items in parallel using multiprocessing.</p> </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.capture_result_and_exception","title":"capture_result_and_exception","text":"<pre><code>capture_result_and_exception(func)\n</code></pre> <p>Decorator to capture result and exception from a function call.</p> Source code in <code>src/chunklet/common/batch_runner.py</code> <pre><code>def capture_result_and_exception(func):\n    \"\"\"Decorator to capture result and exception from a function call.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        try:\n            res = func(*args, **kwargs)\n            return res, None\n        except Exception as e:\n            return None, e\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch","title":"run_in_batch","text":"<pre><code>run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of items in parallel using multiprocessing. Splits the iterable into chunks and executes the function on each.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> Source code in <code>src/chunklet/common/batch_runner.py</code> <pre><code>def run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of items in parallel using multiprocessing.\n    Splits the iterable into chunks and executes the function on each.\n\n    Args:\n        func (Callable): The function to call for each argument.\n        iterable_of_args (Iterable): An iterable of inputs to process.\n        iterable_name: Name of the iterable. needed for logging and exception message.\n        n_jobs (int | None): Number of parallel workers to use.\n            If None, uses all available CPUs. Must be &gt;= 1 if specified.\n        show_progress (bool): Whether to display a progress bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to \"raise\".\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        verbose (bool): Whether to enable verbose logging.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n    \"\"\"\n    from mpire import WorkerPool\n\n    total, iterable_of_args = safely_count_iterable(iterable_name, iterable_of_args)\n\n    if verbose:\n        logger.info(\"Starting batch chunking for {} items.\", total)\n\n    if total == 0:\n        if verbose:\n            logger.info(\"Input {} is empty. Returning empty iterator.\", iterable_name)\n        return iter([])\n\n    failed_count = 0\n    try:\n        with WorkerPool(n_jobs=n_jobs) as pool:\n            imap_func = pool.imap if separator is not None else pool.imap_unordered\n\n            progress_bar_options = {\n                \"bar_format\": \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]\",\n                \"desc\": \"Chunking ...\",\n            }\n\n            task_iter = imap_func(\n                capture_result_and_exception(func),\n                iterable_of_args,\n                iterable_len=total,\n                progress_bar=show_progress,\n                progress_bar_options=progress_bar_options,\n            )\n\n            for res, error in task_iter:\n                if error:\n                    failed_count += 1\n                    if on_errors == \"raise\":\n                        raise error\n                    elif on_errors == \"break\":\n                        logger.error(\n                            \"A task for {} failed. Returning partial results.\\nReason: {}\",\n                            iterable_name,\n                            error,\n                        )\n                        break\n\n                    #  Else: skip\n                    logger.warning(\"Skipping a failed task.\\nReason: {}\", error)\n                    continue\n\n                yield from res\n\n                if separator is not None:\n                    yield separator\n\n    finally:\n        if verbose:\n            logger.info(\n                \"Batch processing completed. {}/{} items processed successfully.\",\n                total - failed_count,\n                total,\n            )\n</code></pre>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(func)","title":"<code>func</code>","text":"(<code>Callable</code>)           \u2013            <p>The function to call for each argument.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_of_args)","title":"<code>iterable_of_args</code>","text":"(<code>Iterable</code>)           \u2013            <p>An iterable of inputs to process.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_name)","title":"<code>iterable_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the iterable. needed for logging and exception message.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display a progress bar.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to \"raise\".</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enable verbose logging.</p>"},{"location":"reference/chunklet/common/path_utils/","title":"path_utils","text":""},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils","title":"chunklet.common.path_utils","text":"<p>Functions:</p> <ul> <li> <code>is_path_like</code>             \u2013              <p>Check if a string looks like a filesystem path (file or folder),</p> </li> </ul>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like","title":"is_path_like","text":"<pre><code>is_path_like(text: str) -&gt; bool\n</code></pre> <p>Check if a string looks like a filesystem path (file or folder), including Unix/Windows paths, hidden files, and scripts without extensions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if string appears to be a filesystem path.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\nTrue\n&gt;&gt;&gt; is_path_like(\"C:\\Users\\User\\file.pdf\")\nTrue\n&gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\nTrue\n&gt;&gt;&gt; is_path_like(\".hidden_file\")\nTrue\n&gt;&gt;&gt; is_path_like(\"no_extension_script\")\nTrue\n&gt;&gt;&gt; is_path_like(\"path/with/newline\\nchar\")\nFalse\n&gt;&gt;&gt; is_path_like(\"string_with_null_byte\\x00\")\nFalse\n</code></pre> Source code in <code>src/chunklet/common/path_utils.py</code> <pre><code>@validate_input\ndef is_path_like(text: str) -&gt; bool:\n    \"\"\"\n    Check if a string looks like a filesystem path (file or folder),\n    including Unix/Windows paths, hidden files, and scripts without extensions.\n\n    Args:\n        text (str): text to check.\n\n    Returns:\n        bool: True if string appears to be a filesystem path.\n\n    Examples:\n        &gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\n        True\n        &gt;&gt;&gt; is_path_like(\"C:\\\\Users\\\\User\\\\file.pdf\")\n        True\n        &gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\n        True\n        &gt;&gt;&gt; is_path_like(\".hidden_file\")\n        True\n        &gt;&gt;&gt; is_path_like(\"no_extension_script\")\n        True\n        &gt;&gt;&gt; is_path_like(\"path/with/newline\\\\nchar\")\n        False\n        &gt;&gt;&gt; is_path_like(\"string_with_null_byte\\\\x00\")\n        False\n    \"\"\"\n    if not text or \"\\n\" in text or \"\\0\" in text:\n        return False\n    if sys.platform == \"win32\" and any(c in text for c in '&lt;&gt;:\"|?*'):\n        return False\n\n    try:\n        # Attempt to call is_file() to trigger OS-level path validation,\n        # especially for path length.\n        Path(text).is_file()\n    except OSError as e:\n        # If an OSError occurs, check if it's specifically due to the name being too long.\n        if e.errno == errno.ENAMETOOLONG:\n            return False\n        else:\n            # For other OSErrors (e.g., permission denied, invalid characters not caught by initial checks),\n            # we let the regex check proceed, as the focus is on structural validity, not existence or access.\n            pass\n\n    return bool(PATH_PATTERN.match(text))\n</code></pre>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>text to check.</p>"},{"location":"reference/chunklet/common/token_utils/","title":"token_utils","text":""},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils","title":"chunklet.common.token_utils","text":"<p>Functions:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count tokens in a string using a provided token counting function.</p> </li> </ul>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens","title":"count_tokens  <code>cached</code>","text":"<pre><code>count_tokens(\n    text: str, token_counter: Callable[[str], int]\n) -&gt; int\n</code></pre> <p>Count tokens in a string using a provided token counting function.</p> <p>Wraps the token counting function with error handling. Ensures the returned value is numeric and converts it to an integer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Number of tokens.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n...     return len(text.split())\n&gt;&gt;&gt; text = \"This is a sample sentence.\"\n&gt;&gt;&gt; count_tokens(text, simple_word_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n...     return len(text)\n&gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with a failing token counter\n&gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n...     raise ValueError(\"Something went wrong!\")\n&gt;&gt;&gt; try:\n...     count_tokens(\"test\", failing_counter)\n... except CallbackError as e:\n...     print(e)\nToken counter failed while processing text starting with: 'test...'.\n\ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\nDetails: Something went wrong!\n</code></pre> Source code in <code>src/chunklet/common/token_utils.py</code> <pre><code>@lru_cache(maxsize=1024)\ndef count_tokens(text: str, token_counter: Callable[[str], int]) -&gt; int:\n    \"\"\"\n    Count tokens in a string using a provided token counting function.\n\n    Wraps the token counting function with error handling. Ensures the returned\n    value is numeric and converts it to an integer.\n\n    Args:\n        text (str): Text to count tokens in.\n        token_counter (Callable[[str], int]): Function that returns the number of tokens.\n\n    Returns:\n        int: Number of tokens.\n\n    Raises:\n        CallbackError: If the token counter fails or returns an invalid type.\n\n    Examples:\n        &gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n        ...     return len(text.split())\n        &gt;&gt;&gt; text = \"This is a sample sentence.\"\n        &gt;&gt;&gt; count_tokens(text, simple_word_counter)\n        5\n\n        &gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n        ...     return len(text)\n        &gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n        5\n\n        &gt;&gt;&gt; # Example with a failing token counter\n        &gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n        ...     raise ValueError(\"Something went wrong!\")\n        &gt;&gt;&gt; try:\n        ...     count_tokens(\"test\", failing_counter)\n        ... except CallbackError as e:\n        ...     print(e)\n        Token counter failed while processing text starting with: 'test...'.\n        \ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\n        Details: Something went wrong!\n    \"\"\"\n    try:\n        token_count = token_counter(text)\n        if isinstance(token_count, (int, float)):\n            return int(token_count)\n        raise CallbackError(\n            f\"Token counter returned invalid type ({type(token_count).__name__}) \"\n            f\"for text starting with: '{text[:100]}'\"\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"Token counter failed while processing text starting with: '{text[:100]}...'.\\n\"\n            \"\ud83d\udca1 Hint: Please ensure the token counter function handles \"\n            f\"all edge cases and returns an integer. \\nDetails: {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>Text to count tokens in.</p>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>)           \u2013            <p>Function that returns the number of tokens.</p>"},{"location":"reference/chunklet/common/validation/","title":"validation","text":""},{"location":"reference/chunklet/common/validation/#chunklet.common.validation","title":"chunklet.common.validation","text":"<p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>restricted_iterable</code>             \u2013              <p>Creates a Pydantic Annotated type that represents a RestrictedIterable</p> </li> <li> <code>safely_count_iterable</code>             \u2013              <p>Counts elements in an iterable while preserving its state and forcing validation.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.restricted_iterable","title":"restricted_iterable","text":"<pre><code>restricted_iterable(*hints: Any) -&gt; Any\n</code></pre> <p>Creates a Pydantic Annotated type that represents a RestrictedIterable containing the specified hints (*hints), and applies a PlainValidator to reject str input.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def restricted_iterable(*hints: Any) -&gt; Any:\n    \"\"\"\n    Creates a Pydantic Annotated type that represents a RestrictedIterable\n    containing the specified hints (*hints), and applies a PlainValidator\n    to reject str input.\n    \"\"\"\n\n    def enforce_non_string(v: Any) -&gt; Any:\n        if isinstance(v, str):\n            # Pydantic-Core is sometimes pickier; using ValueError often works better\n            # with external validators than a raw TypeError\n            # Sliced to avoid overflowing screen\n            input_val = v if len(v) &lt; 500 else v[:500] + \"...\"\n            raise ValueError(\n                f\"Input cannot be a string.\\n  Found: (input={input_val!r}, type=str)\"\n            )\n        return v\n\n    ItemUnion = Union[hints] if len(hints) == 1 else hints[0]\n\n    # Build the Full Container Type\n    TargetType = (\n        list[ItemUnion]\n        | tuple[ItemUnion, ...]\n        | set[ItemUnion]\n        | frozenset[ItemUnion]\n        | Generator[ItemUnion, None, None]\n    )\n\n    # Create the Annotated Type\n    return Annotated[TargetType, PlainValidator(enforce_non_string)]\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable","title":"safely_count_iterable","text":"<pre><code>safely_count_iterable(\n    name: str, iterable: Iterable\n) -&gt; tuple[int, Iterable]\n</code></pre> <p>Counts elements in an iterable while preserving its state and forcing validation.</p> <p>If the input is an Iterator, it is duplicated using <code>itertools.tee</code> to prevent consumption during counting. The iteration simultaneously triggers any underlying Pydantic item validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[int, Iterable]</code>           \u2013            <p>tuple[int, Iterable]: The element count and the original (or preserved)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any element fails validation during the counting process.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With a list\n&gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 5\n&gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\nOriginal list is preserved: [1, 2, 3, 4, 5]\n</code></pre> <pre><code>&gt;&gt;&gt; # With an iterator (generator)\n&gt;&gt;&gt; my_iterator = (x for x in range(10))\n&gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 10\n&gt;&gt;&gt; # The iterator is preserved and can still be consumed\n&gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\nSum of preserved iterator: 45\n</code></pre> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>@validate_input\ndef safely_count_iterable(name: str, iterable: Iterable) -&gt; tuple[int, Iterable]:\n    \"\"\"\n    Counts elements in an iterable while preserving its state and forcing validation.\n\n    If the input is an Iterator, it is duplicated using `itertools.tee` to prevent\n    consumption during counting. The iteration simultaneously triggers any\n    underlying Pydantic item validation.\n\n    Args:\n        name (str): Descriptive name for the iterable (used in error context).\n        iterable (Iterable): The iterable or iterator to count and validate.\n\n    Returns:\n        tuple[int, Iterable]: The element count and the original (or preserved)\n\n    Raises:\n        InvalidInputError: If any element fails validation during the counting process.\n\n    Examples:\n        &gt;&gt;&gt; # With a list\n        &gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n        &gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 5\n        &gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\n        Original list is preserved: [1, 2, 3, 4, 5]\n\n        &gt;&gt;&gt; # With an iterator (generator)\n        &gt;&gt;&gt; my_iterator = (x for x in range(10))\n        &gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 10\n        &gt;&gt;&gt; # The iterator is preserved and can still be consumed\n        &gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\n        Sum of preserved iterator: 45\n    \"\"\"\n    try:  # If pydantic wrap it as ValidatorIterator object\n        # Tee if it's an iterator\n        if isinstance(iterable, Iterator):\n            iterable, copy_iterable = tee(iterable)\n            count = ilen(copy_iterable)\n        else:\n            count = len(iterable)\n    except ValidationError as e:\n        e.subtitle = name  # to be less generic\n        e.hint = \"\ud83d\udca1 Hint: Ensure all elements in the iterable are valid.\"\n        raise InvalidInputError(pretty_errors(e)) from None\n\n    return count, iterable\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Descriptive name for the iterable (used in error context).</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(iterable)","title":"<code>iterable</code>","text":"(<code>Iterable</code>)           \u2013            <p>The iterable or iterator to count and validate.</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker","title":"chunklet.document_chunker","text":"<p>Modules:</p> <ul> <li> <code>converters</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>processors</code>           \u2013            </li> <li> <code>registry</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> <ol> <li>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/","title":"converters","text":""},{"location":"reference/chunklet/document_chunker/converters/#chunklet.document_chunker.converters","title":"chunklet.document_chunker.converters","text":"<p>Modules:</p> <ul> <li> <code>html_2_md</code>           \u2013            </li> <li> <code>latex_2_md</code>           \u2013            </li> <li> <code>rst_2_md</code>           \u2013            </li> <li> <code>table_2_md</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/","title":"html_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md","title":"chunklet.document_chunker.converters.html_2_md","text":"<p>Functions:</p> <ul> <li> <code>html_to_md</code>             \u2013              <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md","title":"html_to_md","text":"<pre><code>html_to_md(\n    file_path: str | Path = None,\n    raw_text: str | None = None,\n    max_url_length: int = 150,\n) -&gt; str\n</code></pre> <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/html_2_md.py</code> <pre><code>def html_to_md(\n    file_path: str | Path = None, raw_text: str | None = None, max_url_length: int = 150\n) -&gt; str:\n    \"\"\"\n    Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.\n\n    Args:\n        file_path (str | Path): Path to the html file.\n        raw_text (str, optional): Raw HTML text. If both file_path and raw_text is provided,\n            then raw_text will be used instead.\n        max_url_length (int): The maximum length of a URL. Defaults to 150.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if md is None:\n        raise ImportError(\n            \"The 'markdownify' library is not installed. \"\n            \"Please install it with 'pip install markdownify' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    if raw_text:\n        markdown_content = md(raw_text)\n    elif file_path:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            markdown_content = md(f.read())\n    else:\n        raise ValueError(\"Either file_path or raw_text must be provided.\")\n\n    # Normalize consecutive newlines that are more than 2\n    markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n    # Truncate long URLs in Markdown links or images\n    def truncate_url(match: re.Match) -&gt; str:\n        prefix, url = match.group(1), match.group(2)\n        if len(url) &gt; max_url_length:\n            url = url[: max_url_length - 3] + \"...\"\n        return f\"{prefix}({url})\"\n\n    return re.sub(r\"(!?\\[[^\\]]*\\])\\((.*?)\\)\", truncate_url, markdown_content)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>, default:                   <code>None</code> )           \u2013            <p>Path to the html file.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(raw_text)","title":"<code>raw_text</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Raw HTML text. If both file_path and raw_text is provided, then raw_text will be used instead.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(max_url_length)","title":"<code>max_url_length</code>","text":"(<code>int</code>, default:                   <code>150</code> )           \u2013            <p>The maximum length of a URL. Defaults to 150.</p>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/","title":"latex_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md","title":"chunklet.document_chunker.converters.latex_2_md","text":"<p>Functions:</p> <ul> <li> <code>latex_to_md</code>             \u2013              <p>Convert LaTeX code to Markdown-style plain text.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md","title":"latex_to_md","text":"<pre><code>latex_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Convert LaTeX code to Markdown-style plain text.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in markdown</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/latex_2_md.py</code> <pre><code>def latex_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Convert LaTeX code to Markdown-style plain text.\n\n    Args:\n        file_path (str | Path): Path to the latex file.\n\n    Returns:\n        str: The full text content in markdown\n    \"\"\"\n    if LatexNodes2Text is None:\n        raise ImportError(\n            \"The 'pylatexenc' library is not installed. \"\n            \"Please install it with 'pip install 'pylatexenc&gt;=2.10'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n        latex_code = f.read()\n\n    # Convert to text\n    latex_node = LatexNodes2Text()\n    text = latex_node.latex_to_text(latex_code)\n\n    # Replace \u00a7 by #\n    markdown_content = re.sub(r\"\u00a7\\.?\", \"#\", text)\n\n    # Normalize consecutive newlines more than two\n    return re.sub(r\"\\n{2,}\", \"\\n\\n\", markdown_content.strip())\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the latex file.</p>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/","title":"rst_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md","title":"chunklet.document_chunker.converters.rst_2_md","text":"<p>Functions:</p> <ul> <li> <code>rst_to_md</code>             \u2013              <p>Converts reStructuredText (RST) content into Markdown.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md","title":"rst_to_md","text":"<pre><code>rst_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Converts reStructuredText (RST) content into Markdown.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/rst_2_md.py</code> <pre><code>def rst_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Converts reStructuredText (RST) content into Markdown.\n\n    Args:\n        file_path (str | Path): Path to the rst file.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if publish_string is None:\n        raise ImportError(\n            \"The 'docutils' library is not installed. \"\n            \"Please install it with 'pip install 'docutils&gt;=0.21.2'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        rst_content = f.read()\n\n    # Convert the rst content to HTML first\n    html_content = publish_string(source=rst_content, writer_name=\"html\").decode(\n        \"utf-8\"\n    )\n\n    # Now we can convert it to markdown\n    markdown_content = html_to_md(raw_text=html_content)\n    return markdown_content\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the rst file.</p>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/","title":"table_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md","title":"chunklet.document_chunker.converters.table_2_md","text":"<p>Functions:</p> <ul> <li> <code>table_to_md</code>             \u2013              <p>Convert a CSV or XLSX file into a Markdown-formatted table string.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md.table_to_md","title":"table_to_md","text":"<pre><code>table_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Convert a CSV or XLSX file into a Markdown-formatted table string.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown table representation of the file contents.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/table_2_md.py</code> <pre><code>def table_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Convert a CSV or XLSX file into a Markdown-formatted table string.\n\n    Args:\n        file_path (str | Path): Path to the input file (.csv or .xlsx).\n\n    Returns:\n        str: Markdown table representation of the file contents.\n    \"\"\"\n    file_path = Path(file_path)\n    ext = file_path.suffix.lower()\n\n    # Read CSV\n    if ext == \".csv\":\n        with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n            data = list(csv.reader(f))\n\n    # Read Excel (.xlsx)\n    elif ext == \".xlsx\":\n        try:\n            from openpyxl import load_workbook\n        except ImportError as e:\n            raise ImportError(\n                \"The 'openpyxl' library is not installed. \"\n                \"Please install it with 'pip install openpyxl&gt;=3.1.2' \"\n                \"or install the document processing extras with \"\n                \"'pip install chunklet-py[document]'\"\n            ) from e\n        wb = load_workbook(file_path, read_only=True)\n        sheet = wb.active\n        data = list(sheet.iter_rows(values_only=True))\n        wb.close()\n\n    else:\n        raise ValueError(f\"Unsupported file type: {ext}\")\n\n    headers = data[0]\n    rows = data[1:]\n\n    if not tabulate:\n        raise ImportError(\n            \"The 'tabulate2' library is not installed. \"\n            \"Please install it with 'pip install tabulate2&gt;=1.10.0' \"\n            \"or install the document processing extras with \"\n            \"'pip install chunklet-py[document]'\"\n        )\n\n    return tabulate(rows, headers=headers, tablefmt=\"pipe\")\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md.table_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the input file (.csv or .xlsx).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker","title":"chunklet.document_chunker.document_chunker","text":"<p>Classes:</p> <ul> <li> <code>DocumentChunker</code>           \u2013            <p>A comprehensive document chunker that handles various file formats.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker","title":"DocumentChunker","text":"<pre><code>DocumentChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseChunker</code></p> <p>A comprehensive document chunker that handles various file formats.</p> <p>This class provides a high-level interface to chunk text from different document types. It automatically detects the file format and uses the appropriate method to extract content before passing it to an underlying <code>PlainTextChunker</code> instance.</p> <p>Key Features: - Multi-Format Support: Chunks text from PDF, TXT, MD, and RST files. - Metadata Enrichment: Automatically adds source file path and other   document-level metadata (e.g., PDF page numbers) to each chunk. - Bulk Processing: Efficiently chunks multiple documents in a single call. - Pluggable Document processors: Integrate custom processors allowing definition of specific logic for extracting text from various file types.</p> <p>Initializes the DocumentChunker.</p> <p>Parameters:</p> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Chunks multiple documents from a list of file paths.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunks a single document from a given path.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>supported_extensions</code>           \u2013            <p>Get the supported extensions, including the custom ones.</p> </li> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>def __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initializes the DocumentChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter | None): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    # Explicit type validation for sentence_splitter\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    self.plain_text_chunker = PlainTextChunker(\n        sentence_splitter=sentence_splitter,\n        verbose=self._verbose,\n        continuation_marker=self.continuation_marker,\n        token_counter=self.token_counter,\n    )\n\n    self.processors = {\n        \".pdf\": pdf_processor.PDFProcessor,\n        \".epub\": epub_processor.EPUBProcessor,\n        \".docx\": docx_processor.DOCXProcessor,\n        \".odt\": odt_processor.ODTProcessor,\n    }\n    self.converters = {\n        \".html\": html_2_md.html_to_md,\n        \".hml\": html_2_md.html_to_md,\n        \".rst\": rst_2_md.rst_to_md,\n        \".tex\": latex_2_md.latex_to_md,\n        \".csv\": table_2_md.table_to_md,\n        \".xlsx\": table_2_md.table_to_md,\n    }\n    self.processor_registry = CustomProcessorRegistry()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.supported_extensions","title":"supported_extensions  <code>property</code>","text":"<pre><code>supported_extensions\n</code></pre> <p>Get the supported extensions, including the custom ones.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Chunks multiple documents from a list of file paths.</p> <p>This method is a memory-efficient generator that yields chunks as they are processed, without loading all documents into memory at once. It handles various file types.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Chunks multiple documents from a list of file paths.\n\n    This method is a memory-efficient generator that yields chunks as they\n    are processed, without loading all documents into memory at once. It\n    handles various file types.\n\n    Args:\n        paths (restricted_iterable[str | Path]): A restricted iterable of paths to the document files.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n               Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors: How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    sentinel = object()\n\n    gathered_data = self._gather_all_data(paths, on_errors)\n\n    all_chunks_gen = self.plain_text_chunker.batch_chunk(\n        texts=gathered_data[\"all_texts_gen\"],\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n        separator=sentinel,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n    )\n\n    all_chunk_groups = split_at(all_chunks_gen, lambda x: x is sentinel)\n    path_section_counts = gathered_data[\"path_section_counts\"]\n    all_metadata = gathered_data[\"all_metadata\"]\n\n    # HACK: Since a sentinel is always at the end of the gen,\n    # the last list of the groups will be an empty one.\n    # The only work-around to add a sentinel at paths\n    paths = list(path_section_counts.keys()) + [None]\n\n    # If no files were successfully processed, return empty\n    if not path_section_counts:\n        return\n\n    doc_count = 0\n    curr_path = paths[0]\n    for chunks in all_chunk_groups:\n        if path_section_counts.get(curr_path, 0) == 0:\n            if separator is not None:\n                yield separator\n\n            doc_count += 1\n            curr_path = paths[doc_count]\n            if curr_path is None:\n                return\n\n        for i, ch in enumerate(chunks, start=1):\n            doc_metadata = all_metadata[doc_count]\n            doc_metadata[\"section_count\"] = path_section_counts[curr_path]\n            doc_metadata[\"curr_section\"] = i\n\n            ch[\"metadata\"].update(doc_metadata)\n            yield ch\n\n        path_section_counts[curr_path] -= 1\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(paths)","title":"<code>paths</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of paths to the document files.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs.    Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single document from a given path.</p> <p>This method automatically detects the file type and uses the appropriate processor to extract text before chunking. It then adds document-level metadata to each resulting chunk.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each representing</p> </li> <li> <code>list[Box]</code>           \u2013            <p>a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single document from a given path.\n\n    This method automatically detects the file type and uses the appropriate\n    processor to extract text before chunking. It then adds document-level\n    metadata to each resulting chunk.\n\n    Args:\n        path (str | Path): The path to the document file.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each representing\n        a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    path = Path(path)\n    ext = self._validate_and_get_extension(path)\n\n    text_content_or_generator, document_metadata = self._extract_data(path, ext)\n\n    if not isinstance(text_content_or_generator, str):\n        raise UnsupportedFileTypeError(\n            f\"File type '{ext}' is not supported by the general chunk method.\\n\"\n            \"Reason: The processor for this file returns iterable, \"\n            \"so it must be processed in parallel for efficiency.\\n\"\n            \"\ud83d\udca1 Hint: use `chunker.batch_chunk([file.ext])` for this file type.\"\n        )\n\n    self.log_info(\"Starting chunk processing for path: {}.\", path)\n\n    text_content = text_content_or_generator\n\n    # Process as a single block of text\n    chunk_boxes = self.plain_text_chunker.chunk(\n        text=text_content,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n        base_metadata=document_metadata,\n    )\n\n    self.log_info(\"Generated {} chunks for {}.\", len(chunk_boxes), path)\n\n    return chunk_boxes\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>The path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/processors/","title":"processors","text":""},{"location":"reference/chunklet/document_chunker/processors/#chunklet.document_chunker.processors","title":"chunklet.document_chunker.processors","text":"<p>Modules:</p> <ul> <li> <code>base_processor</code>           \u2013            </li> <li> <code>docx_processor</code>           \u2013            </li> <li> <code>epub_processor</code>           \u2013            </li> <li> <code>odt_processor</code>           \u2013            </li> <li> <code>pdf_processor</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/","title":"base_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor","title":"chunklet.document_chunker.processors.base_processor","text":"<p>Classes:</p> <ul> <li> <code>BaseProcessor</code>           \u2013            <p>Abstract base class for document processors, providing a unified interface</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor","title":"BaseProcessor","text":"<pre><code>BaseProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for document processors, providing a unified interface for extracting text and metadata from documents.</p> <p>Initializes the processor with the path to the document.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the document.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields text content from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_metadata","title":"extract_metadata  <code>abstractmethod</code>","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the document.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: Dictionary containing document metadata.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts metadata from the document.\n\n    Returns:\n        dict[str, Any]: Dictionary containing document metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_text","title":"extract_text  <code>abstractmethod</code>","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields text content from the document.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Text content chunks from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields text content from the document.\n\n    Yields:\n        str: Text content chunks from the document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/","title":"docx_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor","title":"chunklet.document_chunker.processors.docx_processor","text":"<p>Classes:</p> <ul> <li> <code>DOCXProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from DOCX files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor","title":"DOCXProcessor","text":"<pre><code>DOCXProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from DOCX files.</p> <p>Text content is extracted, images are replaced with a placeholder, and the resulting text is formatted using Markdown conversion.</p> <p>This class extracts metadata which typically uses a mix of Open Packaging Conventions (OPC) properties and elements that align with Dublin Core standards.</p> <p>For more details on the DOCX core properties processed, refer to the <code>python-docx</code> documentation: https://python-docx.readthedocs.io/en/latest/dev/analysis/features/coreprops.html</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - publisher - last_modified_by - created - modified - rights - version</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - publisher\n            - last_modified_by\n            - created\n            - modified\n            - rights\n            - version\n    \"\"\"\n    try:\n        from docx import Document\n    except ImportError as e:\n        raise ImportError(\n            \"The 'python-docx' library is not installed. \"\n            \"Please install it with 'pip install 'python-docx&gt;=1.2.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n\n    doc = Document(self.file_path)\n    props = doc.core_properties\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        value = getattr(props, field, \"\")\n        if value:\n            metadata[field] = str(value)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.</p> <p>Images are replaced with a placeholder \"[Image - num]\". Text is yielded in chunks of approximately 4000 characters each to simulate pages and enhance parallel execution.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A chunk of text, approximately 4000 characters each.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.\n\n    Images are replaced with a placeholder \"[Image - num]\".\n    Text is yielded in chunks of approximately 4000 characters each to simulate pages and enhance parallel execution.\n\n    Yields:\n        str: A chunk of text, approximately 4000 characters each.\n    \"\"\"\n    try:  # Lazy import\n        import mammoth\n    except ImportError as e:\n        raise ImportError(\n            \"The 'mammoth' library is not installed. \"\n            \"Please install it with 'pip install 'mammoth&gt;=1.9.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n\n    count = 0\n\n    def placeholder_images(image):\n        \"\"\"Replace all images with a placeholder text.\"\"\"\n        nonlocal count\n        count += 1\n        return [mammoth.html.text(f\"[Image - {count}]\")]\n\n    with open(self.file_path, \"rb\") as docx_file:\n        # Convert DOCX to HTML first\n        result = mammoth.convert_to_html(\n            docx_file, convert_image=placeholder_images\n        )\n        markdown_content = html_to_md(raw_text=result.value)\n\n    # Split into paragraphs and accumulate by character count (~4000 chars per chunk)\n    curr_chunk = []\n    curr_size = 0\n    max_size = 4000\n\n    for paragraph in markdown_content.split(\"\\n\\n\"):\n        para_len = len(paragraph)\n\n        # If adding this paragraph would exceed the limit, yield current chunk\n        if curr_size + para_len &gt; max_size and curr_chunk:\n            yield \"\\n\\n\".join(curr_chunk)\n            curr_chunk = []\n            curr_size = 0\n\n        curr_chunk.append(paragraph)\n        curr_size += para_len\n\n    # Yield any remaining content\n    if curr_chunk:\n        yield \"\\n\\n\".join(curr_chunk)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/","title":"epub_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor","title":"chunklet.document_chunker.processors.epub_processor","text":"<p>Classes:</p> <ul> <li> <code>EPUBProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from EPUB files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor","title":"EPUBProcessor","text":"<pre><code>EPUBProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from EPUB files.</p> <p>Text content is extracted by concatenating the text from all HTML content documents within the EPUB container.</p> <p>This processor focuses on extracting core metadata following the Dublin Core Metadata Initiative (DCMI) standard, which is the common practice in EPUB files. Not all available metadata fields are extracted.</p> <p>For more details on EPUB metadata and the Dublin Core standard, refer to the <code>ebooklib</code> tutorial:</p> <p>https://docs.sourcefabric.org/projects/ebooklib/en/latest/tutorial.html</p> <p>Initializes the EpubProcessor with a path to the EPUB file and reads the EPUB book into memory.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts Dublin Core metadata from the EPUB file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields Markdown-converted text from all document items in the EPUB file.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the EpubProcessor with a path to the EPUB file\n    and reads the EPUB book into memory.\n\n    Args:\n        file_path (str): Path to the EPUB file.\n    \"\"\"\n    if not epub:\n        raise ImportError(\n            \"The 'ebooklib' library is not installed. \"\n            \"Please install it with 'pip install 'ebooklib&gt;=0.19'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n    self.file_path = file_path\n    self.book = epub.read_epub(file_path)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the EPUB file.</p>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts Dublin Core metadata from the EPUB file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields. - title - creator - contributor - publisher - date - rights</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts Dublin Core metadata from the EPUB file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields.\n            - title\n            - creator\n            - contributor\n            - publisher\n            - date\n            - rights\n    \"\"\"\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        values = [v[0] for v in self.book.get_metadata(\"DC\", field)]\n        if values:\n            metadata[field] = \", \".join(values)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields Markdown-converted text from all document items in the EPUB file.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown-formatted text of each document item.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields Markdown-converted text from all document items in the EPUB file.\n\n    Yields:\n        str: Markdown-formatted text of each document item.\n    \"\"\"\n    for idref, _ in self.book.spine:\n        item = self.book.get_item_with_id(idref)\n        html_content = item.get_body_content().decode(\"utf-8\", errors=\"ignore\")\n        md_content = html_to_md(raw_text=html_content)\n        yield md_content.strip()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/","title":"odt_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor","title":"chunklet.document_chunker.processors.odt_processor","text":"<p>Classes:</p> <ul> <li> <code>ODTProcessor</code>           \u2013            <p>ODT extraction and processing utility using <code>odfpy</code>.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor","title":"ODTProcessor","text":"<pre><code>ODTProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>ODT extraction and processing utility using <code>odfpy</code>.</p> <p>Provides methods to extract text and metadata from ODT (OpenDocument Text) files, while processing the extracted text into manageable chunks.</p> <p>This processor extracts metadata from the ODT document's Dublin Core and OpenDocument standard properties.</p> <p>For more details on ODF metadata fields and <code>odfpy</code> usage, refer to: https://odfpy.readthedocs.io/en/latest/</p> <p>Initialize the ODTProcessor.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Extracts text content from ODT paragraphs, yielding chunks for efficient processing.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize the ODTProcessor.\n\n    Args:\n        file_path (str): Path to the ODT file.\n    \"\"\"\n    try:\n        from odf.opendocument import load\n\n        self._load_odf = load\n    except ImportError as e:\n        raise ImportError(\n            \"The 'odfpy' library is not installed. \"\n            \"Please install it with 'pip install odfpy&gt;=1.4.1' \"\n            \"or install the document processing extras with \"\n            \"'pip install chunklet-py[document]'\"\n        ) from e\n\n    self.file_path = file_path\n    self.doc = self._load_odf(self.file_path)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the ODT file.</p>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.</p> <p>Parses the document's metadata elements, extracting fields such as:</p> <p>Only present fields are included in the returned dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields:  - title  - creator  - initial_creator  - created  - chapter  - author_name</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.\n\n    Parses the document's metadata elements, extracting fields such as:\n\n    Only present fields are included in the returned dictionary.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n             - title\n             - creator\n             - initial_creator\n             - created\n             - chapter\n             - author_name\n\n    \"\"\"\n    from odf import text, meta, dc\n\n    metadata = {}\n    for field in [\n        dc.Title,\n        dc.Creator,\n        meta.InitialCreator,\n        meta.CreationDate,\n        text.Chapter,\n        text.AuthorName,\n    ]:\n        elems = self.doc.getElementsByType(field)\n        value = \"\".join(\n            node.data\n            for e in elems\n            for node in e.childNodes\n            if node.nodeType == node.TEXT_NODE\n        ).strip()\n        if value:  # Only store if not empty\n            key = field.__name__\n\n            # To keep metadata uniform with the other processors\n            key = \"created\" if key == \"CreationDate\" else key\n            key = \"author\" if key == \"Creator\" else key\n            key = \"creator\" if key == \"InitialCreator\" else key\n\n            metadata[key.lower()] = value\n\n    metadata[\"source\"] = str(self.file_path)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Extracts text content from ODT paragraphs, yielding chunks for efficient processing.</p> <p>Iterates through paragraph elements in the document, extracting text content and buffering it into chunks of approximately 4000 characters. This allows for memory-efficient processing of large documents by yielding text blocks that simulate pages and enhance parallel execution.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A chunk of text, approximately 4000 characters each.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Extracts text content from ODT paragraphs, yielding chunks for efficient processing.\n\n    Iterates through paragraph elements in the document, extracting text content\n    and buffering it into chunks of approximately 4000 characters. This allows for memory-efficient\n    processing of large documents by yielding text blocks that simulate pages and enhance parallel execution.\n\n    Yields:\n        str: A chunk of text, approximately 4000 characters each.\n    \"\"\"\n    from odf import text\n\n    curr_chunk = []\n    curr_size = 0\n    max_size = 4000\n\n    for p_elem in self.doc.getElementsByType(text.P):\n        para_text = \"\".join(\n            node.data\n            for node in p_elem.childNodes\n            if node.nodeType == node.TEXT_NODE\n        ).strip()\n        if not para_text:\n            continue\n\n        para_len = len(para_text)\n\n        # If adding this paragraph would exceed the limit, yield current chunk\n        if curr_size + para_len &gt; max_size and curr_chunk:\n            yield \"\\n\".join(curr_chunk)\n            curr_chunk = []\n            curr_size = 0\n\n        curr_chunk.append(para_text)\n        curr_size += para_len\n\n    # Yield any remaining content\n    if curr_chunk:\n        yield \"\\n\".join(curr_chunk)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/","title":"pdf_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor","title":"chunklet.document_chunker.processors.pdf_processor","text":"<p>Classes:</p> <ul> <li> <code>PDFProcessor</code>           \u2013            <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor","title":"PDFProcessor","text":"<pre><code>PDFProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> <p>Provides methods to extract text and metadata from PDF files, while cleaning and normalizing the extracted text using regex patterns.</p> <p>This processor extracts metadata from the PDF document's information dictionary, focusing on core metadata rather than all available fields.</p> <p>For more details on PDF metadata extraction using <code>pdfminer.six</code>, refer to this relevant Stack Overflow discussion:</p> <p>https://stackoverflow.com/questions/75591385/extract-metadata-info-from-online-pdf-using-pdfminer-in-python</p> <p>Initialize the PDFProcessor.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the PDF document's information dictionary.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yield cleaned text from each PDF page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize the PDFProcessor.\n\n    Args:\n        file_path (str): Path to the PDF file.\n    \"\"\"\n    try:\n        from pdfminer.layout import LAParams\n    except ImportError as e:\n        raise ImportError(\n            \"The 'pdfminer.six' library is not installed. \"\n            \"Please install it with 'pip install 'pdfminer.six&gt;=20250324'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n    self.file_path = file_path\n    self.laparams = LAParams(\n        line_margin=0.5,\n    )\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the PDF file.</p>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the PDF document's information dictionary.</p> <p>Includes source path, page count, and PDF info fields.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - creator - producer - publisher - created - modified</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts metadata from the PDF document's information dictionary.\n\n    Includes source path, page count, and PDF info fields.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - creator\n            - producer\n            - publisher\n            - created\n            - modified\n    \"\"\"\n    from pdfminer.pdfpage import PDFPage\n    from pdfminer.pdfparser import PDFParser\n    from pdfminer.pdfdocument import PDFDocument\n\n    metadata = {\"source\": str(self.file_path), \"page_count\": 0}\n    with open(self.file_path, \"rb\") as f:\n        # Initialize parser on the file stream\n        parser = PDFParser(f)\n\n        # The PDFDocument constructor reads file structure and advances the pointer\n        doc = PDFDocument(parser)\n\n        # Count pages: Reset pointer to the start of the file stream to count pages correctly\n        f.seek(0)\n\n        metadata[\"page_count\"] = ilen(PDFPage.get_pages(f))\n\n        # Extract info fields from the document object\n        if hasattr(doc, \"info\") and doc.info:\n            for info in doc.info:\n                for k, v in info.items():\n                    k = self._safe_decode(k)\n                    v = self._safe_decode(v)\n\n                    # To keep metadata uniform with the other processorss\n                    k = \"created\" if k == \"CreationDate\" else k\n                    k = \"modified\" if k == \"ModDate\" else k\n\n                    if k.lower() in self.METADATA_FIELDS:\n                        metadata[k.lower()] = v\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yield cleaned text from each PDF page.</p> <p>Extracts text content page by page using pdfminer.high_level.extract_text for efficient processing. Each page is processed individually to avoid memory issues with large PDF files. The extracted text is cleaned using the _cleanup_text method to remove artifacts and normalize formatting.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Cleaned text content from each PDF page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Yield cleaned text from each PDF page.\n\n    Extracts text content page by page using pdfminer.high_level.extract_text\n    for efficient processing. Each page is processed individually to avoid\n    memory issues with large PDF files. The extracted text is cleaned using\n    the _cleanup_text method to remove artifacts and normalize formatting.\n\n    Yields:\n        str: Cleaned text content from each PDF page.\n    \"\"\"\n    from pdfminer.high_level import extract_text\n    from pdfminer.pdfpage import PDFPage\n\n    with open(self.file_path, \"rb\") as fp:\n        page_count = ilen(PDFPage.get_pages(fp))\n\n        for page_num in range(page_count):\n            # Call extract_text on the file path, specifying the page number.\n            # This is efficient as it avoids repeated file seeks/parsing\n            # within the loop that was present in the old `extract_text_to_fp` approach.\n            raw_text = extract_text(\n                self.file_path,\n                page_numbers=[page_num],\n                laparams=self.laparams,\n            )\n            yield self._cleanup_text(raw_text)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/","title":"registry","text":""},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry","title":"chunklet.document_chunker.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> <ol> <li>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/exceptions/","title":"exceptions","text":""},{"location":"reference/chunklet/exceptions/#chunklet.exceptions","title":"chunklet.exceptions","text":"<p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the `chunk` method, like `chunker.chunk(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p>"},{"location":"reference/chunklet/plain_text_chunker/","title":"plain_text_chunker","text":""},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker","title":"chunklet.plain_text_chunker","text":"<p>Classes:</p> <ul> <li> <code>PlainTextChunker</code>           \u2013            <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker","title":"PlainTextChunker","text":"<pre><code>PlainTextChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseChunker</code></p> <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> <p>Key Features: - Flexible Constraint-Based Chunking: Segment text by specifying limits on sentence count, token count and section breaks or combination of them. - Clause-Level Overlap: Ensures semantic continuity between chunks by overlapping at natural clause boundaries with Customizable continuation marker. - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Pluggable Token Counters: Integrate custom token counting functions (e.g., for specific LLM tokenizers). - Parallel Processing: Efficiently handles batch chunking of multiple texts using multiprocessing. - Memory friendly batching: Yields chunks one at a time, reducing memory usage, especially for very large documents.</p> <p>Initialize The PlainTextChunker.</p> <p>Parameters:</p> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Processes a batch of texts in parallel, splitting each into chunks.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunks a single text into smaller pieces based on specified parameters.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize The PlainTextChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter, optional): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int], optional): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    # Initialize SentenceSplitter\n    self.sentence_splitter = sentence_splitter or SentenceSplitter()\n    self.sentence_splitter.verbose = self._verbose\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of texts in parallel, splitting each into chunks. Leverages multiprocessing for efficient batch chunking.</p> <p>If a task fails, <code>chunklet</code> will now stop processing and return the results of the tasks that completed successfully, preventing wasted work.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If <code>texts</code> is not an iterable of strings, or if <code>n_jobs</code> is less than 1.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of texts in parallel, splitting each into chunks.\n    Leverages multiprocessing for efficient batch chunking.\n\n    If a task fails, `chunklet` will now stop processing and return the results\n    of the tasks that completed successfully, preventing wasted work.\n\n    Args:\n        texts (restricted_iterable[str]): A restricted iterable of input texts to be chunked.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): The token counting function.\n            Required if `max_tokens` is set.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n            Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]): How to handle errors during processing.\n            Defaults to 'raise'.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n\n    Raises:\n        InvalidInputError: If `texts` is not an iterable of strings, or if `n_jobs` is less than 1.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting\n            or token counting within a chunking task.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        overlap_percent=overlap_percent,\n        max_section_breaks=max_section_breaks,\n        offset=offset,\n        base_metadata=base_metadata,\n        token_counter=token_counter or self.token_counter,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=texts,\n        iterable_name=\"texts\",\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        separator=separator,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(texts)","title":"<code>texts</code>","text":"(<code>restricted_iterable[str]</code>)           \u2013            <p>A restricted iterable of input texts to be chunked.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>The token counting function. Required if <code>max_tokens</code> is set.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single text into smaller pieces based on specified parameters. Supports flexible constraint-based chunking, clause-level overlap, and custom token counters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each containing the chunk content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any chunking configuration parameter is invalid.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single text into smaller pieces based on specified parameters.\n    Supports flexible constraint-based chunking, clause-level overlap,\n    and custom token counters.\n\n    Args:\n        text (str): The input text to chunk.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-75). Defaults to 20\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): Optional token counting function.\n            Required for token-based modes only.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each containing the chunk content and metadata.\n\n    Raises:\n        InvalidInputError: If any chunking configuration parameter is invalid.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting or token counting within a chunking task.\n    \"\"\"\n    self._validate_constraints(\n        max_tokens, max_sentences, max_section_breaks, token_counter\n    )\n\n    self.log_info(\n        \"Starting chunk processing for text starting with: {}.\",\n        f\"{text[:100]}...\",\n    )\n\n    # Adjust limits for _group_by_chunk's internal use\n    if max_tokens is None:\n        max_tokens = sys.maxsize\n    if max_sentences is None:\n        max_sentences = sys.maxsize\n    if max_section_breaks is None:\n        max_section_breaks = sys.maxsize\n\n    if not text.strip():\n        self.log_info(\"Input text is empty. Returning empty list.\")\n        return []\n\n    try:\n        sentences = self.sentence_splitter.split(\n            text,\n            lang,\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"An error occurred during the sentence splitting process.\\nDetails: {e}\\n\"\n            \"\ud83d\udca1 Hint: This may be due to an issue with the underlying sentence splitting library.\"\n        ) from e\n\n    if not sentences:\n        return []\n\n    offset = round(offset)\n    if offset &gt;= len(sentences):\n        logger.warning(\n            \"Offset {} &gt;= total sentences {}. Returning empty list.\",\n            offset,\n            len(sentences),\n        )\n        return []\n\n    chunks = self._group_by_chunk(\n        sentences[offset:],\n        token_counter=token_counter or self.token_counter,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n    )\n\n    # Leave the user's original dict untouched\n    base_metadata = (base_metadata or {}).copy()\n\n    return self._create_chunk_boxes(chunks, base_metadata, text)\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to chunk.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-75). Defaults to 20</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required for token-based modes only.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter","title":"chunklet.sentence_splitter","text":"<p>Modules:</p> <ul> <li> <code>languages</code>           \u2013            <p>This module contains the language sets for the supported sentence splitters.</p> </li> <li> <code>registry</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Abstract base class for sentence splitting.</p> </li> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sentence splitting. Defines the interface that all sentence splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str, lang: str) -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>text (str): The input text to be split.     lang (str): The language of the text (e.g., 'en', 'fr', 'auto').</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MySplitter(BaseSplitter):\n...     def split(self, text: str, lang: str) -&gt; list[str]:\n...         return text.split(\".\")\n&gt;&gt;&gt; splitter = MySplitter()\n&gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n['Hello', ' World']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, text: str, lang: str) -&gt; list[str]:\n    \"\"\"\n    Splits the given text into a list of sentences.\n\n    text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n\n    Examples:\n        &gt;&gt;&gt; class MySplitter(BaseSplitter):\n        ...     def split(self, text: str, lang: str) -&gt; list[str]:\n        ...         return text.split(\".\")\n        &gt;&gt;&gt; splitter = MySplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n        ['Hello', ' World']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> <ol> <li>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; List[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        List[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.custom_splitter_registry = CustomSplitterRegistry()\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for langid\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    if self.verbose:\n        logger.info(\n            \"Language detection: '{}' with confidence {}.\",\n            lang_detected,\n            f\"{round(confidence) * 10}/10\",\n        )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        if self.verbose:\n            logger.info(\"Input text is empty. Returning empty list.\")\n        return []\n\n    if lang == \"auto\":\n        logger.warning(\n            \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n        )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if self.custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = self.custom_splitter_registry.split(text, lang)\n        if self.verbose:\n            logger.info(\"Using registered splitter: {}\", splitter_name)\n    else:\n        # Find and use the appropriate handler\n        sentences = None\n        for lang_set, handler in self.LANGUAGE_HANDLERS.items():\n            if lang in lang_set:\n                sentences = handler(lang, text)\n                break\n\n        # If no handler found, use fallback\n        if sentences is None:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n            sentences = self.fallback_splitter.split(text)\n\n    # Apply post-processing filter\n    processed_sentences = self._filter_sentences(sentences)\n\n    if self.verbose:\n        logger.info(\n            \"Text splitted into sentences. Total sentences detected: {}\",\n            len(processed_sentences),\n        )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/","title":"_fallback_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter","title":"chunklet.sentence_splitter._fallback_splitter","text":"<p>Classes:</p> <ul> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; List[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        List[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/languages/","title":"languages","text":""},{"location":"reference/chunklet/sentence_splitter/languages/#chunklet.sentence_splitter.languages","title":"chunklet.sentence_splitter.languages","text":"<p>This module contains the language sets for the supported sentence splitters. Each set is filtered to contain only the languages truly unique to that library.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/","title":"registry","text":""},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry","title":"chunklet.sentence_splitter.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> <ol> <li>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter","title":"chunklet.sentence_splitter.sentence_splitter","text":"<p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Abstract base class for sentence splitting.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sentence splitting. Defines the interface that all sentence splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str, lang: str) -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>text (str): The input text to be split.     lang (str): The language of the text (e.g., 'en', 'fr', 'auto').</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MySplitter(BaseSplitter):\n...     def split(self, text: str, lang: str) -&gt; list[str]:\n...         return text.split(\".\")\n&gt;&gt;&gt; splitter = MySplitter()\n&gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n['Hello', ' World']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, text: str, lang: str) -&gt; list[str]:\n    \"\"\"\n    Splits the given text into a list of sentences.\n\n    text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n\n    Examples:\n        &gt;&gt;&gt; class MySplitter(BaseSplitter):\n        ...     def split(self, text: str, lang: str) -&gt; list[str]:\n        ...         return text.split(\".\")\n        &gt;&gt;&gt; splitter = MySplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n        ['Hello', ' World']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.custom_splitter_registry = CustomSplitterRegistry()\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for langid\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    if self.verbose:\n        logger.info(\n            \"Language detection: '{}' with confidence {}.\",\n            lang_detected,\n            f\"{round(confidence) * 10}/10\",\n        )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        if self.verbose:\n            logger.info(\"Input text is empty. Returning empty list.\")\n        return []\n\n    if lang == \"auto\":\n        logger.warning(\n            \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n        )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if self.custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = self.custom_splitter_registry.split(text, lang)\n        if self.verbose:\n            logger.info(\"Using registered splitter: {}\", splitter_name)\n    else:\n        # Find and use the appropriate handler\n        sentences = None\n        for lang_set, handler in self.LANGUAGE_HANDLERS.items():\n            if lang in lang_set:\n                sentences = handler(lang, text)\n                break\n\n        # If no handler found, use fallback\n        if sentences is None:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n            sentences = self.fallback_splitter.split(text)\n\n    # Apply post-processing filter\n    processed_sentences = self._filter_sentences(sentences)\n\n    if self.verbose:\n        logger.info(\n            \"Text splitted into sentences. Total sentences detected: {}\",\n            len(processed_sentences),\n        )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/terminators/","title":"terminators","text":""},{"location":"reference/chunklet/sentence_splitter/terminators/#chunklet.sentence_splitter.terminators","title":"chunklet.sentence_splitter.terminators","text":""},{"location":"reference/chunklet/visualizer/","title":"visualizer","text":""},{"location":"reference/chunklet/visualizer/#chunklet.visualizer","title":"chunklet.visualizer","text":"<p>Modules:</p> <ul> <li> <code>visualizer</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/visualizer/visualizer/","title":"visualizer","text":""},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer","title":"chunklet.visualizer.visualizer","text":"<p>Classes:</p> <ul> <li> <code>Visualizer</code>           \u2013            <p>A FastAPI-based web interface for visualizing document and code chunks.</p> </li> </ul>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer","title":"Visualizer","text":"<pre><code>Visualizer(\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>A FastAPI-based web interface for visualizing document and code chunks.</p> <p>This server allows users to upload text or code files, processes them with Chunklet's <code>DocumentChunker</code> or <code>CodeChunker</code>, and returns the chunked data along with statistics. A minimal frontend interface is served at the root endpoint.</p> <p>Attributes:</p> <ul> <li> <code>host</code>               (<code>str</code>)           \u2013            <p>Host IP to bind the FastAPI server.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Port number to run the server on.</p> </li> <li> <code>document_chunker</code>               (<code>DocumentChunker</code>)           \u2013            <p>Chunklet document chunker instance.</p> </li> <li> <code>code_chunker</code>               (<code>CodeChunker</code>)           \u2013            <p>Chunklet code chunker instance.</p> </li> <li> <code>app</code>               (<code>FastAPI</code>)           \u2013            <p>FastAPI application instance.</p> </li> </ul> <p>Initializes the Visualizer server and configures chunkers.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>serve</code>             \u2013              <p>Starts the FastAPI server and prints the server URL.</p> </li> </ul> Source code in <code>src/chunklet/visualizer/visualizer.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"Initializes the Visualizer server and configures chunkers.\n\n    Args:\n        host (str): Host IP to run the server. Defaults to \"127.0.0.1\".\n        port (int): Port number to run the server. Defaults to 8000.\n        token_counter (Optional[Callable[[str], int]]): Function to count tokens\n            in text/code. Required for chunkers if used with `max_tokens`.\n    \"\"\"\n    if FastAPI is None:\n        raise ImportError(\n            \"The 'fastapi' library is not installed. \"\n            \"Please install it with 'pip install fastapi&gt;=0.115.12' or install the visualization extras \"\n            \"with 'pip install 'chunklet-py[visualization]''\"\n        )\n\n    self.host = host\n    self.port = port\n    self._token_counter = token_counter\n\n    self.app = FastAPI()\n\n    # Initialize chunkers\n    self.document_chunker = DocumentChunker(token_counter=token_counter)\n    self.code_chunker = CodeChunker(token_counter=token_counter)\n\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    static_dir = os.path.join(base_dir, \"static\")\n\n    self.app.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\n\n    # API endpoints\n    self.app.get(\"/api/token_counter_status\")(self._get_token_counter_status)\n    self.app.get(\"/health\")(self._get_health_check)\n    self.app.get(\"/\")(self._get_index)\n    self.app.post(\"/api/chunk\")(self._chunk_file)\n</code></pre>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(host)","title":"<code>host</code>","text":"(<code>str</code>, default:                   <code>'127.0.0.1'</code> )           \u2013            <p>Host IP to run the server. Defaults to \"127.0.0.1\".</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(port)","title":"<code>port</code>","text":"(<code>int</code>, default:                   <code>8000</code> )           \u2013            <p>Port number to run the server. Defaults to 8000.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(token_counter)","title":"<code>token_counter</code>","text":"(<code>Optional[Callable[[str], int]]</code>, default:                   <code>None</code> )           \u2013            <p>Function to count tokens in text/code. Required for chunkers if used with <code>max_tokens</code>.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer.token_counter","title":"token_counter  <code>property</code> <code>writable</code>","text":"<pre><code>token_counter\n</code></pre> <p>Get the current token counter function.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer.serve","title":"serve","text":"<pre><code>serve()\n</code></pre> <p>Starts the FastAPI server and prints the server URL.</p> Source code in <code>src/chunklet/visualizer/visualizer.py</code> <pre><code>def serve(self):\n    \"\"\"Starts the FastAPI server and prints the server URL.\"\"\"\n    if uvicorn is None:\n        raise ImportError(\n            \"The 'uvicorn' library is not installed. \"\n            \"Please install it with 'pip install uvicorn&gt;=0.34.0' or install the visualization extras \"\n            \"with 'pip install 'chunklet-py[visualization]''\"\n        )\n\n    print(\" =\" * 20)\n    print(\"\\nTEXT CHUNK VISUALIZER\")\n    print(\"= \" * 20)\n    print(f\"URL: http://{self.host}:{self.port}\")\n\n    uvicorn.run(self.app, host=self.host, port=self.port)\n</code></pre>"}]}