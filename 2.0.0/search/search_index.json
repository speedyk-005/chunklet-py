{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Chunklet-py Documentation!","text":"<p>\u201cOne library to split them all: Sentence, Code, Docs.\u201d</p> <p>Hello there! Welcome to the official documentation for Chunklet-py. We're thrilled to have you here and excited to guide you through everything our library has to offer.</p>"},{"location":"#why-bother-with-smart-chunking","title":"Why Bother with Smart Chunking?","text":"<p>You might be thinking, 'Can't I just split my text or code with a simple character count or by arbitrary lines?' Well, you certainly could, but let's be frank \u2013 that's a bit like trying to perform delicate surgery with a butter knife! Standard splitting methods often lead to:</p> <ul> <li>Literary Butchery: Sentences chopped mid-thought or code blocks broken mid-function, leading to a loss of crucial meaning.</li> <li>Monolingual Approach: A disregard for the unique rules of non-English languages or the specific structures of programming languages.</li> <li>A Goldfish's Memory: Forgetting the context of the previous chunk, resulting in disconnected ideas and a less coherent flow.</li> </ul>"},{"location":"#why-chunklet-py-what-is-it-anyway-and-why-should-you-care","title":"\ud83e\udd14 Why Chunklet-py? What is it, Anyway? (And Why Should You Care?)","text":"<p>Chunklet-py is a versatile and powerful library designed to intelligently segment various forms of content\u2014from raw text to complex documents and source code\u2014into perfectly sized, context-aware chunks. It goes beyond simple splitting, offering specialized tools:</p> <ul> <li><code>Sentence Splitter</code></li> <li><code>Plain Text Chunker</code></li> <li><code>Document Chunker</code></li> <li><code>Code Chunker</code></li> </ul> <p>Each of these is tailored to preserve the original meaning and structure of your data.</p> <p>Whether you're preparing data for Large Language Models (LLMs), developing Retrieval-Augmented Generation (RAG) pipelines, or enhancing AI-driven document search, Chunklet-py (version 2.0) provides the precision and flexibility needed for efficient indexing, embedding, and inference across multiple formats and languages.</p> <ul> <li> <p> Blazingly Fast</p> <p>Leverages efficient parallel processing to chunk large volumes of content with remarkable speed.</p> </li> <li> <p> Featherlight Footprint</p> <p>Designed to be lightweight and memory-efficient, ensuring optimal performance without unnecessary overhead.</p> </li> <li> <p> Rich Metadata for RAG</p> <p>Enriches chunks with valuable, context-aware metadata (source, span, document properties, code AST details) crucial for advanced RAG and LLM applications.</p> </li> <li> <p> Infinitely Customizable</p> <p>Offers extensive customization options, from pluggable token counters to custom sentence splitters and processors.</p> </li> <li> <p> Multilingual Mastery</p> <p>Supports over 50 natural languages for text and document chunking with intelligent detection and language-specific algorithms.</p> </li> <li> <p> Code-Aware Intelligence</p> <p>Language-agnostic code chunking that understands and preserves the structural integrity of your source code.</p> </li> <li> <p> Precision Chunking</p> <p>Flexible constraint-based chunking allows you to combine limits based on sentences, tokens, sections, lines, and functions.</p> </li> <li> <p> Dual Interface: CLI &amp; Library</p> <p>Use it as a powerful command-line tool for fast, terminal-based chunking or import it as a library for deep integration into your Python applications.</p> </li> </ul>"},{"location":"#ready-to-dive-in-your-chunklet-adventure-awaits","title":"Ready to Dive In? \ud83d\ude80 Your Chunklet Adventure Awaits!","text":"<p>Welcome, brave adventurer, to the Chunklet starter pack! You've taken the first step towards taming unruly texts and transforming them into bite-sized, manageable chunks. No more wrestling with massive strings of words \u2013 Chunklet is here to make your life (and your LLMs' lives) a whole lot easier.</p> <p>Here's how to get your hands dirty and unleash the power of Chunklet-py:</p> <ul> <li> <p>Installation: Our super-easy, step-by-step guide to get Chunklet-py up and running on your system. No pain, all gain!</p> </li> <li> <p>Choose Your Own Adventure: CLI or Code?     Chunklet offers two main paths to text-chunking glory. Pick the one that suits your style:</p> <ul> <li> <p>Command Line Interface (CLI): For the terminal aficionados and those who prefer quick, direct action. If you like typing commands and seeing immediate results, this is your jam.</p> <ul> <li>Dive into CLI Usage</li> </ul> </li> <li> <p>Programmatic Usage: For the developers, the scripters, and those who want to integrate Chunklet's power directly into their Python applications. If you prefer writing code and building custom workflows, this path is for you.</p> <ul> <li>Explore Programmatic Usage</li> </ul> </li> </ul> </li> </ul> <p>No matter which path you choose, Chunklet is designed to make text chunking as painless (and perhaps, as entertaining) as possible. Let's get chunking!</p>"},{"location":"#how-chunklet-py-compares","title":"How Chunklet-py Compares","text":"<p>While there are other chunking libraries available, Chunklet-py stands out for its unique combination of versatility, performance, and ease of use. Here's a quick look at how it compares to some of the alternatives:</p> Library Key Differentiator Focus chunklet-py All-in-one, lightweight, and language-agnostic with specialized algorithms. Text, Code, Docs CintraAI Code Chunker Relies on <code>tree-sitter</code>, which can add setup complexity. Code Chonkie A feature-rich pipeline tool with cloud/vector integrations, but uses a more basic sentence splitter and <code>tree-sitter</code> for code. Pipelines, Integrations code_chunker (JimAiMoment) Uses basic regex and rules with limited language support. Code Semchunk Primarily for text, using a general-purpose sentence splitter. Text <p>Chunklet-py's rule-based, language-agnostic approach to code chunking avoids the need for heavy dependencies like <code>tree-sitter</code>, which can sometimes introduce compatibility issues. For sentence splitting, it uses specialized libraries and algorithms for higher accuracy, rather than a one-size-fits-all approach. This makes Chunklet-py a great choice for projects that require a balance of power, flexibility, and a small footprint.</p>"},{"location":"#the-grand-tour","title":"The Grand Tour","text":"<p>Wanna know what's under the hood?</p> <ul> <li>Supported Languages: See which languages Chunklet speaks fluently.</li> <li>Exceptions and Warnings: Because sometimes, things go wrong. Here's what to do when they do.</li> <li>Metadata: Understand the rich context <code>chunklet</code> attaches to your chunks.</li> <li>Troubleshooting: Solutions to common issues you might encounter.</li> </ul>"},{"location":"#keeping-up-to-date","title":"Keeping Up-to-Date","text":"<p>Stay informed about Chunklet's evolution:</p> <ul> <li>What's New: Discover all the exciting new features and improvements in Chunklet 2.0.</li> <li> <p>Migration Guide: Learn how to smoothly transition from previous versions to Chunklet 2.0.</p> </li> <li> <p>Changelog: See what's new, what's fixed, and what's been improved in recent versions.</p> </li> </ul>"},{"location":"#planned-features","title":"\ud83e\uddea Planned Features","text":"<ul> <li>[x] CLI interface</li> <li>[x] Documents chunking with metadata.</li> <li>[x] Code chunking based on interest point.</li> <li>[ ] Visualization for chunks (e.g., highlighting spans in original documents)</li> <li>Extend the file supported:</li> <li>[ ] Support for odt and eml files</li> <li>[ ] Support for tabular: csv, excel, ...</li> </ul>"},{"location":"#project-information-contributing-for-the-serious-stuff-and-if-you-want-to-join-the-fun","title":"Project Information &amp; Contributing For the serious stuff (and if you want to join the fun):","text":"<ul> <li>GitHub Repository: The main hub for all things Chunklet.</li> <li>License Information: All the necessary bits and bobs about Chunklet's license.</li> <li>Contributing: Want to help make Chunklet even better? Find out how you can contribute!                                            </li> </ul>"},{"location":"exceptions-and-warnings/","title":"Exceptions and Warnings: Navigating Chunklet's Quirks","text":"<p>Every powerful tool has its moments, and Chunklet-py is no different! This guide is here to help you understand the various messages and signals you might encounter. Most of these are straightforward to resolve, and some are simply helpful hints to keep you on track.</p>"},{"location":"exceptions-and-warnings/#exceptions-when-things-need-a-pause","title":"Exceptions: When Things Need a Pause","text":"<p>Sometimes, unexpected situations arise that require Chunklet-py to pause its operations. When you encounter an exception, it means something significant occurred that prevented the process from continuing. But don't worry, we're here to help you understand what happened!</p>"},{"location":"exceptions-and-warnings/#chunkleterror","title":"<code>ChunkletError</code>","text":"<ul> <li>Description: This is the foundational exception for all operations within the Chunklet-py library. It serves as the base class for more specific exceptions related to chunking and splitting.</li> <li>Inherits from: <code>Exception</code></li> <li>When Raised: While you typically won't encounter <code>ChunkletError</code> directly, it's the common ancestor for all our custom exceptions. This design makes it convenient to catch any Chunklet-related issues using a single <code>except ChunkletError</code> statement.</li> </ul>"},{"location":"exceptions-and-warnings/#invalidinputerror","title":"<code>InvalidInputError</code>","text":"<ul> <li>Description: Oh dear! \ud83d\ude15 This exception appears when Chunklet-py encounters input that's a little... unexpected. It's like trying to bake a cake with salt instead of sugar \u2013 the ingredients just aren't quite right! This could mean a parameter is out of place, the data type is a surprise, or the input structure is a bit wonky. Chunklet-py is always eager to help, but it does need valid input to craft those perfect chunks!</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When an invalid parameter is passed during initialization (e.g., providing a non-boolean value for a boolean flag).</li> <li>When an incorrect type of object is passed (e.g., a sentence splitter that doesn't inherit from the required base class).</li> <li>When a file path is missing a required extension.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#missingtokencountererror","title":"<code>MissingTokenCounterError</code>","text":"<ul> <li>Description: Imagine trying to bake a cake without a key ingredient like flour \u2013 that's pretty much what happens when Chunklet-py needs a <code>token_counter</code> but can't find one! This exception gently reminds you that a token counter is essential for token-based chunking.     &gt; A token_counter is required for token-based chunking.     &gt; \ud83d\udca1 Hint: Pass a token counting function to the <code>chunk</code> method, like <code>chunker.chunk(..., token_counter=tk)</code>     &gt; or configure it in the class initialization: <code>.*Chunker(token_counter=tk)</code></li> <li>Inherits from: <code>InvalidInputError</code></li> <li>When Raised:<ul> <li>When performing token-based chunking (\"token\" or \"hybrid\" modes) without a <code>token_counter</code> function having been provided.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#fileprocessingerror","title":"<code>FileProcessingError</code>","text":"<ul> <li>Description: This exception signals that Chunklet-py encountered a problem while trying to interact with a file. It could be anything from a file that's playing hide-and-seek (not found!), to permission issues, encoding troubles, or even a file that's a bit under the weather (corrupted). Essentially, something prevented us from properly loading, opening, or accessing the file.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a specified file does not exist at the given path.</li> <li>When there are issues reading a file's content due to permissions, encoding problems, or if it's a binary file.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#unsupportedfiletypeerror","title":"<code>UnsupportedFileTypeError</code>","text":"<ul> <li>Description: This exception pops up when Chunklet-py receives a file type it doesn't quite recognize. While we're always working to expand our capabilities, some file formats are simply beyond our current repertoire.</li> <li>Inherits from: <code>FileProcessingError</code></li> <li>When Raised:<ul> <li>When trying to process a file with an extension that is not supported or registered with a custom processor.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#tokenlimiterror","title":"<code>TokenLimitError</code>","text":"<ul> <li>Description: This exception occurs when a chunk exceeds the defined <code>max_tokens</code> limit. It's particularly relevant in modes where maintaining the semantic integrity of your content is paramount, even if it means not splitting a block further.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a structural block of code (like a function or class) exceeds the <code>max_tokens</code> limit while in <code>strict_mode</code>, where splitting the block would compromise its integrity.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#callbackerror","title":"<code>CallbackError</code>","text":"<ul> <li>Description: This exception occurs when a user-provided callback function (such as a <code>token_counter</code>, or a custom <code>sentence splitter</code> or <code>document processor</code>) encounters an error during its execution. It's Chunklet-py's way of letting you know that something went awry within the custom logic you've integrated.</li> <li>Inherits from: <code>ChunkletError</code></li> <li>When Raised:<ul> <li>When a user-provided callback function (like a <code>token_counter</code>, or a custom <code>sentence splitter</code> or <code>document processor</code>) raises an error during its execution in any chunking or batch process.</li> </ul> </li> </ul>"},{"location":"exceptions-and-warnings/#warnings-chunklet-pys-gentle-reminders","title":"Warnings: Chunklet-py's Gentle Reminders","text":"<p>Warnings from Chunklet-py are like friendly nudges \u2013 they let you know about situations that might warrant your attention, even though the process continues. They often highlight opportunities for optimization or important details to be aware of.</p>"},{"location":"exceptions-and-warnings/#the-language-is-set-to-auto-consider-setting-the-lang-parameter-to-a-specific-language-to-improve-reliability","title":"\"The language is set to <code>auto</code>. Consider setting the <code>lang</code> parameter to a specific language to improve reliability.\"","text":"<ul> <li>What it means: This warning appears when Chunklet-py is set to automatically detect the language of your text. While our language detection is quite capable, providing a specific <code>lang</code> parameter (e.g., <code>lang='en'</code> or <code>lang='fr'</code>) can often lead to faster and more accurate results, particularly with shorter texts. Think of it as giving Chunklet-py a helpful head start!</li> <li>Where Logged: <code>src/chunklet/sentence_splitter/sentence_splitter.py</code></li> <li>What to do: If you know the language of your text, setting the <code>lang</code> parameter explicitly is a great idea. If not, no worries \u2013 Chunklet-py will still do its best to figure it out for you.</li> </ul>"},{"location":"exceptions-and-warnings/#using-a-universal-rule-based-splitter-reason-language-not-supported-or-detected-with-low-confidence","title":"\"Using a universal rule-based splitter. Reason: Language not supported or detected with low confidence.\"","text":"<ul> <li>What it means: This warning indicates that Chunklet-py couldn't find a specialized sentence splitter for your language (or its confidence in detection was low). In such cases, it gracefully falls back to its universal rule-based regex splitter. This splitter is designed to be robust, offering a general solution for sentence segmentation.</li> <li>Where Logged: <code>src/chunklet/sentence_splitter/sentence_splitter.py</code></li> <li>What to do: For languages requiring highly accurate and nuanced sentence splitting, exploring a Custom Splitter might be beneficial. Otherwise, our universal splitter is a reliable option for general purposes.</li> </ul>"},{"location":"exceptions-and-warnings/#offset-total-sentences-returning-empty-list","title":"\"Offset {} &gt;= total sentences {}. Returning empty list.\"","text":"<ul> <li>What it means: This warning indicates that the provided <code>offset</code> value is greater than or equal to the total number of sentences in the text. Essentially, you've asked Chunklet-py to start processing beyond the available content.</li> <li>Where Logged: <code>src/chunklet/plain_text_chunker.py</code></li> <li>What to do: To resolve this, simply adjust your <code>offset</code> parameter to a value within the valid range of sentences.</li> </ul>"},{"location":"exceptions-and-warnings/#skipping-a-failed-task-nreason-error","title":"\"Skipping a failed task. \\nReason: {error}\"","text":"<ul> <li>What it means: During a batch operation, if an individual chunking task encounters an error, Chunklet-py is designed to skip that particular task and continue with the rest of the operation. This prevents the entire process from crashing and is a general warning triggered when <code>on_errors='skip'</code>.</li> <li>Where Logged: <code>src/chunklet/utils/batch_runner.py</code> (This is a general-purpose warning from the batch runner, used by <code>PlainTextChunker</code>, <code>DocumentChunker</code>, and <code>CodeChunker</code> when <code>on_errors='skip'</code>).</li> <li>What to do: We recommend checking the <code>Reason</code> provided in the warning for details about the failure. You might need to inspect the problematic input or adjust your <code>on_errors</code> parameter if you'd rather have the operation stop.</li> </ul>"},{"location":"exceptions-and-warnings/#skipping-document-at-paths-due-to-validation-failurenreason","title":"\"Skipping document '{}' at paths[{}] due to validation failure.\\nReason: {}\"","text":"<ul> <li>What it means: This warning appears when a file doesn't pass the initial validation checks before its content can be extracted. The <code>Reason</code> will give you more details about what went wrong (perhaps a corrupted file or an unsupported format). Don't worry, the invalid file is simply skipped, and processing continues.</li> <li>Where Logged: <code>src/chunklet/document_chunker/document_chunker.py</code></li> <li>What to do: Take a look at the file path and the reason provided in the warning. This should help you pinpoint and resolve the validation issue.</li> </ul>"},{"location":"exceptions-and-warnings/#no-valid-files-found-after-validation-returning-empty-generator","title":"\"No valid files found after validation. Returning empty generator.\"","text":"<ul> <li>What it means: During batch processing, after validating all the input paths, it was determined that there are no valid files to process. This could be because the paths were incorrect, the files were of unsupported types, or they were all skipped due to validation errors.</li> <li>Where Logged: <code>src/chunklet/document_chunker/document_chunker.py</code> (during <code>batch_chunk</code> operations)</li> <li>What to do: Verify your input file paths and ensure they are supported file types.</li> </ul>"},{"location":"exceptions-and-warnings/#splitting-oversized-block-tokens-into-sub-chunks","title":"\"Splitting oversized block ({} tokens) into sub-chunks\"","text":"<ul> <li>What it means: In <code>CodeChunker</code>'s lenient mode, a code block was too big for the <code>max_tokens</code> limit. Instead of throwing an error, Chunklet decided to be helpful and split it into smaller sub-chunks.</li> <li>Where Logged: <code>src/chunklet/experimental/code_chunker/code_chunker.py</code> (during <code>chunk</code> operations when <code>strict_mode=False</code>)</li> <li>What to do: This is usually just an informational warning. If you prefer stricter adherence to token limits for code blocks, consider setting <code>strict_mode=True</code> in <code>CodeChunker</code>.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-path-is-path-like-but-was-not-found-or-is-not-a-processable-filedirectory-skipping","title":"\"Warning: '{path}' is path-like but was not found or is not a processable file/directory. Skipping.\"","text":"<ul> <li>What it means: A path provided via the <code>--source</code> option looks like a valid path, but it either doesn't exist or it's a special file type (like a socket or a pipe) that the CLI cannot process. The CLI will skip this path.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Check that the path is correct and points to a regular file or a directory.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-path-does-not-resemble-a-valid-file-system-path-failed-heuristic-check-skipping","title":"\"Warning: '{path}' does not resemble a valid file system path (failed heuristic check). Skipping.\"","text":"<ul> <li>What it means: A string provided via the <code>--source</code> option does not appear to be a valid file system path. The CLI uses a heuristic to check for path-like strings, and this one failed. It will be skipped.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Ensure that the provided source path is a valid and correctly formatted file system path.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-no-processable-files-found-in-the-specified-sources-exiting","title":"\"Warning: No processable files found in the specified source(s). Exiting.\"","text":"<ul> <li>What it means: The CLI was unable to find any files to process from the provided <code>--source</code> paths. This could be because the directories were empty or only contained unsupported file types. The program will exit.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Verify that the source paths are correct and that they contain processable files.</li> </ul>"},{"location":"exceptions-and-warnings/#warning-no-chunks-were-generated-this-might-be-because-the-input-was-empty-or-did-not-contain-any-processable-content","title":"\"Warning: No chunks were generated. This might be because the input was empty or did not contain any processable content.\"","text":"<ul> <li>What it means: The chunking process completed, but no chunks were produced. This usually happens if the input text was empty or if the files provided for chunking contained no extractable text. The program will exit.</li> <li>Where Logged: <code>src/chunklet/cli.py</code></li> <li>What to do: Check your input to make sure it contains processable text content.</li> </ul>"},{"location":"migration/","title":"Migration Guide from v1 to v2: What's New and How to Adapt!","text":"<p>Important: Python Version Support</p> <p>Chunklet-py v2.0.0 has dropped official support for Python 3.8 and 3.9. The minimum required Python version is now 3.10. Please ensure your environment is updated to Python 3.10 or newer for compatibility.</p> <p>Hello there, fellow Chunklet enthusiast! \ud83d\udc4b Ready to explore the exciting new world of Chunklet v2? We've been hard at work, making Chunklet-py even more robust, flexible, and, dare we say, efficient! This guide is designed to walk you through all the fantastic changes and help you smoothly transition your existing code. No need to worry, we're here to support you every step of the way!</p>"},{"location":"migration/#breaking-changes-a-quick-heads-up","title":"\ud83d\udca5 Breaking Changes: A Quick Heads-Up! \ud83d\udca5","text":"<p>We've implemented some significant changes to enhance Chunklet-py's architecture and overall usability. While these updates might require minor adjustments to your existing code, we believe the improvements are well worth it!</p>"},{"location":"migration/#renamed-chunklet-class-to-plaintextchunker","title":"Renamed <code>Chunklet</code> class to <code>PlainTextChunker</code>","text":"<p>Our core chunking class now has a new, more descriptive name!</p> <p>What's new with <code>Chunklet</code>? The class you knew as <code>Chunklet</code> has been thoughtfully renamed to <code>PlainTextChunker</code>!</p> <p>Why the glow-up? We wanted to give it a name that really screams 'I chunk plain text!' This clears the stage for other awesome chunkers (like our new DocumentChunker and CodeChunker) to shine. It's all about clarity and making Chunklet's family tree a bit more logical!</p> <p>How to adapt your code? A simple find-and-replace will do the trick! Just update your imports and class instantiations:</p> Before (v1.4.0)After (v2.0.0) <pre><code>from chunklet import Chunklet\nchunker = Chunklet()\n</code></pre> <pre><code>from chunklet import PlainTextChunker\nchunker = PlainTextChunker()\n</code></pre>"},{"location":"migration/#removed-use_cache-flag-from-plaintextchunker","title":"Removed <code>use_cache</code> flag from <code>PlainTextChunker</code>","text":"<p>The <code>use_cache</code> flag has been removed from the <code>PlainTextChunker</code>.</p> <p>Where did <code>use_cache</code> go? The <code>use_cache=False</code> flag has been officially retired! You won't find it in <code>PlainTextChunker</code>'s <code>chunk</code> or <code>batch_chunk</code> methods anymore.</p> <p>Why the change? We've streamlined Chunklet-py to manage its own caching internally, optimizing for speed without requiring any manual intervention from you. This simplifies the API, allowing you to focus on the core task of chunking!</p> <p>How to adapt your code: Simply remove the <code>use_cache</code> argument from your <code>chunk</code> and <code>batch_chunk</code> calls. It's a small change that leads to a cleaner API!</p> Before (v1.4.0)After (v2.0.0) <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, use_cache=False)\n</code></pre> <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text)\n</code></pre>"},{"location":"migration/#removed-preview_sentences-method","title":"Removed <code>preview_sentences</code> method","text":"<p>The <code>preview_sentences</code> method has been removed from the main chunker class.</p> <p>Missing <code>preview_sentences</code>? This method has transitioned out of the <code>PlainTextChunker</code> (formerly <code>Chunklet</code>) instance. It's a sign of growth!</p> <p>Why the change? We've refactored the sentence splitting logic into its own dedicated utility, SentenceSplitter! This enhances modularity and flexibility, giving sentence splitting the focused attention it deserves.</p> <p>How to access sentence splitting now? You can directly utilize the <code>SentenceSplitter</code> class. It's ready for action:</p> Before (v1.4.0)After (v2.0.0) <pre><code>from chunklet import Chunklet\nchunker = Chunklet()\nsentences, warnings = chunker.preview_sentences(text, lang=\"en\")\n</code></pre> <pre><code>from chunklet import SentenceSplitter\nsplitter = SentenceSplitter()\nsentences = splitter.split(text, lang=\"en\")\n</code></pre>"},{"location":"migration/#constraint-handling-mode-removed-explicit-limits","title":"Constraint Handling: <code>mode</code> Removed, Explicit Limits","text":"<p>We've streamlined how chunking constraints are managed, moving towards more explicit control and introducing new options for granular segmentation.</p> <p>What's changed? - Removed <code>mode</code> argument: The <code>mode</code> argument (e.g., \"sentence\", \"token\", \"hybrid\") has been removed from the CLI and <code>PlainTextChunker</code>'s <code>chunk</code> and <code>batch_chunk</code> methods. The chunking strategy is now implicitly determined by the combination of constraint flags you provide (<code>max_tokens</code>, <code>max_sentences</code>, <code>max_section_breaks</code>). - No More Default Values: <code>max_tokens</code> and <code>max_sentences</code> no longer have implicit default values. You must now explicitly set these if you wish to use them. - New Constraint Flags: We've introduced <code>max_section_breaks</code> (for <code>PlainTextChunker</code> and <code>DocumentChunker</code>) and <code>max_lines</code> (for <code>CodeChunker</code>) to provide even more granular control over your chunking strategy.</p> <p>Why the change? This approach provides clearer, more explicit control over your chunking strategy, preventing unexpected behavior from implicit defaults and allowing for more precise customization. It simplifies the API by removing a redundant argument and provides more direct control over how chunks are formed.</p> <p>How to adapt your code: - Instead of specifying a <code>mode</code>, simply provide the desired constraint flags. For example, to chunk by sentences, provide <code>max_sentences</code>. To chunk by tokens, provide <code>max_tokens</code>. - Explicitly set <code>max_tokens</code> or <code>max_sentences</code> if you were relying on their previous defaults. - Utilize the new <code>max_section_breaks</code> and <code>max_lines</code> flags for advanced chunking control.</p> Before (v1.4.0)After (v2.0.0) <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, mode=\"sentence\", max_sentences=5) # Implicit max_tokens=512\n</code></pre> <pre><code>chunker = PlainTextChunker()\nchunks = chunker.chunk(text, max_sentences=5, max_tokens=512) # Mode is implicit, max_tokens explicit\n</code></pre>"},{"location":"migration/#language-detection-logic-integrated","title":"Language Detection Logic Integrated","text":"<p>The standalone language detection utility has found a new home!</p> <p>Where's the language expert? Our old <code>detect_text_language.py</code> has hung up its hat (or rather, its file path). Its brilliant brainpower is now living directly inside <code>src/chunklet/sentence_splitter/sentence_splitter.py</code>, making language detection a super-integrated part of the splitting process!</p> <p>Why the internal move? We wanted to simplify the internal magic and put our language detective right on the front lines with the sentence-splitting squad! Optimized, integrated, and ready to roll!</p> <p>Need to find your language? If you were directly importing <code>detect_text_language</code>, you'll need to update those imports. But good news for most: if you're interacting with Chunklet via the <code>PlainTextChunker</code>, all this magic happens behind the scenes! Need to explicitly detect language? <code>SentenceSplitter</code>'s got a fresh <code>detected_top_language</code> method just for you:</p> Before (v1.4.0)After (v2.0.0) <pre><code>from chunklet.utils.detect_text_language import detect_text_language\nlang, confidence = detect_text_language(text)\n</code></pre> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\nsplitter = SentenceSplitter()\nlang, confidence = splitter.detected_top_language(text)\n</code></pre>"},{"location":"migration/#custom-sentence-splitters-your-rules-our-game","title":"Custom Sentence Splitters: Your Rules, Our Game!","text":"<p>Have a unique way you prefer your sentences split? We've made it even simpler to integrate your own custom sentence splitting logic!</p> <p>Say goodbye to <code>custom_splitters</code> parameter! The <code>custom_splitters</code> parameter in our (now named <code>PlainTextChunker</code>) constructor has gracefully retired. Custom splitters now reside in a super handy, centralized registry (<code>src/chunklet/sentence_splitter/registry.py</code>)!</p> <p>Why the registry revamp? We've crafted a more robust, organized, and flexible hub for your custom splitting logic! This change thoughtfully decouples splitter registration from the <code>PlainTextChunker</code>, enabling global registration and effortless reuse of your fantastic custom splitters across all instances. It's like giving your custom splitters the VIP treatment they deserve!</p> <p>Time for a quick code update: If you were using that old <code>custom_splitters</code> parameter, it's time to embrace our new, more elegant registry system. We're confident you'll find it a breeze! For more details on how to create and register your own splitters, see the Custom Sentence Splitter documentation.</p> Before (v1.4.0)After (v2.0.0) <pre><code>import re\nfrom chunklet import Chunklet\nfrom typing import List\n\n# Define a simple custom sentence splitter\ndef my_custom_splitter(text: str) -&gt; List[str]:\n    # This is a very basic splitter for demonstration\n    # In a real scenario, this would be a more sophisticated function\n    return [s.strip() for s in re.split(r'(?&lt;=\\.)\\\\s+', text) if s.strip()]\n\n# Initialize Chunklet with the custom splitter\nchunker = Chunklet(\n    custom_splitters=[\n        {\n            \"name\": \"MyCustomEnglishSplitter\",\n            \"languages\": \"en\",\n            \"callback\": my_custom_splitter,\n        }\n    ]\n)\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences, warnings = chunker.preview_sentences(text=text, lang=\"en\")\n\nprint(\"---\" + \" Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings:\n    print(\"\\n---\" + \" Warnings ---\")\n    for warning in warnings:\n        print(warning)\n</code></pre> <pre><code>from chunklet.sentence_splitter.registry import registered_splitter\nfrom chunklet import PlainTextChunker # Updated class name\nimport re\n\n# If 'name' is not provided, the function's name ('my_awesome_splitter' in this case) will be used.\n# When using the decorator, the decorated function itself is automatically registered as the callback;\n@registered_splitter(\"en\", name=\"MyAwesomeEnglishSplitter\")\ndef my_awesome_splitter(text: str) -&gt; list[str]:\n    # Your super-duper custom splitting logic here!\n    return [s.strip() for s in re.split(r'[.!?]\\s+', text) if s.strip()]\n\n# If you prefer not to use decorators, you can use the 'register_splitter' function instead.\n# from chunklet.sentence_splitter.registry import register_splitter\n# register_splitter(\"en\", callback=my_awesome_splitter, name=\"MyAwesomeEnglishSplitter\")\n\n# Now, when you use PlainTextChunker with lang=\"en\", it will use your splitter!\nchunker = PlainTextChunker()\ntext = \"Hello world! How are you? I am fine.\"\nsentences = chunker.chunk(text, lang=\"en\", max_sentences=1) # Use chunk method\nprint(sentences)\n</code></pre>"},{"location":"migration/#exception-renames-and-changes","title":"Exception Renames and Changes","text":"<p>We've refined our exception handling to provide more clarity and specificity.   -   <code>TokenNotProvidedError</code> is now <code>MissingTokenCounterError</code>: This exception is raised when a <code>token_counter</code> is required but not provided.   -   <code>CallbackError</code> for Token Counter Failures: Previously, issues within user-provided token counters might have raised a generic <code>ChunkletError</code>. Now, a more specific <code>CallbackError</code> is raised, making debugging easier.  </p>"},{"location":"migration/#cli-usage-changes","title":"CLI Usage Changes","text":"<p>In v1.4.0, the <code>chunklet</code> CLI had a simpler structure, primarily focused on plain text. In v2.0.0, the CLI has been reorganized for clarity and to support different chunkers.</p> <p>New <code>chunk</code> command and chunker selection! A new <code>chunk</code> command is now the main entrypoint for all chunking operations. - When you provide text directly as an argument, <code>PlainTextChunker</code> is used. - When you use <code>--source</code> to provide a file path, <code>DocumentChunker</code> is used by default to handle a variety of document types. - You can use flags like <code>--code</code> to explicitly select the <code>CodeChunker</code>.</p> <p>Why the new structure? This change provides a clearer and more extensible command-line interface, making it easier to select the right chunker for your content.</p> <p>For more details, see the CLI Usage documentation.</p> Before (v1.4.0)After (v2.0.0) <pre><code>chunklet \"Your text here.\" --mode sentence --max-sentences 5\n</code></pre> <pre><code># Chunking a string uses PlainTextChunker\nchunklet chunk \"Your text here.\" --max-sentences 5\n\n# Chunking a file from a path uses DocumentChunker by default\nchunklet chunk --source your_text.txt --max-sentences 5\n</code></pre> <p>That's all for this migration guide! We truly hope these updates enhance your Chunklet-py experience and make your chunking tasks even more enjoyable. Happy chunking! \ud83c\udf89</p>"},{"location":"supported-languages/","title":"Supported Languages: A World Tour","text":"<p>Curious about the languages Chunklet-py supports? You're in the right place! We've built Chunklet-py to be quite the language expert, thanks to some fantastic third-party libraries. When we talk about language codes, we're usually using the ISO 639-1 standard (those handy two-letter codes). If you're ever wondering about other language codes, Wikipedia's List of ISO 639 language codes is a great resource.</p>"},{"location":"supported-languages/#the-all-stars-officially-supported-languages","title":"\u2b50 The All-Stars: Officially Supported Languages","text":"<p>Let's dive into the languages where Chunklet-py truly shines! Through wonderful collaborations with various libraries, we're proud to offer dedicated, high-quality splitters for over 50 languages. And if your language isn't in this impressive lineup, don't you worry \u2013 our dependable Fallback Splitter is always ready to lend a hand. Below, you'll discover the specific libraries that make this extensive language support possible.</p>"},{"location":"supported-languages/#headliner-pysbd","title":"Headliner: <code>pysbd</code>","text":"<p>Meet <code>pysbd</code>, one of our primary tools for accurate sentence boundary detection. This library is highly effective at identifying sentence endings, even in complex linguistic contexts.</p> Language Code Language Name Flag en English \ud83c\uddec\ud83c\udde7 mr Marathi \ud83c\uddee\ud83c\uddf3 hi Hindi \ud83c\uddee\ud83c\uddf3 bg Bulgarian \ud83c\udde7\ud83c\uddec es Spanish \ud83c\uddea\ud83c\uddf8 ru Russian \ud83c\uddf7\ud83c\uddfa ar Arabic \ud83c\uddf8\ud83c\udde6 am Amharic \ud83c\uddea\ud83c\uddf9 hy Armenian \ud83c\udde6\ud83c\uddf2 fa Persian (Farsi) \ud83c\uddee\ud83c\uddf7 ur Urdu \ud83c\uddf5\ud83c\uddf0 pl Polish \ud83c\uddf5\ud83c\uddf1 zh Chinese (Mandarin) \ud83c\udde8\ud83c\uddf3 nl Dutch \ud83c\uddf3\ud83c\uddf1 da Danish \ud83c\udde9\ud83c\uddf0 fr French \ud83c\uddeb\ud83c\uddf7 it Italian \ud83c\uddee\ud83c\uddf9 el Greek \ud83c\uddec\ud83c\uddf7 my Burmese (Myanmar) \ud83c\uddf2\ud83c\uddf2 ja Japanese \ud83c\uddef\ud83c\uddf5 de German \ud83c\udde9\ud83c\uddea kk Kazakh \ud83c\uddf0\ud83c\uddff sk Slovak \ud83c\uddf8\ud83c\uddf0"},{"location":"supported-languages/#special-guest-sentsplit","title":"Special Guest: <code>sentsplit</code>","text":"<p><code>sentsplit</code> complements our primary tools by providing support for additional languages. It effectively extends our coverage for diverse linguistic needs.</p> Language Code Language Name Flag ko Korean \ud83c\uddf0\ud83c\uddf7 lt Lithuanian \ud83c\uddf1\ud83c\uddf9 pt Portuguese \ud83c\uddf5\ud83c\uddf9 tr Turkish \ud83c\uddf9\ud83c\uddf7"},{"location":"supported-languages/#the-dance-troupe-indic-nlp-library","title":"The Dance Troupe: <code>Indic NLP Library</code>","text":"<p>The <code>Indic NLP Library</code> is crucial for supporting the rich and diverse languages of the Indian subcontinent. It provides comprehensive linguistic support for these languages.</p> Language Code Language Name Flag as Assamese \ud83c\uddee\ud83c\uddf3 bn Bengali \ud83c\uddee\ud83c\uddf3 gu Gujarati \ud83c\uddee\ud83c\uddf3 kn Kannada \ud83c\uddee\ud83c\uddf3 ml Malayalam \ud83c\uddee\ud83c\uddf3 ne Nepali \ud83c\uddf3\ud83c\uddf5 or Odia \ud83c\uddee\ud83c\uddf3 pa Punjabi \ud83c\uddee\ud83c\uddf3 sa Sanskrit \ud83c\uddee\ud83c\uddf3 ta Tamil \ud83c\uddee\ud83c\uddf3 te Telugu \ud83c\uddee\ud83c\uddf3"},{"location":"supported-languages/#the-versatile-voice-sentencex","title":"The Versatile Voice: <code>Sentencex</code>","text":"<p><code>Sentencex</code> significantly expands Chunklet's language capabilities. This library contributes a substantial collection of languages, ensuring broad and comprehensive coverage.</p> <p>Note</p> <p><code>Sentencex</code> is a powerful library that uses a fallback system to support a vast number of languages.  It uses a fallback system to support a vast number of languages. Many languages are mapped to fallbacks of more common languages. The list below is a curated selection of the more reliable and unique languages from <code>Sentencex</code>. It has been filtered to: *   Include only languages with an ISO 639-1 code. *   Exclude languages that are already covered by <code>pysbd</code>, <code>sentsplit</code>, or <code>Indic NLP Library</code>. *   Exclude languages that are fallbacks to other languages in the list but are not reliable enough.</p> Language Code Language Name Flag an Aragonese \ud83c\uddea\ud83c\uddf8 ca Catalan \ud83c\uddea\ud83c\uddf8 co Corsican \ud83c\uddeb\ud83c\uddf7 cs Czech \ud83c\udde8\ud83c\uddff fi Finnish \ud83c\uddeb\ud83c\uddee gl Galician \ud83c\uddea\ud83c\uddf8 io Ido \ud83c\udff3\ufe0f jv Javanese \ud83c\uddee\ud83c\udde9 li Limburgish \ud83c\uddf3\ud83c\uddf1 mo Moldovan \ud83c\uddf2\ud83c\udde9 nds Low German \ud83c\udde9\ud83c\uddea nn Norwegian Nynorsk \ud83c\uddf3\ud83c\uddf4 oc Occitan \ud83c\uddeb\ud83c\uddf7 su Sundanese \ud83c\uddee\ud83c\udde9 wa Walloon \ud83c\udde7\ud83c\uddea"},{"location":"supported-languages/#the-universal-translator-fallback-splitter","title":"The Universal Translator: Fallback Splitter","text":"<p>API Reference</p> <p>The API documentation for the universal fallback splitter can be found in the <code>FallbackSplitter</code> API docs file.</p> <p>For languages not covered by our specialized libraries, the Fallback Splitter steps in. Consider it Chunklet's adaptable solution, a rule-based regex splitter designed to provide a reasonable attempt at sentence segmentation for any language. While it may not offer the nuanced precision of language-specific tools, it's a dependable option to ensure no language is left unaddressed.</p>"},{"location":"supported-languages/#teaching-chunklet-new-tricks-custom-splitters","title":"Teaching Chunklet New Tricks: Custom Splitters","text":"<p>What if your specific language or domain requires a unique approach to sentence splitting? Or perhaps you have a very particular method in mind? No need to worry! Chunklet-py is designed to be flexible, allowing you to implement and integrate your own Custom Splitter.</p> <p>You can integrate your own sentence splitting logic in two ways:</p> <p>a) The Function Call Method (A Direct Approach):</p> <pre><code>from chunklet.sentence_splitter.registry import register_splitter\n\ndef my_custom_splitter(text: str) -&gt; list[str]:\n    # Your brilliant, custom splitting logic here\n    return text.split('.')\n\n# Teach Chunklet your new trick for English\nregister_splitter('en', callback=my_custom_splitter, name='MyCustomSplitter')\n</code></pre> <p>b) The Decorator Method (An Elegant Approach):</p> <pre><code>from chunklet.sentence_splitter.registry import registered_splitter\n\n@registered_splitter('fr', name='MyFrenchSplitter')\ndef my_french_splitter(text: str) -&gt; list[str]:\n    # Your magnifique splitting logic for French\n    return text.split('!')\n</code></pre> <p>Global Splitter Magic</p> <p>Feeling extra global? You can register a splitter with the special language code <code>xx</code>. This makes it a universal fallback that you can explicitly call by setting <code>lang='xx'</code> in your chunking operations. Pretty neat, huh?</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Welcome to the troubleshooting guide! Here you'll find solutions to common issues you might encounter while using <code>chunklet-py</code>.</p>"},{"location":"troubleshooting/#batch-processing-hangs-or-fails-on-exit","title":"Batch Processing Hangs or Fails on Exit","text":"Why does <code>batch_chunk</code> hang, and then show a <code>TypeError</code> when I try to exit with <code>Ctrl+C</code>? <p>This can happen if you are using <code>batch_chunk</code> but do not fully iterate through all the results it yields. For example, if you use a <code>break</code> statement in your loop to stop early.</p> <p>The <code>batch_chunk</code> method uses a multiprocessing pool in the background. If you exit the loop early, the generator is abandoned without being properly closed. The background processes in the pool can be left in a hanging state. When you then try to exit your script with <code>Ctrl+C</code>, the Python interpreter tries to clean up, which can lead to a <code>TypeError: 'NoneType' object is not callable</code> as it fails to shut down the orphaned processes correctly.</p> <p>Solution:</p> <p>To fix this, you must ensure that the generator is always fully consumed or explicitly closed.</p> <p>Option 1: Explicitly Close the Generator (Recommended)</p> <p>The most robust way is to wrap your loop in a <code>try...finally</code> block and call the <code>close()</code> method on the generator in the <code>finally</code> block. This ensures proper cleanup even if you break out of the loop early.</p> <p>Here is an example:</p> <pre><code>from chunklet import DocumentChunker\n\npaths = [\"path/to/your/doc1.pdf\", \"path/to/your/doc2.txt\"]\nchunker = DocumentChunker()\nchunks_generator = chunker.batch_chunk(paths)\n\ntry:\n    for i, chunk in enumerate(chunks_generator):\n        if i &gt;= 10:  # Example: Stop after 10 chunks\n            break\n        print(chunk.content)\nfinally:\n    chunks_generator.close()\n</code></pre> <pre><code>By explicitly closing the generator, you ensure that all background processes are properly cleaned up, preventing the hang and allowing your program to exit cleanly.\n</code></pre> <p>Option 2: Convert to a List</p> <p>If you don't need to process chunks as they are generated and prefer to have all chunks available at once, you can convert the generator to a list. This forces the generator to be fully consumed, ensuring the multiprocessing pool is properly terminated.</p> <pre><code>from chunklet import DocumentChunker\n\npaths = [\"path/to/your/doc1.pdf\", \"path/to/your/doc2.txt\"]\nchunker = DocumentChunker()\nall_chunks = list(chunker.batch_chunk(paths))\n\nfor i, chunk in enumerate(all_chunks):\n    if i &gt;= 10:  # Example: You can still break, but the pool is already closed\n        break\n    print(chunk.content)\n</code></pre> <pre><code>This approach is simpler if memory is not a concern and you need all chunks before proceeding.\n</code></pre> <p>Related Issues: *   mpire Issue #141: Fork-mode processes hanging *   Why your multiprocessing Pool is stuck</p>"},{"location":"whats-new/","title":"What's New in Chunklet v2.0.0! \ud83c\udf89","text":"<p>Welcome to Chunklet v2.0.0, where we've taken a giant leap forward in text and code chunking! Our team has been hard at work refining the core architecture, enhancing language support, and making the library even more robust and user-friendly. Get ready for a quick tour of the exciting new features and significant improvements:</p>"},{"location":"whats-new/#highlights-of-v200","title":"\u2728 Highlights of v2.0.0","text":"<ul> <li>Class Renaming: The <code>Chunklet</code> class has been thoughtfully renamed to <code>PlainTextChunker</code>! This change provides clearer semantics and sets the stage for other specialized chunkers. And don't worry about adapting your code \u2013 our Migration Guide is here to help you every step of the way!</li> <li>Continuation marker: \ud83d\udcd1 Improved the continuation marker logic and exposed it's value so users can define thier own or set it to an empty str to disabled it.</li> <li>Code Chunker Introduction: We're excited to introduce <code>CodeChunker</code>! \ud83e\uddd1\u200d\ud83d\udcbb This new, rule-based, language-agnostic chunker offers a significant advancement for syntax-aware code splitting. It's designed to be highly effective for your code-related RAG needs!</li> <li>Document Chunker Introduction: We're pleased to introduce <code>DocumentChunker</code>! \ud83d\udcc4 This robust tool is designed to efficiently process a wide variety of file formats, including <code>.pdf</code>, <code>.docx</code>, <code>.txt</code>, <code>.md</code>, <code>.rst</code>, <code>.rtf</code>, <code>.tex</code>, <code>.html/hml</code>, and <code>.epub</code>. It's ready to help you manage your diverse document processing needs!</li> <li>Expanded Language Support: \u00a1Hola! Bonjour! Namaste! \ud83d\udde3\ufe0f We've gone from 36+ to a staggering 50+ languages! This linguistic leap is thanks to our integrated library dream team (<code>sentsplit</code>, <code>sentencex</code>, <code>indic-nlp-library</code>) and some seriously clever fallback mechanisms. Prepare for global domination!</li> <li>New Constraint Flags: We've introduced <code>max_section_breaks</code> for PlainTextChunker and DocumentChunker, and <code>max_lines</code> for CodeChunker. These new flags provide even more granular control over how your text and code are chunked, allowing for highly customized segmentation strategies!</li> <li>Improved Error Handling: No more head-scratching! \ud83e\udd2f We've rolled out more specific exception types (like <code>FileProcessingError</code> and <code>CallbackError</code>) and a centralized batch error handling system. This means crystal-clear feedback and ultimate control when things inevitably go a little wonky.</li> <li>Flexible Batch Error Handling: Ever wished you could tell Chunklet exactly what to do when an error pops up in a batch? Now you can! The new <code>on_errors</code> parameter lets you play puppet master: <code>raise</code> a dramatic fuss, <code>skip</code> it like a pro, or <code>break</code> for a well-deserved coffee. Your kingdom, your rules!</li> <li>CLI Refactoring: Our command-line interface has been to the spa and gotten a full makeover! \u2728 It's now super streamlined with simplified input/output flags and turbocharged batch processing capabilities. Get ready for a CLI experience so smooth, it's practically butter!</li> <li>Modularity &amp; Extensibility: We've gone full LEGO! \ud83e\uddf1 The library's architecture is now incredibly modular, boasting a dedicated <code>SentenceSplitter</code> and a flexible custom splitter registry. Build your chunking dreams, piece by piece, with unparalleled ease!</li> <li>Performance &amp; Memory Optimization: Say goodbye to memory hogs and hello to lightning speed! \u26a1 We've unleashed significant refactoring, harnessing generators for batch methods to drastically slash memory footprint for even the most colossal documents. Your RAM will sing praises!</li> <li>Caching Strategy Refined: We've gone lean and mean!\u267b\ufe0f In-memory caching has been largely (but strategically) removed to prioritize raw performance optimization. Only <code>count_tokens</code> gets to keep its cozy cache. Speed is the name of the game, and we're playing to win!</li> <li>Python 3.8/3.9 Support Dropped: Time marches on, and so do we! \ud83d\udd70\ufe0f Official support for Python 3.8 and 3.9 has gracefully retired. To keep up with the latest and greatest, the minimum required Python version is now 3.10. Upgrade your Python, upgrade your life!</li> <li>CLI Flags Deprecation (--no-cache, --batch, --mode): Out with the old, in with the streamlined! \ud83d\udc4b We've tidied up the CLI by waving goodbye to the redundant <code>--no-cache</code>, the deprecated <code>--batch</code>, and the <code>--mode</code> arguments. Less clutter, more clarity, and a happier command line!</li> </ul>"},{"location":"whats-new/#want-to-see-how-far-weve-come-check-out-previous-versions","title":"\ud83d\uddfa\ufe0f Want to see how far we've come? Check out previous versions!","text":"<p>For a detailed list of every single tweak, fix, and improvement across all versions, including the nitty-gritty details of v2.0.0 and beyond, explore our comprehensive Changelog. It's like a historical novel, but for code!</p>"},{"location":"getting-started/cli/","title":"Chunklet Command Line Interface (CLI): Your Chunking Powerhouse! \ud83d\ude80","text":"<p>Welcome to the <code>chunklet</code> CLI, your one-stop shop for all things text splitting and intelligent chunking. Whether you're segmenting sentences, dicing up documents, or carving code into semantic blocks, <code>chunklet</code> has your back. It's designed to be super flexible and RAG-ready, making your LLM workflows smoother than ever.</p> <p>Before we dive into the fun stuff, you can always check your <code>chunklet</code> version or get a quick help guide.:</p> <pre><code>chunklet --version\nchunklet --help\n</code></pre> <p>You can also get specific help for each command</p> <pre><code>chunklet split --help\nchunklet chunk --help\n</code></pre>"},{"location":"getting-started/cli/#the-split-command-precision-sentence-segmentation","title":"The <code>split</code> Command: Precision Sentence Segmentation \u2702\ufe0f","text":"<p>Need to break down text into individual sentences with surgical precision? The <code>split</code> command is your go-to! It leverages <code>chunklet</code>'s powerful <code>SentenceSplitter</code> to give you clean, segmented sentences.</p>"},{"location":"getting-started/cli/#quick-facts-for-split","title":"Quick Facts for <code>split</code>","text":"<ul> <li>Operates on a raw text string or a single file (<code>--source</code>).</li> <li>Outputs sentences separated by newline characters.</li> <li>Perfect for preprocessing text before more complex chunking.</li> </ul> Flag Description Default <code>&lt;TEXT&gt;</code> The input text to split. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path to a single file to read input from. Cannot be a directory. None <code>--destination, -d &lt;PATH&gt;</code> Path to a single file to write the segmented sentences. If not provided, output goes to STDOUT. STDOUT <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). Use 'auto' for automatic detection. auto <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#scenarios-splitting-like-a-pro","title":"Scenarios: Splitting Like a Pro!","text":""},{"location":"getting-started/cli/#scenario-1-splitting-text-directly-and-multilingually","title":"Scenario 1: Splitting Text Directly (and Multilingually!)","text":"<p>Segment a direct text input containing multiple languages into individual sentences, leveraging automatic language detection.</p> <pre><code>chunklet split \"This is the first sentence. Here is the second sentence, in French. C'est la vie! \u00bfC\u00f3mo est\u00e1s?\" --lang auto\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-splitting-a-file-and-saving-the-output","title":"Scenario 2: Splitting a File and Saving the Output","text":"<p>Process a document and save its segmented sentences to a new file. Easy peasy!</p> <pre><code>chunklet split --source my_novel_chapter.txt --destination sentences.txt --lang en\n</code></pre>"},{"location":"getting-started/cli/#the-chunk-command-your-intelligent-chunking-workhorse","title":"The <code>chunk</code> Command: Your Intelligent Chunking Workhorse!","text":"<p>The <code>chunk</code> command is where the real magic happens! It's your versatile tool for breaking down text, documents, and even code into RAG-ready chunks. The \"flavor\" of chunking (plain text, document, or code) is determined by the flags you provide.</p>"},{"location":"getting-started/cli/#key-flags-for-chunk-the-essentials","title":"Key Flags for <code>chunk</code> (The Essentials!)","text":"Flag Description Default <code>&lt;TEXT&gt;</code> The input text to chunk. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path(s) to one or more files or directories to read input from. Repeat for multiple sources (e.g., <code>-s file1.txt -s dir/</code>). None <code>--destination, -d &lt;PATH&gt;</code> Path to a file (for single output) or a directory (for batch output) to write the chunks. If not provided, output goes to STDOUT. STDOUT <code>--max-tokens</code> Maximum number of tokens per chunk. Applies to all chunking strategies. (Must be &gt;= 12) None <code>--max-sentences</code> Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (Must be &gt;= 1) None <code>--max-section-breaks</code> Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (Must be &gt;= 1) None <code>--overlap-percent</code> Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. 20.0 <code>--offset</code> Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. 0 <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto) auto <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#general-text-document-chunking-default-or-with-doc","title":"General Text &amp; Document Chunking (Default or with <code>--doc</code>) \ud83d\udcc4","text":"<p>This is your bread-and-butter chunking for everyday text and diverse document types.</p> <ul> <li>Default Behavior: If neither <code>--doc</code> nor <code>--code</code> is specified, <code>chunklet</code> uses the PlainTextChunker for direct text input. The <code>PlainTextChunker</code> is designed to transform unruly text into perfectly sized, context-aware chunks.</li> <li>Document Power-Up: Activate the DocumentChunker with the <code>--doc</code> flag to process <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, and <code>.rtf</code> files! It intelligently extracts text and then applies the same robust chunking logic.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-document-power-up","title":"Key Flags for Document Power-Up","text":"Flag Description Default <code>--doc</code> Activate the <code>DocumentChunker</code> for multi-format file processing. False <code>--n-jobs</code> Number of parallel jobs for batch processing. (None uses all available cores) None <code>--on-errors</code> How to handle errors during batch processing: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise"},{"location":"getting-started/cli/#scenarios-text-document-chunking-in-action","title":"Scenarios: Text &amp; Document Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-basic-text-chunking-with-token-limits-and-overlap","title":"Scenario 1: Basic Text Chunking with Token Limits and Overlap","text":"<p>Chunk a long text string into segments, ensuring no chunk exceeds 200 tokens, with a healthy 15% overlap for context.</p> <pre><code>chunklet chunk \"The quick brown fox jumps over the lazy dog. This is the first sentence. The second sentence is a bit longer. And this is the third one. Finally, the fourth sentence concludes our example. The last sentence is here to finish the text.\"\n  --max-tokens 200 \\\n  --overlap-percent 15\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-chunking-a-pdf-document-with-sentence-and-section-break-limits","title":"Scenario 2: Chunking a PDF Document with Sentence and Section Break Limits","text":"<p>Process a PDF document, ensuring chunks are no more than 10 sentences or 2 section breaks, and save the output to a file.</p> <pre><code>chunklet chunk --doc --source my_report.pdf \\\n  --max-sentences 10 \\\n  --max-section-breaks 2 \\\n  --destination processed_report_chunks.txt\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-batch-processing-a-directory-of-documents-with-error-handling","title":"Scenario 3: Batch Processing a Directory of Documents (with Error Handling!)","text":"<p>Process all supported documents within a directory, saving the chunks to a new folder. If any file causes an error, <code>chunklet</code> will gracefully skip it and continue!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/my/project_docs \\\n  --destination ./processed_chunks \\\n  --n-jobs 4 \\\n  --on-errors skip \\\n  --max-tokens 1024 \\\n  --metadata # Don't forget your metadata!\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-a-text-file-with-a-specific-language-and-metadata","title":"Scenario 4: Chunking a Text File with a Specific Language and Metadata","text":"<p>Chunk a French text file, limiting by tokens, and include all the juicy metadata for later analysis.</p> <pre><code>chunklet chunk --source french_article.txt \\\n  --lang fr \\\n  --max-tokens 300 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#code-chunking-with-code","title":"Code Chunking (with <code>--code</code>) \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>For the developers, by the developers! The CodeChunker is a language-agnostic wizard that breaks your source code into semantically meaningful blocks (functions, classes, etc.). Activate it with the <code>--code</code> flag.</p> <ul> <li>Heads Up! This mode is primarily token-based. <code>--max-sentences</code>, <code>--max-section-breaks</code>, and <code>--overlap-percent</code> are generally ignored here, as code structure takes precedence.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-code-chunking","title":"Key Flags for Code Chunking","text":"Flag Description Default <code>--code</code> Activate the <code>CodeChunker</code> for structurally-aware code segmentation. False <code>--max-lines</code> Maximum number of lines per chunk. (Must be &gt;= 5) None <code>--max-functions</code> Maximum number of functions per chunk. (Must be &gt;= 1) None <code>--docstring-mode</code> Docstring processing strategy: <code>summary</code> (first line), <code>all</code>, or <code>excluded</code>. all <code>--strict</code> If <code>True</code>, raise an error when structural blocks exceed <code>--max-tokens</code>. If <code>False</code>, split oversized blocks. True <code>--include-comments</code> Include comments in output chunks. True"},{"location":"getting-started/cli/#scenarios-code-chunking-in-action","title":"Scenarios: Code Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-chunking-a-single-python-file-excluding-comments","title":"Scenario 1: Chunking a Single Python File, Excluding Comments","text":"<p>Get a clean, comment-free view of your code's structure. Perfect for quick reviews!</p> <pre><code>chunklet chunk --code --source my_script.py \\\n  --max-tokens 512 \\\n  --include-comments False\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-chunking-a-codebase-allowing-oversized-blocks","title":"Scenario 2: Batch Chunking a Codebase, Allowing Oversized Blocks","text":"<p>Process an entire code repository, letting <code>chunklet</code> split any functions or classes that are just too long, and save everything to a dedicated folder.</p> <pre><code>chunklet chunk --code \\\n  --source ./my_awesome_repo \\\n  --destination ./code_chunks \\\n  --max-tokens 1024 \\\n  --strict False \\\n  --n-jobs 8 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-extracting-function-summaries-docstring-mode-summary","title":"Scenario 3: Extracting Function Summaries (Docstring Mode: Summary)","text":"<p>Focus on the \"what\" of your functions by only including the first line of their docstrings.</p> <pre><code>chunklet chunk --code --source utils.py \\\n  --docstring-mode summary \\\n  --max-functions 1 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-by-lines-and-functions-for-granular-control","title":"Scenario 4: Chunking by Lines and Functions for Granular Control","text":"<p>For super-fine-grained control, chunk a file by both maximum lines and maximum functions per chunk.</p> <pre><code>chunklet chunk --code --source main.go \\\n  --max-lines 100 \\\n  --max-functions 2 \\\n  --max-tokens 700\n</code></pre>"},{"location":"getting-started/cli/#advanced-system-hooks","title":"\ud83d\udee0\ufe0f Advanced System Hooks","text":"<p>These flags are your secret weapons for scaling up operations, integrating with external tools, and getting the most out of your chunked data. They apply to the <code>chunk</code> command.</p>"},{"location":"getting-started/cli/#system-hook-flags","title":"System Hook Flags","text":"Flag Description Default <code>--tokenizer-command</code> A shell command string for token counting. It must take text via STDIN and output the integer count via STDOUT. None <code>--n-jobs</code> Number of parallel processes to use during batch operations. (None uses all available CPU cores) None <code>--on-errors</code> Defines batch error handling: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for debugging or process detail. False"},{"location":"getting-started/cli/#scenarios-unleashing-advanced-power","title":"Scenarios: Unleashing Advanced Power!","text":""},{"location":"getting-started/cli/#scenario-1-verbose-debugging-for-a-single-file","title":"Scenario 1: Verbose Debugging for a Single File","text":"<p>When things get tricky, crank up the verbosity to see exactly what <code>chunklet</code> is doing under the hood while chunking a specific file.</p> <pre><code>chunklet chunk --source problematic_file.txt \\\n  --max-tokens 100 \\\n  --verbose\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-processing-with-parallelism-and-error-skipping","title":"Scenario 2: Batch Processing with Parallelism and Error Skipping","text":"<p>Process a large collection of diverse documents, leveraging all your CPU cores, and gracefully skip any problematic files without halting the entire operation. Plus, get all the metadata!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/massive_document_archive \\\n  --destination ./final_chunks \\\n  --n-jobs -1 # Use all available cores!\n  --on-errors skip \\\n  --max-tokens 512 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-processing-multiple-specific-files-with-advanced-hooks","title":"Scenario 3: Processing Multiple Specific Files with Advanced Hooks","text":"<p>Process a selection of individual files, explicitly listing each one, and apply advanced chunking parameters. This demonstrates how to handle a non-directory batch of files, ensuring each is processed with metadata and error handling.</p> <pre><code>chunklet chunk --doc \\\n  --source my_document.pdf \\\n  --source another_report.docx \\\n  --source plain_text_notes.txt \\\n  --destination ./processed_specific_files \\\n  --max-tokens 700 \\\n  --metadata \\\n  --on-errors skip\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-custom-token-counting-with-an-external-script","title":"Scenario 4: Custom Token Counting with an External Script","text":"<p>Align <code>chunklet</code>'s chunk sizes perfectly with your LLM's token limits using any external tokenizer you can imagine!</p> <p>Create your external script (e.g., <code>my_llm_tokenizer.py</code>):</p> <pre><code># my_llm_tokenizer.py\nimport sys\nimport tiktoken # Or your LLM's specific tokenizer library\n\n# Read text from stdin\ntext = sys.stdin.read()\n\n# Replace with your actual token counting logic (e.g., for OpenAI's GPT models)\nencoding = tiktoken.encoding_for_model(\"gpt-4\")\ntoken_count = len(encoding.encode(text))\n\nprint(token_count) # Must print only the integer count\n</code></pre> <p>Now, run <code>chunklet</code> with your custom tokenizer:</p> <pre><code>chunklet chunk \\\n  --text \"This is a super important piece of text that needs precise token counting for my large language model.\"\n  --max-tokens 50 \\\n  --tokenizer-command \"python ./my_llm_tokenizer.py\" \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#diving-deeper-into-metadata","title":"Diving Deeper into Metadata","text":"<p>Want to know exactly what kind of rich context <code>chunklet</code> attaches to your chunks? From source paths and character spans to document-specific properties and code AST details.</p> <p>\ud83d\udc49 Head over to the Metadata in Chunklet-py guide to unlock all its secrets!</p> API Reference <p>For a deep dive into the <code>chunklet</code> CLI, its commands, and all the nitty-gritty details, check out the full API documentation</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Ready to get Chunklet-py up and running? Fantastic! This guide will walk you through the installation process, making it as smooth as possible.</p>"},{"location":"getting-started/installation/#the-easy-way","title":"The Easy Way","text":"<p>The most straightforward method to install Chunklet-py is by using <code>pip</code>:</p> <pre><code># Install and verify version\npip install chunklet-py\nchunklet --version\n</code></pre> <p>And that's all there is to it! You're now ready to start using Chunklet-py.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Chunklet-py offers optional dependencies to unlock additional functionalities, such as document processing or code chunking. You can install these extras using the following syntax:</p> <ul> <li>Document Processing: For handling <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, and other document formats:     <pre><code>pip install \"chunklet-py[document]\"\n</code></pre></li> <li>Code Chunking: For advanced code analysis and chunking features:     <pre><code>pip install \"chunklet-py[code]\"\n</code></pre></li> <li>All Extras: To install all optional dependencies:     <pre><code>pip install \"chunklet-py[document,code]\"\n</code></pre></li> </ul>"},{"location":"getting-started/installation/#the-alternative-way","title":"The Alternative Way","text":"<p>For those who prefer to build from source, you can clone the repository and install it manually:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install .\n</code></pre> <p>But why would you want to do that? The easy way is so much easier.</p>"},{"location":"getting-started/installation/#contributing-to-chunklet-py","title":"Contributing to Chunklet-py","text":"<p>Interested in helping make Chunklet-py even better? That's fantastic! Before you dive in, please take a moment to review our Contributing Guide. Here's how you can set up your development environment:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install -e \".[dev]\"\n</code></pre> <p>This command installs Chunklet-py in \"editable\" mode, ensuring that any changes you make to the source code are immediately reflected. The <code>[dev]</code> option includes all the necessary dependencies for running tests and building the documentation.</p> <p>Now, go forth and code! And remember, good developers always write tests. (Even in a Python project, we appreciate all forms of excellent code examples!)</p>"},{"location":"getting-started/metadata/","title":"Metadata in Chunklet-py","text":"<p>Chunklet-py automatically generates and manages metadata for each chunk, providing valuable context about its origin and characteristics. This guide explains how metadata is handled, its structure, and how you can leverage it effectively in your applications.</p> <p>Each chunk generated by Chunklet-py is encapsulated in a <code>Box</code> object. This <code>Box</code> object contains a <code>metadata</code> attribute, which is a dictionary holding various pieces of information about the chunk. You can access this metadata using either dot notation (<code>chunk.metadata</code>) or dictionary-style access (<code>chunk[\"metadata\"]</code>).</p>"},{"location":"getting-started/metadata/#common-metadata","title":"Common Metadata","text":"<p>Regardless of the chunking strategy used, every chunk generated by Chunklet-py includes the following common metadata fields:</p> <ul> <li><code>chunk_num</code> (int): A unique, sequential identifier for the chunk within its original source. This helps in ordering and referencing chunks.</li> <li><code>span</code> (tuple[int, int]): Represents the start and end character indices of the chunk within the original text. This is particularly useful for pinpointing the exact location of a chunk in the source material.</li> <li><code>source</code> (str): Indicates the origin of the chunk.<ul> <li>If the chunk was generated from a file and processed by DocumentChunker or CodeChunker, this will be the absolute path to that file.</li> <li>If the chunk was generated from a direct text input via the CLI, this will be <code>\"stdin\"</code>.</li> <li>When PlainTextChunker is used programmatically with a string, it does not inherently assign a <code>source</code> unless explicitly provided via <code>base_metadata</code>. If not provided, the <code>source</code> metadata field might be absent or default to an empty string/<code>None</code> depending on the <code>base_metadata</code> passed.</li> <li>For <code>CodeChunker</code> when a file is processed but the source cannot be determined (e.g., an internal error), it might be <code>\"N/A\"</code>.</li> </ul> </li> </ul>"},{"location":"getting-started/metadata/#plaintextchunker-metadata","title":"PlainTextChunker Metadata","text":"<p>When using the <code>PlainTextChunker</code>, chunks will primarily include the Common Metadata fields: <code>chunk_num</code> and <code>span</code>. The <code>source</code> field is only included if explicitly provided via the <code>base_metadata</code> parameter during the chunking call. <code>PlainTextChunker</code> does not add any additional metadata beyond these basic identifiers.</p>"},{"location":"getting-started/metadata/#documentchunker-metadata","title":"DocumentChunker Metadata","text":"<p>The <code>DocumentChunker</code> is designed to process various file types and, in addition to the Common Metadata fields, it extracts rich, file-specific metadata. This additional metadata provides deeper insights into the document's characteristics and origin.</p> <ul> <li><code>section_count</code> (int): For multi-section documents (e.g., multi-page PDFs, multi-chapter EPUBs), this indicates the total number of sections in the document.</li> <li><code>curr_section</code> (int): For multi-section documents, this denotes the current section number of the chunk.</li> </ul> <p>The exact fields extracted depend on the file type:</p> <ul> <li>PDF Files: If present in the document, metadata fields such as <code>title</code>, <code>author</code>, <code>creator</code>, <code>producer</code>, <code>publisher</code>, <code>created</code>, <code>modified</code>, and <code>page_count</code> will be extracted. While <code>pdfminer.six</code> supports a wider range, these are the primary fields included. For more details, refer to the pdfminer.six documentation.</li> <li>EPUB Files: Following the Dublin Core standard, if available in the document, fields like <code>title</code>, <code>creator</code>, <code>contributor</code>, <code>publisher</code>, <code>date</code>, and <code>rights</code> will be extracted. These are a selection of the supported metadata. For more details, refer to the ebooklib tutorial.</li> <li>DOCX Files: Core properties such as <code>title</code>, <code>author</code>, <code>publisher</code>, <code>last_modified_by</code>, <code>created</code>, <code>modified</code>, <code>rights</code>, and <code>version</code> will be extracted if they exist in the document. These are the key properties included from the broader set supported by <code>python-docx</code>. For more details, refer to the python-docx documentation.</li> </ul> <p>Important Note on Optional Metadata</p> <p>These file-specific metadata fields are optional and may not be present in every document. For robust access, it is highly recommended to use the dictionary's <code>get()</code> method (e.g., <code>chunk.metadata.get(\"author\")</code>) to avoid <code>KeyError</code>s if a field is missing.</p>"},{"location":"getting-started/metadata/#codechunker-metadata","title":"CodeChunker Metadata","text":"<p>The <code>CodeChunker</code> is specialized for processing code files and, in addition to the Common Metadata fields, it provides rich, code-specific metadata to help understand the structure and context of code chunks.</p> <p>The additional metadata fields include:</p> <ul> <li><code>tree</code> (str): A string representation of the abstract syntax tree (AST) or a relevant portion of it for the code chunk. This provides structural context for the chunk.</li> <li><code>start_line</code> (int): The starting line number of the code chunk in the original file.</li> <li><code>end_line</code> (int): The ending line number of the code chunk in the original file.</li> </ul> <p>This code-specific metadata is automatically included in the <code>metadata</code> dictionary of each <code>Box</code> object when you chunk code using <code>CodeChunker</code>.</p>"},{"location":"getting-started/metadata/#cli-metadata-output","title":"CLI Metadata Output","text":"<p>When using the <code>chunklet</code> CLI, the inclusion and type of metadata in the output are dynamically determined by your input and the chunker flags you provide.</p> <ul> <li> <p>Controlling Metadata Output: The <code>--metadata</code> flag (e.g., <code>chunklet chunk \"...\" --metadata</code>) is your primary control for whether metadata is included in the final output.</p> <ul> <li>If <code>--metadata</code> is used, metadata will be displayed alongside your chunks (either printed to stdout or saved in <code>.json</code> files if a directory is specified for <code>--destination</code>).</li> <li>If <code>--metadata</code> is not used, only the chunk content will be output.</li> </ul> </li> <li> <p>Metadata Varies by Input and Chunker:</p> <ul> <li>Direct Text Input (e.g., <code>chunklet chunk \"Your text here...\"</code>): When you provide text directly, the <code>PlainTextChunker</code> is used. The metadata will primarily consist of Common Metadata fields like <code>chunk_num</code> and <code>span</code>. The <code>source</code> will be <code>\"stdin\"</code>.</li> <li>File Input with <code>--doc</code> (e.g., <code>chunklet chunk --doc --source document.pdf</code>): The <code>DocumentChunker</code> is employed. In addition to Common Metadata, you'll receive rich, file-specific metadata (e.g., <code>title</code>, <code>author</code>, <code>page_count</code> for PDFs) as detailed in the DocumentChunker Metadata section.</li> <li>File Input with <code>--code</code> (e.g., <code>chunklet chunk --code --source code.py</code>): The <code>CodeChunker</code> is used. Metadata will include Common Metadata along with code-specific fields like <code>tree</code>, <code>start_line</code>, <code>end_line</code>, and <code>source_path</code>, as described in the CodeChunker Metadata section.</li> </ul> </li> </ul> <p>This dynamic approach ensures you get the most relevant contextual information for your chunks, tailored to how you're using Chunklet-py.</p>"},{"location":"getting-started/programmatic/","title":"Overview","text":"<p>Welcome to the programmatic side of Chunklet-py! This is where you integrate Chunklet's intelligent text and code chunking capabilities directly into your Python applications. Whether you're building RAG pipelines, data processing workflows, or custom AI solutions, Chunklet provides a flexible and powerful API to meet your needs.</p> <ul> <li> <p> Sentence Splitter</p> <p>Precisely split raw text into semantically meaningful sentences, fluently handling over 50 languages with intelligent detection and complex structures.</p> <p> Learn More</p> </li> <li> <p> Plain Text Chunker</p> <p>Process and chunk plain text documents into perfectly sized, context-aware segments. It offers flexible constraint-based chunking with intelligent overlap, optimizing text for downstream tasks like embedding and LLM input.</p> <p> Learn More</p> </li> <li> <p> Document Chunker</p> <p>Work with diverse document formats including <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, and <code>.rtf</code>, transforming them into structured, chunked outputs.</p> <p> Learn More</p> </li> <li> <p> Code Chunker</p> <p>Intelligently chunk source code, preserving its logical structure and context. This language-agnostic and lightweight tool is perfect for analysis, documentation, or AI model training.</p> <p> Learn More</p> </li> </ul> <p>Ready to get started? Click on any of the cards above to explore the specific chunker that fits your needs!</p>"},{"location":"getting-started/programmatic/code_chunker/","title":"Code Chunker","text":""},{"location":"getting-started/programmatic/code_chunker/#cracking-the-code-with-ease","title":"Cracking the Code with Ease","text":"<p>Ever found yourself staring at a massive codebase, feeling like you're trying to decipher ancient hieroglyphs? The <code>CodeChunker</code> is here to be your trusty sidekick, transforming those tangled messes of code into clean, understandable, and semantically meaningful chunks.</p> <p>This isn't just any old regex splitter! The <code>CodeChunker</code> is a sophisticated, language-agnostic tool that truly understands the structure of your code. It doesn't need a PhD in every programming language; instead, it uses a clever set of rules and patterns to identify functions, classes, and other logical blocks, no matter what language you're working with.</p>"},{"location":"getting-started/programmatic/code_chunker/#the-brains-of-the-operation","title":"The Brains of the Operation","text":"<p>The <code>CodeChunker</code> is powered by a set of design principles that make it both powerful and flexible:</p> <ul> <li>Rule-Based and Language-Agnostic: It uses a set of universal patterns to find code blocks, so it works with a ton of languages right out of the box \u2013 Python, C++, Java, JavaScript, and many more.</li> <li>Convention-Aware: It assumes your code follows standard formatting conventions, which allows it to be surprisingly accurate without needing a full-blown parser for every language.</li> <li>Structurally Neutral: It doesn't get bogged down in the details of mixed-language code. If you've got SQL or JavaScript embedded in your Python, it just treats it as part of the block, keeping things clean and simple.</li> <li>Flexible Constraint-Based Chunking: The <code>CodeChunker</code> offers precise control over how your code is segmented. You can define limits based on <code>max_tokens</code>, <code>max_lines</code>, or <code>max_functions</code>, allowing you to combine these constraints for unparalleled precision over your chunk's size and structural content.</li> <li>Annotation-Aware: It's smart about comments and docstrings, using them to understand the structure of your code without getting them mixed up with the code itself.</li> <li>Flexible Source Input: The <code>source</code> parameter can accept a raw code string, a file path as a string, or a <code>pathlib.Path</code> object. If a path is provided, <code>CodeChunker</code> automatically reads the file content.</li> <li>Strict Mode Control: By default, <code>CodeChunker</code> operates in a strict mode that prevents splitting structural blocks (like functions or classes) even if they exceed the token limit, raising a <code>TokenLimitError</code> instead. This can be disabled by setting <code>strict=False</code>.</li> </ul>"},{"location":"getting-started/programmatic/code_chunker/#constraint-based-chunking-explained","title":"Constraint-Based Chunking Explained","text":"<p>The <code>CodeChunker</code> operates primarily in a structural chunking mode, allowing you to define chunk boundaries based on code structure. This process can be further constrained by the following options:</p> Constraint Value Requirement Description <code>max_tokens</code> <code>int &gt;= 12</code> The maximum number of tokens allowed per chunk. When provided, the <code>CodeChunker</code> ensures that code blocks do not exceed this token limit, splitting them if necessary based on their structural elements. <code>max_lines</code> <code>int &gt;= 5</code> The maximum number of lines allowed per chunk. This constraint helps manage chunk size by line count, which is particularly useful for code where lines often correlate with logical units. <code>max_functions</code> <code>int &gt;= 1</code> The maximum number of functions allowed per chunk. This constraint helps to group related functions together, or split them into separate chunks if the limit is exceeded. <p>Constraint Requirement</p> <p>You must provide at least one of these limits (<code>max_tokens</code>, <code>max_lines</code>, or <code>max_functions</code>) when calling <code>chunk</code> or <code>batch_chunk</code>. Failing to specify any limit will result in an <code>InvalidInputError</code>. The <code>CodeChunker</code> will ensure your code blocks are appropriately sized according to the provided constraints.</p> <p>The <code>CodeChunker</code> has two main methods: <code>chunk</code> for processing a single text and <code>batch_chunk</code> for processing multiple codes. Both methods return a generator that yields a <code>Box</code> object for each chunk. The <code>Box</code> object has two main keys: <code>content</code> (str) and <code>metadata</code> (dict). For detailed information about metadata structure and usage, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#single-run","title":"Single Run","text":"<p>This section demonstrates how to use the <code>CodeChunker</code> to process a single code input with various constraints. To begin, it's important to note the flexibility of the <code>source</code> parameter, which is designed to accommodate your preferred method of input. It can accept:</p> <ul> <li>A string containing the source code directly.</li> <li>A string representing the absolute path to a file.</li> <li>A <code>pathlib.Path</code> object pointing to a file.</li> </ul> <p>When a file path (either as a string or a <code>pathlib.Path</code> object) is provided, the <code>CodeChunker</code> will automatically handle reading the file's content for you.</p> <pre><code>from pathlib import Path\n\n# All of the following are valid:\nchunks_from_string = chunker.chunk(\"def my_func():\\n  return 1\")\nchunks_from_path_str = chunker.chunk(\"/path/to/your/code.py\")\nchunks_from_path_obj = chunker.chunk(Path(\"/path/to/your/code.py\"))\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-lines","title":"Chunking by Lines","text":"<p>Here's how you can use <code>CodeChunker</code> to chunk code by the number of lines:</p> <pre><code>from chunklet.experimental.code_chunker import CodeChunker\n\nPYTHON_CODE = '''\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n'''\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk(\n    PYTHON_CODE,                \n    max_lines=10,               # (1)!\n    include_comments=True,      # (2)!\n    docstring_mode=\"all\",       # (3)!\n    strict=False,               # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> <ol> <li>Sets the maximum number of lines per chunk. If a code block exceeds this limit, it will be split.</li> <li>Set to True to include comments in the output chunks.</li> <li><code>docstring_mode=\"all\"</code> ensures that complete docstrings, with all their multi-line details, are preserved in the code chunks. Other options are <code>\"summary\"</code> to include only the first line, or <code>\"excluded\"</code> to remove them entirely. Default is \"all\".</li> <li>When <code>strict=False</code>, structural blocks (like functions or classes) that exceed the limit set will be split into smaller chunks. If <code>strict=True</code> (default), a <code>TokenLimitError</code> would be raised instead.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\n\nMetadata:\n    chunk_num: 1\n    tree: global\n    start_line: 1\n    end_line: 7\n    span: (0, 38)\n    source: N/A\n\n--- Chunk 2 ---\nContent:\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n\nMetadata:\n    chunk_num: 2\n    tree: global\n    \u2514\u2500 class Calculator\n    start_line: 8\n    end_line: 14\n    span: (38, 192)\n    source: N/A\n\n--- Chunk 3 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n\nMetadata:\n    chunk_num: 3\n    tree: global\n    \u2514\u2500 class Calculator\n       \u2514\u2500 def add(\n    start_line: 15\n    end_line: 23\n    span: (192, 444)\n    source: N/A\n\n--- Chunk 4 ---\nContent:\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\n\nMetadata:\n    chunk_num: 4\n    tree: global\n    \u251c\u2500 class Calculator\n    \u2502  \u2514\u2500 def multiply(\n    \u2514\u2500 def standalone_function(\n    start_line: 24\n    end_line: 30\n    span: (444, 603)\n    source: N/A\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>CodeChunker</code>: <pre><code>chunker = CodeChunker(verbose=True)\n</code></pre></p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-tokens","title":"Chunking by Tokens","text":"<p>Here's how you can use <code>CodeChunker</code> to chunk code by the number of tokens:</p> <pre><code># Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk(\n    PYTHON_CODE,                \n    max_tokens=50,                        \n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n\nstart_line: 1\nend_line: 14\nspan: (0, 192)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u251c\u2500 def add(\n   \u2514\u2500 def multiply(\n\nstart_line: 15\nend_line: 27\nspan: (192, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-functions","title":"Chunking by Functions","text":"<p>This constraint is useful when you want to ensure that each chunk contains a specific number of functions, helping to maintain logical code units.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_functions=1,\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def add(\n\nstart_line: 1\nend_line: 23\nspan: (0, 444)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def multiply(self, x, y):\n\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def multiply(\n\nstart_line: 24\nend_line: 27\nspan: (444, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#combining-multiple-constraints","title":"Combining Multiple Constraints","text":"<p>The real power of <code>CodeChunker</code> comes from combining multiple constraints. This allows for highly specific and granular control over how your code is chunked. Here are a few examples of how you can combine different constraints.</p>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-and-tokens","title":"By Lines and Tokens","text":"<p>This is useful when you want to limit by both the number of lines and the overall token count, whichever is reached first.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=5,\n    max_tokens=50\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-and-functions","title":"By Lines and Functions","text":"<p>This combination is great for ensuring that chunks don't span across too many functions while also keeping the line count in check.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=10,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-tokens-and-functions","title":"By Tokens and Functions","text":"<p>A powerful combination for structured code where you want to respect function boundaries while adhering to a strict token budget.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_tokens=100,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#by-lines-tokens-and-functions","title":"By Lines, Tokens, and Functions","text":"<p>For the ultimate level of control, you can combine all three constraints. The chunking will stop as soon as any of the three limits is reached.</p> <pre><code>chunks = chunker.chunk(\n    PYTHON_CODE,\n    max_lines=8,\n    max_tokens=150,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#batch-run","title":"Batch Run","text":"<p>While the <code>chunk</code> method is perfect for processing a single text, the <code>batch_chunk</code> method is designed for efficiently processing multiple code sources in parallel. It returns a generator, allowing you to process large volumes of code without exhausting memory.</p> <p>Given we have the following code snippets saved as individual files in a <code>code_examples</code> directory:</p>"},{"location":"getting-started/programmatic/code_chunker/#cpp_calculatorcpp","title":"cpp_calculator.cpp","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n\n// Function 1: Simple greeting\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\n// Function 2: Logic block\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1; // Error code\n    }\n    int result = a + b;\n    return result;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#javadataprocessorjava","title":"JavaDataProcessor.java","text":"<pre><code>package com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n    // Constructor\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n    // Method 1: Getter\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n    // Method 2: Core processing logic\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n        // Assume processing logic here\n        return true;\n    }\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#js_utilsjs","title":"js_utils.js","text":"<pre><code>// Utility function\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n// Main function with control flow\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n    // Loop structure\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#go_configgo","title":"go_config.go","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\n// Struct definition\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n// Function 1: Factory function\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n// Function 2: Method on the struct\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\\\n\", c.Timeout, c.Retries)\n}\n</code></pre> <p>We can process them all at once by providing a list of paths to the <code>batch_chunk</code> method. Assuming these files are saved in a <code>code_examples</code> directory: <pre><code>from chunklet.code_chunker import CodeChunker\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\n# Initialize the chunker\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nsources = [\n    \"code_examples/cpp_calculator.cpp\",\n    \"code_examples/JavaDataProcessor.java\",\n    \"code_examples/js_utils.js\",\n    \"code_examples/go_config.go\",\n]\n\nchunks = chunker.batch_chunk(\n    sources=sources,\n    max_tokens=50,      \n    n_jobs=2,               # (1)!\n    on_errors=\"raise\",      # (2)!\n    show_progress=True,     # (3)!\n)\n\n# Output the results\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content.strip()}\\n\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"  {k}: {v}\")\n    print()\n</code></pre></p> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to view output <pre><code>Chunking ...:   0%|          | 0/4 [00:00, ?it/s]\n--- Chunk 1 ---\nContent:\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1;\n    }\n    int result = a + b;\n    return result;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n  start_line: 1\n  end_line: 17\n  span: (0, 329)\n  source: code_examples/cpp_calculator.cpp\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 2 ---\nContent:\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u2514\u2500 function processArray(\n  start_line: 1\n  end_line: 19\n  span: (0, 372)\n  source: code_examples/js_utils.js\n\n--- Chunk 3 ---\nContent:\npackage com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n\n        return true;\n    }\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package com\n\u2514\u2500 public class DataProcessor\n   \u251c\u2500 public DataProcessor(\n   \u251c\u2500 public String getPath(\n   \u2514\u2500 public boolean process(\n\n  start_line: 1\n  end_line: 25\n  span: (0, 500)\n  source: code_examples/JavaDataProcessor.java\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 4 ---\nContent:\npackage main\n\nimport (\n    \"fmt\"\n)\n\n\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\n\", c.Timeout, c.Retries)\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package main\n\u251c\u2500 type Config\n\u2514\u2500 func NewConfig(\n\n  start_line: 1\n  end_line: 26\n  span: (0, 382)\n  source: code_examples/go_config.go\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00, 19.71it/s]\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#separator","title":"Separator","text":"<p>The <code>separator</code> parameter allows you to specify a custom value to be yielded after all chunks for a given text have been processed. This is particularly useful when processing multiple texts in a batch, as it helps to clearly distinguish between the chunks originating from different input texts in the output stream.</p> <p>note</p> <p><code>None</code> cannot be used as a separator.</p> <pre><code>from chunklet.code_chunker import CodeChunker\nfrom more_itertools import split_at\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nSIMPLE_SOURCES = [\n    # Python: Simple Function Definition Boundary\n    '''\ndef greet_user(name):\n    \"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n''',\n    # C#: Simple Method and Class Boundary\n    '''\npublic class Utility\n{\n    // C# Method\n    public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n'''\n]\n\nchunker = CodeChunker(token_counter=simple_token_counter)\ncustom_separator = \"---END_OF_SOURCE---\"\n\nchunks_with_separators = chunker.batch_chunk(\n    sources=SIMPLE_SOURCES,\n    max_tokens=20,\n    separator=custom_separator,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, code_chunks in enumerate(chunk_groups):\n    if code_chunks: # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in code_chunks:\n            print(f\"Content:\\n {chunk.content}\\n\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>Chunking ...:   0%|          | 0/2 [00:00, ?it/s]\n--- Chunks for Document 1 ---\nContent:\ndef greet_user(name):\n\"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 def greet_user(\\n', 'start_line': 1, 'end_line': 5, 'span': (0, 124), 'source': 'N/A'}\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 1/2 [00:00,  9.48it/s]\n--- Chunks for Document 2 ---\nContent:\npublic class Utility\n{\n\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 public class Utility\\n', 'start_line': 1, 'end_line': 4, 'span': (0, 41), 'source': 'N/A'}\nContent:\n     public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n\nMetadata: {'chunk_num': 2, 'tree': 'global\\n\u2514\u2500 public class Utility\\n   \u2514\u2500 public int Add(\\n', 'start_line': 5, 'end_line': 10, 'span': (41, 133), 'source': 'N/A'}\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00,  1.92it/s]\n</code></pre> <p>What are the limitations of CodeChunker?</p> <p>While powerful, <code>CodeChunker</code> isn't magic! It assumes your code is reasonably well-behaved (syntactically conventional). Highly obfuscated, minified, or macro-generated sources might give it a headache. Also, nested docstrings or comment blocks can be a bit tricky for it to handle perfectly.</p>"},{"location":"getting-started/programmatic/code_chunker/#inspiration","title":"Inspiration","text":"<p>The <code>CodeChunker</code> draws inspiration from various projects and concepts in the field of code analysis and segmentation. These influences have shaped its design principles and capabilities:</p> <ul> <li>code_chunker by Camel AI</li> <li>code_chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> API Reference <p>For a deep dive into the <code>CodeChunker</code> class, its methods, and all the nitty-gritty details, check out the full API documentation.</p>"},{"location":"getting-started/programmatic/document_chunker/","title":"Document Chunker","text":"<p>Psst... Read <code>PlainTextChunker</code> First!</p> <p>Think of <code>DocumentChunker</code> as the cool, sophisticated older sibling of <code>PlainTextChunker</code>. While <code>DocumentChunker</code> knows how to handle all sorts of fancy file formats, the real brains of the chunking operation\u2014the part that does the heavy lifting of splitting text by sentences, tokens, and section breaks\u2014lives inside <code>PlainTextChunker</code>.</p> <p>So, before you dive in here, do yourself a favor and get cozy with the PlainTextChunker documentation first. It\u2019s the key to unlocking the full power of chunking! We\u2019ll cover the awesome document-specific stuff here, but the core concepts are all explained over there.</p>"},{"location":"getting-started/programmatic/document_chunker/#streamlined-document-processing","title":"Streamlined Document Processing","text":"<p>Are you tired of managing multiple tools for different file types? The <code>DocumentChunker</code> offers a unified solution for processing a wide array of document formats. From PDFs and DOCX files to EPUBs and more, it handles diverse documents efficiently and elegantly.</p> <p>Consider the <code>DocumentChunker</code> as the orchestrator of your document processing workflow. It intelligently identifies the file type, employs the appropriate specialist to extract and convert the text (often to Markdown if possible), and then seamlessly passes the processed content to the <code>PlainTextChunker</code> for further segmentation. This provides a smooth, end-to-end solution for transforming your documents into clean, chunked data.</p> <p>Ready to simplify your document processing? Let's explore how!</p>"},{"location":"getting-started/programmatic/document_chunker/#whats-in-the-magic-bag","title":"What's in the Magic Bag?","text":"<p>The <code>DocumentChunker</code> comes with a bag of tricks that makes document processing a breeze:</p> <ul> <li>Multi-Format Mastery: From the corporate world of DOCX to the academic realm of PDFs, this chunker speaks a multitude of file languages. It can handle <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, and even <code>.rtf</code> files.</li> <li>Metadata Enrichment: It's not just about the text. The <code>DocumentChunker</code> automatically enriches your chunks with valuable metadata, like the source file path and page numbers for PDFs.</li> <li>Bulk Processing Beast: Got a whole folder of documents to process? No sweat. This chunker is built for bulk, efficiently processing multiple documents in a single go.</li> <li>Pluggable Processors: Have a weird, esoteric file format that nobody else has heard of? The <code>DocumentChunker</code> is ready for the challenge. You can plug in your own custom processors to handle any file type you can throw at it.</li> </ul> <p>The <code>DocumentChunker</code> has two main methods: <code>chunk</code> for processing a single file and <code>batch_chunk</code> for processing multiple files. Both methods return a generator that yields a <code>Box</code> object for each chunk. The <code>Box</code> object has two main keys: <code>content</code> (str) and <code>metadata</code> (dict).</p> <p>Supported Processors and Batch Processing</p> <p>The <code>DocumentChunker</code> supports the following file types out-of-the-box: - <code>.docx</code>: Processed by <code>DocxProcessor</code> - <code>.epub</code>: Processed by <code>EpubProcessor</code> - <code>.pdf</code>: Processed by <code>PDFProcessor</code></p> <p>These processors can also provide unique optional metadata, enriching your chunks with valuable context. For more details, check out the Metadata in Chunklet-py documentation.</p> <p>Due to the streaming nature of these processors (they yield content page by page or in blocks), it is highly recommended to use them with the <code>batch_chunk</code> method. Attempting to use these processors with the <code>chunk</code> method (which expects a single string input) will result in a <code>FileProcessingError</code> as the <code>chunk</code> method is not designed to consume the generator output of these processors.</p>"},{"location":"getting-started/programmatic/document_chunker/#single-run","title":"Single Run","text":"<p>The <code>chunk</code> method of <code>DocumentChunker</code> shares most of its arguments with <code>PlainTextChunker.chunk</code>. The key differences are:</p> <ul> <li>The first argument is <code>path</code> (a string representing the file path or a <code>pathlib.Path</code> object) instead of <code>text</code>.</li> <li>The <code>base_metadata</code> argument is not available, as metadata is automatically extracted and managed by the document processors.</li> </ul> <p>Remember, just like with <code>PlainTextChunker</code>, at least one of <code>max_sentences</code>, <code>max_tokens</code>, or <code>max_section_breaks</code> must be provided to avoid an <code>InvalidInputError</code>.</p> <p>Let's say we have the following text content:</p> <pre><code>The quick brown fox jumps over the lazy dog. This is the first sentence, and it's a classic.\nHere is the second sentence, which is a bit longer and more descriptive. And this is the third one, short and sweet.\nThe fourth sentence concludes our initial example, but we need more text to demonstrate the chunking effectively.\nLet's add a fifth sentence to make the text a bit more substantial. The sixth sentence will provide even more content for our test.\nThis is the seventh sentence, and we are still going. The eighth sentence is here to make the text even longer.\nFinally, the ninth sentence will be the last one for this example, making sure we have enough content to create multiple chunks.\n</code></pre> <p>Save this content into a file named <code>sample_text.txt</code>. Here's how you would use it with <code>DocumentChunker</code>:</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\n# Assuming sample_text.txt is in the same directory as your script\nfile_path = \"sample_text.txt\"\n\nchunker = DocumentChunker()\n\nchunks = chunker.chunk(\n    path=file_path,\n    lang=\"auto\",             # (1)!\n    max_sentences=4,         # (2)!\n    overlap_percent=0,       # (3)!\n    offset=0                 # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Instructs the chunker to automatically detect the language of the input text. While convenient, explicitly setting the language (e.g., <code>lang=\"en\"</code>) can improve accuracy and performance for known languages.</li> <li>Sets the maximum number of sentences allowed per chunk. In this example, chunks will contain at most four sentences.</li> <li>Configures the chunker to have no overlap between consecutive chunks. The default behavior includes a 20% overlap to maintain context across chunks.</li> <li>Specifies that chunking should start from the very beginning of the text (the first sentence). The default is 0.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 1, 'span': (0, 209)}\nContent: The quick brown fox jumps over the lazy dog.\nThis is the first sentence, and it's a classic.\nHere is the second sentence, which is a bit longer and more descriptive.\nAnd this is the third one, short and sweet.\n\n--- Chunk 2 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 2, 'span': (210, 509)}\nContent: The fourth sentence concludes our initial example, but we need more text to demonstrate the chunking effectively.\nLet's add a fifth sentence to make the text a bit more substantial.\nThe sixth sentence will provide even more content for our test.\nThis is the seventh sentence, and we are still going.\n\n--- Chunk 3 ---\nMetadata: {'source': 'sample_text.txt', 'chunk_num': 3, 'span': (510, 696)}\nContent: The eighth sentence is here to make the text even longer.\nFinally, the ninth sentence will be the last one for this example, making sure we have enough content to create multiple chunks.\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>DocumentChunker</code>: <pre><code>chunker = DocumentChunker(verbose=True)\n</code></pre></p> <p>Customizing the Continuation Marker</p> <p>You can customize the continuation marker, which is prepended to clauses that don't fit in the previous chunk. To do this, pass the <code>continuation_marker</code> parameter to the chunker's constructor.</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"[...]\")\n</code></pre> <p>If you don't want any continuation marker, you can set it to an empty string:</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"\")\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#batch-run","title":"Batch Run","text":"<p>While the <code>chunk</code> method is perfect for processing a single document, the <code>batch_chunk</code> method is designed for efficiently processing multiple documents in parallel. It returns a generator, allowing you to process large volumes of documents without exhausting memory.</p> <p>The <code>batch_chunk</code> method shares most of its core arguments with <code>PlainTextChunker.batch_chunk</code> (like <code>lang</code>, <code>max_tokens</code>, <code>max_sentences</code>, <code>max_section_breaks</code>, <code>overlap_percent</code>, <code>offset</code>, <code>token_counter</code>, <code>separator</code>, <code>n_jobs</code>, <code>show_progress</code>, and <code>on_errors</code>). The key differences are:</p> <ul> <li>The first argument is <code>paths</code> (a list of strings or <code>pathlib.Path</code> objects representing the file paths) instead of <code>texts</code>.</li> <li>The <code>base_metadata</code> argument is not available, as metadata is automatically extracted and managed by the document processors.</li> </ul> <p>For this example, we'll be using some sample files from our repository's samples directory.</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\npaths = [\n    \"samples/Lorem Ipsum.docx\",\n    \"samples/What_is_rst.rst\",\n    \"samples/minimal.epub\",\n    \"samples/sample-pdf-a4-size.pdf\",\n]                                     # (1)!\n\nchunker = DocumentChunker(token_counter=word_counter) # (2)!\n\nchunks_generator = chunker.batch_chunk(\n    paths=paths,\n    overlap_percent=30,\n    max_sentences=12,\n    max_tokens=256,\n    max_section_breaks=1,\n    n_jobs=2,                    # (3)!\n    on_errors=\"raise\",           # (4)!\n    show_progress=False,         # (5)!\n)\n\nfor i, chunk in enumerate(chunks_generator):\n    if i == 10:                      # (6)!\n        break\n\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content}\\n\")\n    print(f\"Metadata:\")\n    for k, v in chunk.metadata.items():\n        print(f\" | {k}: {v}\")\n\n    print()\n\nchunks_generator.close()          # (7)!\nprint(\"\\nAnd so on...\")\n</code></pre> <ol> <li>If your files are saved elsewhere make sure to update that accoordingly.</li> <li>Initializes the <code>DocumentChunker</code> with a <code>token_counter</code> function. This is crucial when using <code>max_tokens</code> for chunking.</li> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> <li>We break the loop early to demonstrate the cleanup mechanism.</li> <li>Explicitly close the generator to ensure proper cleanup.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nContent:\nQuantum Aristoxeni ingenium consumptum videmus in musicis?\nLorem ipsum dolor sit amet, consectetur adipiscing elit.\nQuid nunc honeste dicit?\nTum Torquatus: Prorsus, inquit, assentior; Duo Reges: constructio interrete.\nIam in altera philosophiae parte.\nSed haec omittamus; Haec para/doca illi, nos admirabilia dicamus.\nNihil sane.\n**Expressa vero in iis aetatibus, quae iam confirmatae sunt.**\nSit sane ista voluptas.\nNon quam nostram quidem, inquit Pomponius iocans; An tu me de L.\nSed haec omittamus; Cave putes quicquam esse verius.\n[Image - 1]\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/Lorem Ipsum.docx\n author: train11\n last_modified_by: Microsoft Office User\n created: 2012-08-07 08:50:00+00:00\n modified: 2019-12-05 23:29:00+00:00\n section_count: 1\n curr_section: 1\n\n--- Chunk 2 ---\nContent:\nxml version=\"1.0\" encoding=\"utf-8\"?\nReStructuredText (rst): plain text markup\nReStructuredText (rst): plain text markup=====\n[The tiny table of contents](#top)\n* [1   What is reStructuredText?](#what-is-restructuredtext)\n* [2   What is it good for?](#what-is-it-good-for)\n* [3   Show me some formatting examples](#show-me-some-formatting-examples)\n* [4   Where can I learn more?](#where-can-i-learn-more)\n* [5   Show me some more stuff, please](#show-me-some-more-stuff-please)\n[1   What is reStructuredText?](#toc-entry-1)=====\nAn easy-to-read, what-you-see-is-what-you-get plaintext markup syntax\nand parser system, abbreviated *rst*.\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 1\n\n--- Chunk 3 ---\nContent:\nAn easy-to-read,\nwhat-you-see-is-what-you-get plaintext markup syntax\nand parser system,\nabbreviated *rst*.\nIn other words, using a simple\ntext editor, documents can be created which\n* are easy to read in text editor and\n* can be *automatically* converted to\n+ html and\n+ latex (and therefore pdf)\n[2   What is it good for?](#toc-entry-2)=====\nreStructuredText can be used, for example, to\n\nMetadata:\n chunk_num: 2\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 2\n\n--- Chunk 4 ---\nContent:\n... + latex (and therefore pdf)\n[2   What is it good for?](#toc-entry-2)=====\nreStructuredText can be used,\nfor example,\nto\n* write technical documentation (so that it can easily be offered as a\npdf file or a web page)\n* create html webpages without knowing html\n* to document source code\n[3   Show me some formatting examples](#toc-entry-3)=====\nYou can highlight text in *italics* or, to provide even more emphasis\nin **bold**.\n\nMetadata:\n chunk_num: 3\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 3\n\n--- Chunk 5 ---\nContent:\n... [3   Show me some formatting examples](#toc-entry-3)=====\nYou can highlight text in *italics* or,\nto provide even more emphasis\nin **bold**.\nOften, when describing computer code, we like to use a\nfixed space font to quote code snippets.\nWe can also include footnotes [[1]](#footnote-1).\nWe could include source code files\n(by specifying their name) which is useful when documenting code.\nWe\ncan also copy source code verbatim (i.e. include it in the rst\ndocument) like this:```\n\nMetadata:\n chunk_num: 4\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 4\n\n--- Chunk 6 ---\nContent:\n... which is useful when documenting code.\nWe\ncan also copy source code verbatim (i.e. include it in the rst\ndocument)\nlike this:```\nint main ( int argc, char *argv[] ) {\nprintf(\"Hello World\\n\");\nreturn 0;}```\nWe have already seen at itemised list in section [What is it good\nfor?\n](#what-is-it-good-for).\nEnumerated list and descriptive lists are supported as\n\nMetadata:\n chunk_num: 5\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 5\n\n--- Chunk 7 ---\nContent:\nWe have already seen at itemised list in section [What is it good\nfor?\n](#what-is-it-good-for).\nEnumerated list and descriptive lists are supported as\nwell.\nIt provides very good support for including html-links in a\nvariety of ways.\nAny section and subsections defined can be linked to,\nas well.\n[4   Where can I learn more?](#toc-entry-4)=====\nreStructuredText is described at\n&lt;http://docutils.sourceforge.net/rst.html&gt;.\n\nMetadata:\n chunk_num: 6\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 6\n\n--- Chunk 8 ---\nContent:\n... as well.\n[4   Where can I learn more?](#toc-entry-4)=====\nreStructuredText is described at\n&lt;http://docutils.sourceforge.net/rst.html&gt;.\nWe provide some geeky small\nprint in this footnote [[2]](#footnote-2).\n[5   Show me some more stuff, please](#toc-entry-5)=====\nWe can also include figures:\n![image.png](image.png)\nThe magnetisation in a small ferromagnetic disk.\nThe diametre is of the order of 120 nanometers and the material is Ni20Fe\n80.\nPng is a file format that is both acceptable for html pages as well as fo\nr (pdf)latex.\n\nMetadata:\n chunk_num: 7\n span: (-1, -1)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 7\n\n--- Chunk 9 ---\nContent:\n... ![image.png](image.png)\nThe magnetisation in a small ferromagnetic disk.\nThe diametre is of the order of 120 nanometers and the material is Ni20Fe\n80.\nPng is a file format that is both acceptable for html pages as well as fo\nr (pdf)latex.\n\n---\n|  |  |\n| --- | --- |\n| [[1]](#footnote-reference-1) | although there isn't much point of using\n a footnote here.|\n|  |  |\n| --- | --- |\n| [[2]](#footnote-reference-2) | Random facts:   * Emacs provides an rst\n mode * when converting rst to html, a style sheet can be provided (there\n is a similar feature for latex) * rst can also be converted into XML * th\ne recommended file extension for rst is .txt |\n\nMetadata:\n chunk_num: 8\n span: (2499, 3150)\n source: samples/What_is_rst.rst\n section_count: 1\n curr_section: 8\n\n--- Chunk 10 ---\nContent:\nTable of Contents=====\n1. [Chapter 1](chapter_1.xhtml)\n2. [Chapter 2](chapter_2.xhtml)\n3. [Copyright](copyright.xhtml)\n\nMetadata:\n chunk_num: 1\n span: (-1, -1)\n source: samples/minimal.epub\n title: Sample .epub Book\n creator: Thomas Hansen\n section_count: 4\n curr_section: 1\n\nAnd so on...\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p> <p>Token Counter Requirement</p> <p>When using the <code>max_tokens</code> constraint, a <code>token_counter</code> function is essential. This function, which you provide, should accept a string and return an integer representing its token count. Failing to provide a <code>token_counter</code> will result in a <code>MissingTokenCounterError</code>.</p> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/document_chunker/#separator","title":"Separator","text":"<p>The <code>separator</code> parameter is also available in <code>DocumentChunker</code>. It is used to yield a custom value after all chunks for a given document have been processed, which is useful for distinguishing between documents in a batch run.</p> <p>Since the functionality is very similar to the <code>PlainTextChunker</code>, please refer to the PlainTextChunker documentation for a detailed explanation and code examples.</p>"},{"location":"getting-started/programmatic/document_chunker/#custom-processors","title":"Custom Processors","text":"<p>You can provide your own custom document processors to <code>DocumentChunker</code>. This is useful if you have a specialized processor for a particular file type that you want to prioritize over <code>DocumentChunker</code>'s built-in processors.</p> <p>Global Registry: Be Mindful of Side Effects</p> <p>Custom processors are registered globally. This means that once you register a processor, it's available everywhere in your application. Be mindful of potential side effects if you're registering processors in different parts of your codebase, especially in multi-threaded or long-running applications.</p> <p>To use a custom processor, you leverage the <code>@registry.register</code> decorator. This decorator allows you to register your function for one or more file extensions directly. Your custom processor function must accept a single <code>file_path</code> parameter (str) and return a <code>tuple[str | list[str], dict]</code> containing extracted text (or list of texts for multi-section documents) and a metadata dictionary.</p> <p>Important constraints</p> <ul> <li>Your function must accept exactly one required parameter (the file path)</li> <li>Optional parameters with default values are allowed</li> <li>File extensions must start with a dot (e.g., <code>.json</code>, <code>.custom</code>)</li> <li>Lambda functions are not supported unless you provide a <code>name</code> parameter</li> <li>The metadata dictionary will be merged with common metadata (chunk_num, span, source)</li> <li>For multi-section documents, return a list of strings - each will be processed as a separate section</li> <li>If an error occurs during the document processing (e.g., an issue with the custom processor function), a <code>CallbackError</code> will be raised</li> </ul> <pre><code>import os\nimport re\nimport json\nimport tempfile\nfrom chunklet.document_chunker import DocumentChunker, CustomProcessorRegistry\n\n\nregistry = CustomProcessorRegistry()\n\n# Define a simple custom processor for .json files\n@registry.register(\".json\", name=\"MyJSONProcessor\")\ndef my_json_processor(file_path: str) -&gt; tuple[str, dict]:\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Assuming the json has a \"text\" field with paragraphs\n    text_content = \"\\n\".join(data.get(\"text\", []))\n    metadata = data.get(\"metadata\", {})\n    metadata[\"source\"] = file_path\n    return text_content, metadata\n\n# A longer and more complex JSON sample\njson_data = {\n    \"metadata\": {\n        \"document_id\": \"doc-12345\",\n        \"created_at\": \"2025-11-05\"\n    },\n    \"text\": [\n        \"This is the first paragraph of our longer JSON sample. It contains multiple sentences to test the chunking process.\",\n        \"The second paragraph introduces a new topic. We are exploring the capabilities of custom processors in the chunklet library.\",\n        \"Finally, the third paragraph concludes our sample. We hope this demonstrates the flexibility of the system in handling various data formats.\"\n    ]\n}\n\nchunker = DocumentChunker()\n\n# Use a temporary file\nwith tempfile.NamedTemporaryFile(mode='w+', suffix=\".json\") as tmp:\n    json.dump(json_data, tmp)\n    tmp.seek(0)\n    tmp_path = tmp.name\n\n    chunks = chunker.chunk(\n        path=tmp_path,\n        max_sentences=5,\n    )\n\n    for i, chunk in enumerate(chunks):\n        print(f\"--- Chunk {i+1} ---\")\n        print(f\"Content:\\n{chunk.content}\\n\")\n        print(f\"Metadata:\\n{chunk.metadata}\")\n        print()\n\n# Optionally unregister\nregistry.unregister(\".json\")\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\nThis is the first paragraph of our longer JSON sample.\nIt contains multiple sentences to test the chunking process.\nThe second paragraph introduces a new topic.\nWe are exploring the capabilities of custom processors in the chunklet library.\nFinally, the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 1, 'span': (0, 292)}\n\n--- Chunk 2 ---\nContent:\n... the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 2, 'span': (250, 292)}\n</code></pre> <p>Registering Without the Decorator</p> <p>If you prefer not to use decorators, you can directly use the <code>registry.register()</code> method. This is particularly useful when registering processors dynamically.</p> <pre><code>from chunklet.document_chunker import CustomProcessorRegistry\n\nregistry = CustomProcessorRegistry()\n\ndef my_other_processor(file_path: str) -&gt; tuple[str, dict]:\n    # ... your logic ...\n    return \"some text\", {\"source\": file_path}\n\nregistry.register(my_other_processor, \".custom\", name=\"MyOtherProcessor\")\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#customprocessorregistry-methods-summary","title":"<code>CustomProcessorRegistry</code> Methods Summary","text":"<ul> <li><code>processors</code>: Returns a shallow copy of the dictionary of registered processors.</li> <li><code>is_registered(ext: str)</code>: Checks if a processor is registered for the given file extension, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], ReturnType] | None = None, *exts: str, name: str | None = None)</code>: Registers a processor callback for one or more file extensions.</li> <li><code>unregister(*exts: str)</code>: Removes processor(s) from the registry.</li> <li><code>clear()</code>: Clears all registered processors from the registry.</li> <li><code>extract_data(file_path: str, ext: str)</code>: Processes a file using a registered processor, returning the extracted data and the name of the processor used.</li> </ul> API Reference <p>For a deep dive into the <code>DocumentChunker</code> class, its methods, and all the nitty-gritty details, check out the full API documentation.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/","title":"Plain Text Chunker","text":""},{"location":"getting-started/programmatic/plain_text_chunker/#taming-your-text-with-precision","title":"Taming Your Text with Precision","text":"<p>Do you have large blocks of text that need to be organized into smaller, more manageable pieces? The <code>PlainTextChunker</code> is designed to help you transform unruly text into perfectly sized, context-aware chunks. Whether you're preparing data for a Large Language Model (LLM) or building a search index, this tool aims to simplify your workflow.</p> <p>Our approach goes beyond simple text splitting; it's about intelligent segmentation. The <code>PlainTextChunker</code> prioritizes context, working diligently to preserve the meaning and natural flow of your text.</p> <p>Ready to bring order to your text? Let's explore how!</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#where-plaintextchunker-excels","title":"Where <code>PlainTextChunker</code> Excels","text":"<p>The <code>PlainTextChunker</code> is engineered with a robust set of features, making it highly adaptable for diverse text-chunking needs:</p> <ul> <li>Flexible Constraint-Based Chunking: Imagine having ultimate control over your text chunks! <code>PlainTextChunker</code> empowers you to define exactly how your text is segmented. You can set limits based on sentence count, token count, or even those handy Markdown section breaks. The best part? You can combine them in any way you like, giving you unparalleled precision over your chunk's size and structure.</li> <li>Intelligent Overlap for Context Preservation: Features clause-level overlap to ensure smooth transitions and maintain contextual coherence between consecutive chunks.</li> <li>Extensive Multilingual Support: Leveraging the capabilities of our sentence splitter, this chunker supports a broad spectrum of languages, enhancing its global applicability.</li> <li>Customizable Token Counting: Facilitates easy integration of custom token counting functions, allowing precise alignment with the specific tokenization requirements of different Large Language Models.</li> <li>Optimized Parallel Processing: Designed to efficiently handle large text volumes by utilizing multiple processors, significantly accelerating the chunking workflow.</li> <li>Memory-Conscious Operation: Processes even extensive documents with high memory efficiency by yielding chunks iteratively, thereby minimizing overall memory footprint.</li> </ul>"},{"location":"getting-started/programmatic/plain_text_chunker/#constraint-based-chunking-explained","title":"Constraint-Based Chunking Explained","text":"<p><code>PlainTextChunker</code> uses a constraint-based approach to chunking. You can mix and match constraints to get the perfect chunk size. Here's a quick rundown of the available constraints:</p> Constraint Value Requirement Description <code>max_sentences</code> <code>int &gt;= 1</code> This constraint is all about sentence power! You tell Chunklet how many sentences you want per chunk, and it gracefully groups them together, making sure your ideas flow smoothly from one chunk to the next. <code>max_tokens</code> <code>int &gt;= 12</code> Got a strict token budget? This constraint is your best friend! Chunklet carefully adds sentences to your chunk, keeping a watchful eye on the token limit. If a sentence is a bit too chatty, it'll even politely split it at clause boundaries to keep everything perfectly snug. <code>max_section_breaks</code> <code>int &gt;= 1</code> Perfect for structured documents! This constraint lets you limit the number of Markdown-style section breaks (like headings <code>##</code> or horizontal rules <code>---</code>) in each chunk. It's a fantastic way to keep your document's structure intact while chunking. <p>The <code>PlainTextChunker</code> has two main methods: <code>chunk</code> for processing a single text and <code>batch_chunk</code> for processing multiple texts. Both methods return a generator that yields a <code>Box</code> object for each chunk. The <code>Box</code> object has two main keys: <code>content</code> (str) and <code>metadata</code> (dict). For detailed information about metadata structure and usage, see the Metadata guide.</p> <p>Constraint Requirement</p> <p>At least one limit mode (e.g., <code>max_sentences</code>, <code>max_tokens</code>, or <code>max_section_breaks</code>) must be provided when calling <code>chunk</code> or <code>batch_chunk</code>. Failing to specify any limit mode will result in an <code>InvalidInputError</code>.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#single-run","title":"Single Run","text":"<p>Given the following text for our examples:</p> <pre><code>text = \"\"\"\n# Introduction to Chunking\n\nThis is the first paragraph of our document. It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization. We aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information. It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings. Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\n\nBeyond basic splitting, advanced techniques involve understanding the document's structure. For instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents. This section will delve into such methods.\n\n### Overlap Considerations\n\nTo ensure smooth transitions between chunks, an overlap mechanism is often employed. This means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n---\n\n# Conclusion\n\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data. Experiment with different constraints to find the optimal strategy for your needs.\n\"\"\"\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-sentences","title":"Chunking by Sentences","text":"<p>Here's how you can use <code>PlainTextChunker</code> to chunk text by the number of sentences:</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\nchunker = PlainTextChunker()  # (1)!\n\nchunks = chunker.chunk(\n    text=text,\n    lang=\"auto\",             # (2)!\n    max_sentences=2,\n    overlap_percent=0,       # (3)!\n    offset=0                 # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Setting <code>verbose=True</code> enables detailed logging, which is <code>False</code> by default, showing internal processes like language detection.</li> <li>Instructs the chunker to automatically detect the language of the input text. While convenient, explicitly setting the language (e.g., <code>lang=\"en\"</code>) can improve accuracy and performance for known languages.</li> <li>Configures the chunker to have no overlap between consecutive chunks. The default behavior includes a 20% overlap to maintain context across chunks.</li> <li>Specifies that chunking should start from the very beginning of the text (the first sentence). The default is 0.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 73)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (74, 259)}\nContent: It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (260, 370)}\nContent: ## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (371, 529)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (531, 748)}\nContent: There are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 6, 'span': (749, 786)}\nContent: ---\n\n## Advanced Chunking Techniques\n\n--- Chunk 7 ---\nMetadata: {'chunk_num': 7, 'span': (788, 995)}\nContent: Beyond basic splitting, advanced techniques involve understanding the document's structure.\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\n\n--- Chunk 8 ---\nMetadata: {'chunk_num': 8, 'span': (996, 1066)}\nContent: This section will delve into such methods.\n\n### Overlap Considerations\n\n--- Chunk 9 ---\nMetadata: {'chunk_num': 9, 'span': (1068, 1264)}\nContent: To ensure smooth transitions between chunks, an overlap mechanism is often employed.\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n--- Chunk 10 ---\nMetadata: {'chunk_num': 10, 'span': (749, 763)}\nContent: ---\n\n# Conclusion\n\n--- Chunk 11 ---\nMetadata: {'chunk_num': 11, 'span': (1285, 1459)}\nContent: In conclusion, mastering chunking is key to unlocking the full potential of your text data.\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>DocumentChunker</code>: <pre><code>chunker = PlainTextChunker(verbose=True)\n</code></pre></p>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-tokens","title":"Chunking by Tokens","text":"<p>Token Counter Requirement</p> <p>When using the <code>max_tokens</code> constraint, a <code>token_counter</code> function is essential. This function, which you provide, should accept a string and return an integer representing its token count. Failing to provide a <code>token_counter</code> will result in a <code>MissingTokenCounterError</code>.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#setup","title":"Setup","text":"<pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\n# Simple counter for demonstration purpose\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nchunker = PlainTextChunker(token_counter=word_counter)         # (1)!\n</code></pre> <ol> <li>Initializes <code>PlainTextChunker</code> with a custom <code>word_counter</code> function. This function will be used to count tokens when <code>max_tokens</code> is used.</li> </ol> <pre><code>chunks = chunker.chunk(\n    text=text,\n    lang=\"auto\",\n    max_tokens=12,\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 291)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (256, 573)}\nContent: ...\n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking,\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (531, 880)}\nContent: There are several strategies for chunking,\nincluding splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nBeyond basic splitting, advanced techniques involve understanding the document's structure.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (808, 1153)}\nContent: ... advanced techniques involve understanding the document's structure.\n\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n### Overlap Considerations\nTo ensure smooth transitions between chunks, an overlap mechanism is often employed.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1109, 1377)}\nContent: ... an overlap mechanism is often employed.\n\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n---\n\n# Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 6, 'span': (1296, 1459)}\nContent: ... mastering chunking is key to unlocking the full potential of your text data.\n\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to the <code>chunk</code> method. within the <code>chunk</code> method call (e.g., <code>chunker.chunk(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the <code>chunk</code> method, the one in the <code>chunk</code> method will be used.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#chunking-by-section-breaks","title":"Chunking by Section Breaks","text":"<p>This constraint is useful for documents structured with Markdown headings or thematic breaks.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_section_breaks=2\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 503)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (371, 753)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own,\nwhich is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (678, 1038)}\nContent: Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nBeyond basic splitting, advanced techniques involve understanding the document's structure.\nFor instance, preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (890, 1269)}\nContent: ... preserving section breaks can significantly improve the quality of chunks for hierarchical documents.\nThis section will delve into such methods.\n\n### Overlap Considerations\nTo ensure smooth transitions between chunks, an overlap mechanism is often employed.\nThis means that a portion of the previous chunk is included in the beginning of the next, providing continuity.\n\n---\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1239, 1459)}\nContent: ... providing continuity.\n\n---\n\n# Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\nExperiment with different constraints to find the optimal strategy for your needs.\n</code></pre> <p>Adding Base Metadata</p> <p>You can pass a <code>base_metadata</code> dictionary to the <code>chunk</code> method. This metadata will be included in the <code>metadata</code> of each chunk. For example: <code>chunker.chunk(..., base_metadata={\"source\": \"my_document.txt\"})</code>. For more details on metadata structure and available fields, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#combining-multiple-constraints","title":"Combining Multiple Constraints","text":"<p>The real power of <code>PlainTextChunker</code> comes from combining multiple constraints. This allows for highly specific and granular control over how your text is chunked. Here are a few examples of how you can combine different constraints.</p> <p>Token Counter Requirement</p> <p>Remember, whenever you use the <code>max_tokens</code> constraint, you must provide a <code>token_counter</code> function.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-and-tokens","title":"By Sentences and Tokens","text":"<p>This is useful when you want to limit by both the number of sentences and the overall token count, whichever is reached first.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=5,\n    max_tokens=100\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-and-section-breaks","title":"By Sentences and Section Breaks","text":"<p>This combination is great for ensuring that chunks don't span across too many sections while also keeping the sentence count in check.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=10,\n    max_section_breaks=2\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-tokens-and-section-breaks","title":"By Tokens and Section Breaks","text":"<p>A powerful combination for structured documents where you want to respect section boundaries while adhering to a strict token budget.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_tokens=256,\n    max_section_breaks=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#by-sentences-tokens-and-section-breaks","title":"By Sentences, Tokens, and Section Breaks","text":"<p>For the ultimate level of control, you can combine all three constraints. The chunking will stop as soon as any of the three limits is reached.</p> <pre><code>chunks = chunker.chunk(\n    text=text,\n    max_sentences=8,\n    max_tokens=200,\n    max_section_breaks=2\n)\n</code></pre> <p>Customizing the Continuation Marker</p> <p>You can customize the continuation marker, which is prepended to clauses that don't fit in the previous chunk. To do this, pass the <code>continuation_marker</code> parameter to the chunker's constructor.</p> <pre><code>chunker = PlainTextChunker(continuation_marker=\"[...]\")\n</code></pre> <p>If you don't want any continuation marker, you can set it to an empty string:</p> <pre><code>chunker = PlainTextChunker(continuation_marker=\"\")\n</code></pre>"},{"location":"getting-started/programmatic/plain_text_chunker/#batch-run","title":"Batch Run","text":"<p>While the <code>chunk</code> method is perfect for processing a single text, the <code>batch_chunk</code> method is designed for efficiently processing multiple texts in parallel. It returns a generator, allowing you to process large volumes of text without exhausting memory. It shares most of its core arguments with <code>chunk</code> (like <code>max_sentences</code>, <code>max_tokens</code>, <code>lang</code>, <code>overlap_percent</code>, <code>offset</code>, and <code>token_counter</code>), but introduces additional parameters to manage batch processing.</p> <p>Here's an example of how to use <code>batch_chunk</code>:</p> <pre><code>from chunklet.plain_text_chunker import PlainTextChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nEN_TEXT = \"This is the first document. It has multiple sentences for chunking. Here is the second document. It is a bit longer to test batch processing effectively. And this is the third document. Short and sweet, but still part of the batch. The fourth document. Another one to add to the collection for testing purposes.\"\nES_TEXT = \"Este es el primer documento. Contiene varias frases para la segmentaci\u00f3n de texto. El segundo ejemplo es m\u00e1s extenso. Queremos probar el procesamiento en diferentes idiomas.\"\nFR_TEXT = \"Ceci est le premier document. Il est essentiel pour l'\u00e9valuation multilingue. Le deuxi\u00e8me document est court mais important. La variation est la cl\u00e9.\"\n\n# Initialize PlainTextChunker\nchunker = PlainTextChunker(token_counter=word_counter)\n\nchunks = chunker.batch_chunk(\n    texts=[EN_TEXT, ES_TEXT, FR_TEXT],\n    max_sentences=5,\n    max_tokens=20,\n    n_jobs=2,                    # (1)!\n    on_errors=\"raise\",           # (2)!\n    show_progress=True,          # (3)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to show output <pre><code>  0%|                                              | 0/3 [00:00&lt;?, ?it/s]\n--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 97)}\nContent: This is the first document.\nIt has multiple sentences for chunking.\nHere is the second document.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (96, 202)}\nContent: It is a bit longer to test batch processing effectively.\nAnd this is the third document.\nShort and sweet,\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (186, 253)}\nContent: Short and sweet,\nbut still part of the batch.\nThe fourth document.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (252, 311)}\nContent: Another one to add to the collection for testing purposes.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 1, 'span': (0, 118)}\nContent: Este es el primer documento.\nContiene varias frases para la segmentaci\u00f3n de texto.\nEl segundo ejemplo es m\u00e1s extenso.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 2, 'span': (117, 173)}\nContent: Queremos probar el procesamiento en diferentes idiomas.\n\nChunking ...: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e            | 2/3 [00:00, 10.09it/s]\n--- Chunk 7 ---\nMetadata: {'chunk_num': 1, 'span': (0, 125)}\nContent: Ceci est le premier document.\nIl est essentiel pour l'\u00e9valuation multilingue.\nLe deuxi\u00e8me document est court mais important.\n\n--- Chunk 8 ---\nMetadata: {'chunk_num': 2, 'span': (125, 149)}\nContent: La variation est la cl\u00e9.\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00, 19.88it/s]\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>batch_chunk</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p> <p>Adding Base Metadata to Batches</p> <p>Just like with the <code>chunk</code> method, you can pass a <code>base_metadata</code> dictionary to <code>batch_chunk</code>. This is useful for adding common information, like a source filename, to all chunks processed in the batch. For more details on metadata structure and available fields, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/plain_text_chunker/#separator","title":"Separator","text":"<p>The <code>separator</code> parameter allows you to specify a custom value to be yielded after all chunks for a given text have been processed. This is particularly useful when processing multiple texts in a batch, as it helps to clearly distinguish between the chunks originating from different input texts in the output stream.</p> <p>note</p> <p><code>None</code> cannot be used as a separator.</p> <pre><code>from more_itertools import split_at\nfrom chunklet.plain_text_chunker import PlainTextChunker\n\nchunker = PlainTextChunker()\ntexts = [\n    \"This is the first document. It has two sentences.\",\n    \"This is the second document. It also has two sentences.\"\n]\ncustom_separator = \"---END_OF_DOCUMENT---\"\n\nchunks_with_separators = chunker.batch_chunk(\n    texts,\n    max_sentences=1,\n    separator=custom_separator,\n    show_progress=False,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, doc_chunks in enumerate(chunk_groups):\n    if doc_chunks: # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in doc_chunks:\n            print(f\"Content: {chunk.content}\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>--- Chunks for Document 1 ---\nContent: This is the first document.\nMetadata: {'chunk_num': 1, 'span': (0, 27)}\nContent: It has two sentences.\nMetadata: {'chunk_num': 2, 'span': (28, 49)}\n\n--- Chunks for Document 2 ---\nContent: This is the second document.\nMetadata: {'chunk_num': 1, 'span': (0, 28)}\nContent: It also has two sentences.\nMetadata: {'chunk_num': 2, 'span': (29, 55)}\n</code></pre> API Reference <p>For a deep dive into the <code>PlainTextChunker</code> class, its methods, and all the nitty-gritty details, check out the full API documentation.````</p>"},{"location":"getting-started/programmatic/sentence_splitter/","title":"Sentence Splitter","text":""},{"location":"getting-started/programmatic/sentence_splitter/#the-art-of-precise-sentence-splitting","title":"The Art of Precise Sentence Splitting","text":"<p>Let's be honest, simply splitting text by periods can be a bit like trying to perform delicate surgery with a butter knife \u2013 it often leads to more problems than solutions! This approach can result in sentences being cut mid-thought, abbreviations being misinterpreted, and a general lack of clarity that can leave your NLP models scratching their heads.</p> <p>This common challenge in NLP, known as Sentence Boundary Disambiguation, is precisely what the <code>SentenceSplitter</code> is designed to address.</p> <p>Imagine the <code>SentenceSplitter</code> as a skilled linguistic surgeon. It applies its understanding of grammar and context to make precise cuts, cleanly separating sentences while preserving their original meaning. It's intelligent, multilingual, and an excellent tool for transforming your raw text into a well-structured list of sentences, perfectly prepared for your next NLP endeavor.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#whats-under-the-hood","title":"What's Under the Hood?","text":"<p>The <code>SentenceSplitter</code> is more than just a basic rule-based tool; it's a sophisticated system packed with powerful features:</p> <ul> <li>Multilingual Maestro: It fluently handles over 50 languages, intelligently detecting the language of your text and applying the most appropriate splitting methods. For a complete overview, check out our supported languages list.</li> <li>Customizable Genius: Encounter a unique text format? No problem! You can easily integrate your own custom splitters to manage specific requirements.</li> <li>Fallback Guardian: For languages not explicitly covered, a reliable fallback mechanism ensures that sentence splitting is still performed effectively.</li> <li>Error Detective: It actively monitors for potential issues, providing clear feedback if any problems arise with your custom splitters.</li> <li>Punctuation Tamer: It meticulously refines the output, removing empty sentences and correctly reattaching any misplaced punctuation.</li> </ul>"},{"location":"getting-started/programmatic/sentence_splitter/#example-usage","title":"Example Usage","text":"<p>Here's a quick example of how you can use the <code>SentenceSplitter</code> to split a block of text into sentences:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nTEXT = \"\"\"\nShe loves cooking. He studies AI. \"You are a Dr.\", she said. The weather is great. We play chess. Books are fun, aren't they?\n\nThe Playlist contains:\n  - two videos\n  - one image\n  - one music\n\nRobots are learning. It's raining. Let's code. Mars is red. Sr. sleep is rare. Consider item 1. This is a test. The year is 2025. This is a good year since N.A.S.A. reached 123.4 light year more.\n\"\"\"\n\nsplitter = SentenceSplitter(verbose=True)\nsentences = splitter.split(TEXT, lang=\"en\")\n\nfor sentence in sentences:\n    print(sentence)\n</code></pre> Click to show output <pre><code>2025-11-02 16:27:29.277 | WARNING  | chunklet.sentence_splitter.sentence_splitter:split:136 - The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\n2025-11-02 16:27:29.316 | INFO     | chunklet.sentence_splitter.sentence_splitter:detected_top_language:109 - Language detection: 'en' with confidence 10/10.\n2025-11-02 16:27:29.447 | INFO     | chunklet.sentence_splitter.sentence_splitter:split:167 - Text splitted into sentences. Total sentences detected: 19\nShe loves cooking.\nHe studies AI.\n\"You are a Dr.\", she said.\nThe weather is great.\nWe play chess.\nBooks are fun, aren't they?\nThe Playlist contains:\n- two videos\n- one image\n- one music\nRobots are learning.\nIt's raining.\nLet's code.\nMars is red.\nSr. sleep is rare.\nConsider item 1.\nThis is a test.\nThe year is 2025.\nThis is a good year since N.A.S.A. reached 123.4 light year more.\n</code></pre> <p>Note</p> <p>While the example above uses English (<code>en</code>), the <code>SentenceSplitter</code> is a true polyglot! It supports over 50 languages out of the box. You can see the full list of supported languages.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#detecting-top-languages","title":"Detecting Top Languages","text":"<p>Here's how you can detect the top language of a given text using the <code>SentenceSplitter</code>:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nlang_texts = {\n    \"en\": \"This is a sentence. This is another sentence. Mr. Smith went to Washington. He said 'Hello World!'. The quick brown fox jumps over the lazy dog.\",\n    \"fr\": \"Ceci est une phrase. Voici une autre phrase. M. Smith est all\u00e9 \u00e0 Washington. Il a dit 'Bonjour le monde!'. Le renard brun et rapide saute par-dessus le chien paresseux.\",\n    \"es\": \"Esta es una oraci\u00f3n. Aqu\u00ed hay otra oraci\u00f3n. El Sr. Smith fue a Washington. Dijo '\u00a1Hola Mundo!'. El r\u00e1pido zorro marr\u00f3n salta sobre el perro perezoso.\",\n    \"de\": \"Dies ist ein Satz. Hier ist ein weiterer Satz. Herr Smith ging nach Washington. Er sagte 'Hallo Welt!'. Der schnelle braune Fuchs springt \u00fcber den faulen Hund.\",\n    \"hi\": \"\u092f\u0939 \u090f\u0915 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u092f\u0939 \u090f\u0915 \u0914\u0930 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u0936\u094d\u0930\u0940 \u0938\u094d\u092e\u093f\u0925 \u0935\u093e\u0936\u093f\u0902\u0917\u091f\u0928 \u0917\u090f\u0964 \u0909\u0938\u0928\u0947 \u0915\u0939\u093e '\u0928\u092e\u0938\u094d\u0924\u0947 \u0926\u0941\u0928\u093f\u092f\u093e!'\u0964 \u0924\u0947\u091c \u092d\u0942\u0930\u093e \u0932\u094b\u092e\u0921\u093c\u0940 \u0906\u0932\u0938\u0940 \u0915\u0941\u0924\u094d\u0924\u0947 \u092a\u0930 \u0915\u0942\u0926\u0924\u093e \u0939\u0948\u0964\"\n}\n\nsplitter = SentenceSplitter()\n\nfor lang, text in lang_texts.items():\n    detected_lang, confidence = splitter.detected_top_language(text)\n    print(f\"Original language: {lang}\")\n    print(f\"Detected language: {detected_lang} with confidence {confidence:.2f}\")\n    print(\"-\" * 20)\n</code></pre> Click to show output <pre><code>Original language: en\nDetected language: en with confidence 1.00\n--------------------\nOriginal language: fr\nDetected language: fr with confidence 1.00\n--------------------\nOriginal language: es\nDetected language: es with confidence 1.00\n--------------------\nOriginal language: de\nDetected language: de with confidence 1.00\n--------------------\nOriginal language: hi\nDetected language: hi with confidence 1.00\n--------------------\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#custom-sentence-splitter","title":"Custom Sentence Splitter","text":"<p>You can provide your own custom sentence splitting functions to Chunklet. This is useful if you have a specialized splitter for a particular language or domain that you want to prioritize over Chunklet's built-in splitters.</p> <p>Global Registry: Be Mindful of Side Effects</p> <p>Custom splitters are registered globally. This means that once you register a splitter, it's available everywhere in your application. Be mindful of potential side effects if you're registering splitters in different parts of your codebase, especially in multi-threaded or long-running applications.</p> <p>To use a custom splitter, you leverage the <code>@registry.register</code> decorator. This decorator allows you to register your function for one or more languages directly. Your custom splitter function must accept a single <code>text</code> parameter (str) and return a <code>list[str]</code> of sentences.</p> <p>Important constraints</p> <ul> <li>Your function must accept exactly one required parameter (the text)</li> <li>Optional parameters with default values are allowed</li> <li>The function must return a list of strings</li> <li>Empty strings in the returned list will be filtered out automatically</li> <li>Lambda functions are not supported unless you provide a <code>name</code> parameter</li> <li>If an error occurs during the sentence splitting process (e.g., an issue with the custom splitter function), a <code>CallbackError</code> will be raised</li> </ul> <pre><code>import re\nfrom chunklet.sentence_splitter import SentenceSplitter, CustomSplitterRegistry\n\n\nsplitter = SentenceSplitter(verbose=False)\nregistry = CustomSplitterRegistry()\n\n@registry.register(\"en\", name=\"MyCustomEnglishSplitter\")\ndef my_custom_splitter(text: str) -&gt; list[str]:\n    \"\"\"A simple custom sentence splitter\"\"\"\n    return [s.strip() for s in re.split(r'(?&lt;=\\\\.)\\s+', text) if s.strip()]\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences = splitter.split(text=text, lang=\"en\")\n\nprint(\"---\" + \" Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n\n# Example with a custom splitter for multiple languages\n@registry.register(\"fr\", \"es\", name=\"MultiLangExclamationSplitter\")\ndef multi_lang_splitter(text: str) -&gt; list[str]:\n    return [s.strip() for s in re.split(r'(?&lt;=!)\\s+', text) if s.strip()]\n\nsplitter_multi = SentenceSplitter(verbose=False)\n\ntext_fr = \"Bonjour! Comment \u00e7a va? C'est super. Au revoir!\"\nsentences_fr = splitter_multi.split(text=text_fr, lang=\"fr\")\nprint(\"\\n--- Sentences using Multi-language Custom Splitter (French) ---\")\nfor i, sentence in enumerate(sentences_fr):\n    print(f\"Sentence {i+1}: {sentence}\")\n\ntext_es = \"Hola. Qu\u00e9 tal? Muy bien! Adi\u00f3s.\"\nsentences_es = splitter_multi.split(text=text_es, lang=\"es\")\nprint(\"\\n--- Sentences using Multi-language Custom Splitter (Spanish) ---\")\nfor i, sentence in enumerate(sentences_es):\n    print(f\"Sentence {i+1}: {sentence}\")\n\n\n# Unregistering Custom Splitters\nregistry.unregister(\"en\")            # (1)!\n</code></pre> <ol> <li>This will remove the custom splitter associated with the \"en\" language code. Note that you can unregister multiple languages if you had registered them with the same function: registry.unregister(\"fr\", \"es\")</li> </ol> Click to show output <pre><code>--- Sentences using Custom Splitter ---\nSentence 1: This is the first sentence.\nSentence 2: This is the second sentence.\nSentence 3: And the third.\n\n--- Sentences using Multi-language Custom Splitter (French) ---\nSentence 1: Bonjour!\nSentence 2: Comment \u00e7a va? C'est super. Au revoir!\n\n--- Sentences using Multi-language Custom Splitter (Spanish) ---\nSentence 1: Hola. Qu\u00e9 tal? Muy bien!\nSentence 2: Adi\u00f3s.\n</code></pre> <p>Registering Without the Decorator</p> <p>If you prefer not to use decorators, you can directly use the <code>registry.register()</code> method. This is particularly useful when registering splitters dynamically or when the callback function is not defined in the global scope.</p> <p>```py     from chunklet.sentence_splitter import CustomSplitterRegistry</p> <pre><code>registry = CustomSplitterRegistry()\n\ndef my_other_splitter(text: str) -&gt; list[str]:\n    return text.split(' ')\n\nregistry.register(my_other_splitter, \"jp\", name=\"MyOtherSplitter\")\n```\n</code></pre> <p>Building from Scratch: Leveraging BaseSplitter</p> <p>If you're looking to implement a sentence splitter entirely from scratch or integrate a highly custom logic, consider inheriting directly from the <code>BaseSplitter</code> abstract class. This provides a clear interface (<code>def split(self, text: str, lang: str) -&gt; list[str]</code>) that your custom class must implement, ensuring compatibility with Chunklet's architecture. You can then pass an instance of your custom splitter to the <code>PlainTextChunker</code> (documentation) or <code>DocumentChunker</code> (documentation) during their initialization.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#customsplitterregistry-methods-summary","title":"<code>CustomSplitterRegistry</code> Methods Summary","text":"<ul> <li><code>splitters</code>: Returns a shallow copy of the dictionary of registered splitters.</li> <li><code>is_registered(lang: str)</code>: Checks if a splitter is registered for the given language, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], list[str]] | None = None, *langs: str, name: str | None = None)</code>: Registers a splitter callback for one or more languages.</li> <li><code>unregister(*langs: str)</code>: Removes splitter(s) from the registry.</li> <li><code>clear()</code>: Clears all registered splitters from the registry.</li> <li><code>split(text: str, lang: str)</code>: Processes a text using a splitter registered for the given language, returning a list of sentences and the name of the splitter used.</li> </ul> API Reference <p>For a deep dive into the <code>SentenceSplitter</code> class, its methods, and all the nitty-gritty details, check out the full API documentation.</p>"},{"location":"reference/chunklet/","title":"chunklet","text":""},{"location":"reference/chunklet/#chunklet","title":"chunklet","text":"<p>Chunklet: The v2.0.0 Evolution - Multi-strategy, Context-aware, Multilingual Text &amp; Code Chunker</p> <p>This package provides a robust and flexible solution for splitting large texts and code into smaller, manageable chunks. Designed for applications like Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) pipelines, and other context-aware Natural Language Processing (NLP) tasks.</p> <p>Version 2.0.0 introduces a revamped architecture with: - Dedicated chunkers: <code>PlainTextChunker</code> (formerly <code>Chunklet</code>), <code>DocumentChunker</code>, and <code>CodeChunker</code>. - Expanded language support (50+ languages) and improved error handling. - Flexible batch processing with <code>on_errors</code> parameter and memory-optimized generators. - Enhanced modularity, extensibility, and performance.</p> <p>Modules:</p> <ul> <li> <code>cli</code>           \u2013            </li> <li> <code>code_chunker</code>           \u2013            </li> <li> <code>common</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>exceptions</code>           \u2013            </li> <li> <code>plain_text_chunker</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/#chunklet.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/#chunklet.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/#chunklet.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/#chunklet.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/#chunklet.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the `chunk` method, like `chunker.chunk(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/#chunklet.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/#chunklet.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p>"},{"location":"reference/chunklet/cli/","title":"cli","text":""},{"location":"reference/chunklet/cli/#chunklet.cli","title":"chunklet.cli","text":"<p>Functions:</p> <ul> <li> <code>chunk_command</code>             \u2013              <p>Chunk text or files based on specified parameters.</p> </li> <li> <code>split_command</code>             \u2013              <p>Split text or a single file into sentences using the SentenceSplitter.</p> </li> </ul>"},{"location":"reference/chunklet/cli/#chunklet.cli.chunk_command","title":"chunk_command","text":"<pre><code>chunk_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to chunk. If not provided, --source must be used.\",\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    code: bool = typer.Option(\n        False,\n        \"--code\",\n        help=\"Use CodeChunker for code files.\",\n    ),\n    doc: bool = typer.Option(\n        False,\n        \"--doc\",\n        help=\"Use DocumentChunker for document files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto)\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. (default: 20)\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. (default: 0)\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=\"A shell command to use for token counting. The command should take text as stdin and output the token count as a number.\",\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        help=\"Include metadata in the output. If --destination is a directory, metadata is saved as separate .json files; otherwise, it's included inline in the output.\",\n    ),\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking. (default: None, uses all available cores)\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n)\n</code></pre> <p>Chunk text or files based on specified parameters.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"chunk\", help=\"Chunk text or files based on specified parameters.\")\ndef chunk_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to chunk. If not provided, --source must be used.\"\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    # flags for chunker type\n    code: bool = typer.Option(False, \"--code\", help=\"Use CodeChunker for code files.\"),\n    doc: bool = typer.Option(\n        False, \"--doc\", help=\"Use DocumentChunker for document files.\"\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto'). (default: auto)\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. Applies to PlainTextChunker and DocumentChunker. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85). Applies to PlainTextChunker and DocumentChunker. (default: 20)\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking. Applies to PlainTextChunker and DocumentChunker. (default: 0)\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=(\n            \"A shell command to use for token counting. \"\n            \"The command should take text as stdin and output the token count as a number.\"\n        ),\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        help=(\n            \"Include metadata in the output. If --destination is a directory, \"\n            \"metadata is saved as separate .json files; otherwise, it's \"\n            \"included inline in the output.\"\n        ),\n    ),\n    # for Batching\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking. (default: None, uses all available cores)\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    # CodeChunker specific arguments\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n):\n    \"\"\"\n    Chunk text or files based on specified parameters.\n    \"\"\"\n    # --- Input validation logic ---\n    provided_inputs = [arg for arg in [text, source] if arg]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please provide a text, or use the --source option.\",\n            err=True,\n        )\n        typer.echo(\n            \"\ud83d\udca1 Hint: Use 'chunklet --help' for more information and usage examples.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Please provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        typer.echo(\n            \"\ud83d\udca1 Hint: Use 'chunklet --help' for more information and usage examples.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if doc and code:\n        typer.echo(\n            \"Error: Please specify either '--doc' or '--code', but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # --- Tokenizer setup ---\n    token_counter = None\n    if tokenizer_command:\n        token_counter = create_external_tokenizer(tokenizer_command)\n\n    all_results = []\n\n    # Construct chunk_kwargs dynamically\n    chunk_kwargs = {\n        \"max_tokens\": max_tokens,\n        \"token_counter\": token_counter,\n    }\n\n    if code:\n        chunker_instance = CodeChunker(\n            verbose=verbose,\n            token_counter=token_counter,\n        )\n        chunk_kwargs.update(\n            {\n                \"max_lines\": max_lines,\n                \"max_functions\": max_functions,\n                \"docstring_mode\": docstring_mode,\n                \"strict\": strict,\n                \"include_comments\": include_comments,\n            }\n        )\n    else:\n        if text:\n            chunker_instance = PlainTextChunker(\n                verbose=verbose,\n                token_counter=token_counter,\n            )\n        else:\n            chunker_instance = DocumentChunker(\n                verbose=verbose,\n                token_counter=token_counter,\n            )\n        chunk_kwargs.update(\n            {\n                \"lang\": lang,\n                \"max_sentences\": max_sentences,\n                \"max_section_breaks\": max_section_breaks,\n                \"overlap_percent\": overlap_percent,\n                \"offset\": offset,\n            }\n        )\n\n    # --- Chunking logic ---\n    if text:\n        chunks = chunker_instance.chunk(\n            text=text,\n            **chunk_kwargs,\n        )\n        all_results.append(chunks)\n\n    elif source:\n        file_paths = []\n\n        for path in source:\n            path = path.resolve()\n\n            if is_path_like(str(path)):\n                if path.is_file():\n                    file_paths.append(path)\n                elif path.is_dir():\n                    file_paths.extend([p for p in path.glob(\"**/*\") if p.is_file()])\n                else:\n                    # This single 'else' catches paths that pass the heuristic but\n                    # either don't exist OR exist but are special file types\n                    # (e.g., pipes, sockets, broken symlinks, etc.)\n                    typer.echo(\n                        f\"Warning: '{path}' is path-like but was not found \"\n                        \"or is not a processable file/directory. Skipping.\",\n                        err=True,\n                    )\n            else:\n                # Fails the path-like regex heuristic check\n                typer.echo(\n                    f\"Warning: '{path}' does not resemble a valid file system path \"\n                    \"(failed heuristic check). Skipping.\",\n                    err=True,\n                )\n\n        if not file_paths:\n            typer.echo(\n                \"Warning: No processable files found in the specified source(s). Exiting.\",\n                err=True,\n            )\n            raise typer.Exit(code=0)\n\n        if len(file_paths) == 1 and file_paths[0].suffix not in {\n            \".pdf\",\n            \".epub\",\n            \".docx\",\n        }:\n            single_file = file_paths[0]\n            chunks = chunker_instance.chunk(\n                path=single_file,\n                **chunk_kwargs,\n            )\n            all_results.append(chunks)\n        else:\n            # Batch input logic\n            all_results_gen = chunker_instance.batch_chunk(\n                paths=file_paths,\n                n_jobs=n_jobs,\n                show_progress=True,\n                on_errors=on_errors,\n                **chunk_kwargs,\n            )\n            all_results.append(all_results_gen)\n\n    if not all_results:\n        typer.echo(\n            \"Warning: No chunks were generated. \"\n            \"This might be because the input was empty or did not contain any processable content.\",\n            err=True,\n        )\n        raise typer.Exit(code=0)\n\n    # --- Output handling ---\n\n    # Check for conflict: multi-input requires directory destination\n    if destination and destination.is_file():\n        typer.echo(\n            \"Error: When processing multiple inputs, '--destination' must be a directory, not a file.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if destination and len(destination):\n        # This is the equivalent of the old `if output_dir:` block\n        destination.mkdir(parents=True, exist_ok=True)\n        total_chunks_written = 0\n        processed_sources = set()\n\n        for res in all_results:\n            for chunk_box in res:\n                source_name = chunk_box.metadata[\"source\"]\n                base_name = Path(source_name).stem\n\n                base_output_filename = (\n                    f\"{base_name}_chunk_{chunk_box.metadata['chunk_num']}\"\n                )\n\n                # Write content file\n                output_txt_path = destination / f\"{base_output_filename}.txt\"\n                with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(chunk_box.content + \"\\n\")\n\n                total_chunks_written += 1\n\n                # Write metadata file if requested\n                if metadata:\n                    output_json_path = destination / f\"{base_output_filename}.json\"\n                    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n                        # Ensures metadata is a standard dict before dumping\n                        data_to_dump = (\n                            chunk_box.metadata.to_dict()\n                            if hasattr(chunk_box.metadata, \"to_dict\")\n                            else dict(chunk_box.metadata)\n                        )\n                        json.dump(data_to_dump, f, indent=4)\n\n                processed_sources.add(source_name)\n\n        message = (\n            f\"Successfully processed {len(processed_sources)} input(s)\"\n            f\"and wrote {total_chunks_written} chunk file(s) to {destination}\"\n        )\n        if metadata:\n            message += \" (with .json metadata files).\"\n        else:\n            message += \".\"\n        typer.echo(message)\n\n    else:\n        # This is the equivalent of the old `else:` block (stdout or single output_file)\n        output_content = []\n\n        chunk_counter = 0\n        for res in all_results:\n            for chunk_box in res:\n                chunk_counter += 1\n                output_content.append(f\"## --- Chunk {chunk_counter} ---\")\n                output_content.append(chunk_box.content)\n                output_content.append(\"\")\n                if metadata:\n                    chunk_metadata = chunk_box.metadata.to_dict()\n                    output_content.append(\"\\n--- Metadata ---\")  # Use a sub-header\n\n                    for key, value in chunk_metadata.items():\n                        # Use clean pipe formatting for terminal style tables\n                        output_content.append(f\"| {key}: {value}\")\n\n                    output_content.append(\"\\n\")\n\n        output_str = \"\\n\".join(output_content)\n\n        if destination:\n            destination.write_text(output_str, encoding=\"utf-8\")\n        else:\n            typer.echo(output_str)\n</code></pre>"},{"location":"reference/chunklet/cli/#chunklet.cli.split_command","title":"split_command","text":"<pre><code>split_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to split. If not provided, --source must be used.\",\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n)\n</code></pre> <p>Split text or a single file into sentences using the SentenceSplitter.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"split\", help=\"Splits text or a single file into sentences.\")\ndef split_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to split. If not provided, --source must be used.\"\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n):\n    \"\"\"\n    Split text or a single file into sentences using the SentenceSplitter.\n    \"\"\"\n    # Validation and Input Acquisition\n    provided_inputs = [arg for arg in [text, source] if arg is not None]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please use a text argument or the --source option.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if source:\n        # --- Source Constraints ---\n        if source.is_dir():\n            typer.echo(\n                f\"Error: Source path '{source}' cannot be a directory for the 'split' command.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n        if not source.is_file():\n            typer.echo(\n                f\"Error: Source path '{source}' not found or is not a file.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        try:\n            input_text = source.read_text(encoding=\"utf-8\")\n        except Exception as e:\n            typer.echo(f\"Error reading source file: {e}\", err=True)\n            raise typer.Exit(code=1)\n    else:\n        input_text = text\n\n    # --- Destination Constraint ---\n    if destination and destination.is_dir():\n        typer.echo(\n            f\"Error: Destination path '{destination}' cannot be a directory for the 'split' command.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Split Logic\n    splitter = SentenceSplitter(verbose=verbose)\n    sentences, confidence = splitter.split(input_text, lang=lang)\n\n    # Output Handling\n    if destination:\n        output_str = \"\\n\".join(sentences)\n        source_display = f\"from {source.name}\" if source else \"(from stdin)\"\n\n        try:\n            destination.write_text(output_str, encoding=\"utf-8\")\n            typer.echo(\n                f\"Successfully split and wrote {len(sentences)} sentences \"\n                f\"{source_display} to {destination} (Confidence: {confidence})\",\n                err=True,\n            )\n        except Exception as e:\n            typer.echo(f\"Error writing to destination file: {e}\", err=True)\n            raise typer.Exit(code=1)\n    else:\n        source_display = f\"Source: {source.name}\" if source else \"Source: stdin\"\n\n        typer.echo(\n            f\"--- Sentences ({len(sentences)}): \"\n            f\" [{source_display} | Lang: {lang.upper()} | Confidence: {confidence}] ---\"\n        )\n\n        for sentence in sentences:\n            typer.echo(sentence)\n</code></pre>"},{"location":"reference/chunklet/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/#chunklet.code_chunker","title":"chunklet.code_chunker","text":"<p>Modules:</p> <ul> <li> <code>code_chunker</code>           \u2013            <p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> </li> <li> <code>helpers</code>           \u2013            </li> <li> <code>patterns</code>           \u2013            <p>regex_patterns.py</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker","title":"chunklet.code_chunker.code_chunker","text":"<p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> <p>Language-Agnostic Code Chunking Utility</p> <p>This module provides a robust, convention-aware engine for segmenting source code into semantic units (\"chunks\") such as functions, classes, namespaces, and logical blocks. Unlike purely heuristic or grammar-dependent parsers, the <code>CodeChunker</code> relies on anchored, multi-language regex patterns and indentation rules to identify structures consistently across a variety of programming languages.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker--limitations","title":"Limitations","text":"<p><code>CodeChunker</code> assumes syntactically conventional code. Highly obfuscated, minified, or macro-generated sources may not fully respect its boundary patterns, though such cases fall outside its intended domain.</p> Inspired by <ul> <li>Camel.utils.chunker.CodeChunker (@ CAMEL-AI.org)</li> <li>code-chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> <p>Classes:</p> <ul> <li> <code>CodeChunker</code>           \u2013            <p>Language-agnostic code chunking utility for semantic code segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker","title":"CodeChunker","text":"<pre><code>CodeChunker(\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>Language-agnostic code chunking utility for semantic code segmentation.</p> <p>Extracts structural units (functions, classes, namespaces) from source code across multiple programming languages using pattern-based detection and token-aware segmentation.</p> Key Features <ul> <li>Cross-language support (Python, C/C++, Java, C#, JavaScript, Go, etc.)</li> <li>Structural analysis with namespace hierarchy tracking</li> <li>Configurable token limits with strict/lenient overflow handling</li> <li>Flexible docstring and comment processing modes</li> <li>Accurate line number preservation and source tracking</li> <li>Parallel batch processing for multiple files</li> <li>Comprehensive logging and progress tracking</li> </ul> <p>Initialize the CodeChunker with optional token counter and verbosity control.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Process multiple source files or code strings in parallel.</p> </li> <li> <code>chunk</code>             \u2013              <p>Extract semantic code chunks from source using structural analysis.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize the CodeChunker with optional token counter and verbosity control.\n\n    Args:\n        verbose (bool): Enable verbose logging.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n    \"\"\"\n    self.token_counter = token_counter\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    sources: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = False,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple source files or code strings in parallel.</p> <p>Leverages multiprocessing to efficiently chunk multiple code sources, applying consistent chunking rules across all inputs.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata. Includes: - content (str): Code content - tree (str): Namespace hierarchy - start_line (int): Starting line in original source - end_line (int): Ending line in original source - span (tuple[int, int]): Character-level span (start and end offsets) in the original source. - source_path (str): Source file path or \"N/A\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid input parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    sources: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = False,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple source files or code strings in parallel.\n\n    Leverages multiprocessing to efficiently chunk multiple code sources,\n    applying consistent chunking rules across all inputs.\n\n    Args:\n        sources (restricted_iterable[str | Path]): A restricted iterable of file paths or raw code strings to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable | None): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        include_comments (bool): Include comments in output chunks. Default: False.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n        n_jobs (int | None): Number of parallel workers. Uses all available CPUs if None.\n        show_progress (bool): Display progress bar during processing. Defaults to True.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to 'raise'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n            Includes:\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path or \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid input parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=sources,\n        iterable_name=\"sources\",\n        separator=separator,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(sources)","title":"<code>sources</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of file paths or raw code strings to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Include comments in output chunks. Default: False.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy: - \"summary\": Include only first line of docstrings - \"all\": Include complete docstrings - \"excluded\": Remove all docstrings Defaults to \"all\"</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers. Uses all available CPUs if None.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Display progress bar during processing. Defaults to True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = False,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True\n) -&gt; list[Box]\n</code></pre> <p>Extract semantic code chunks from source using structural analysis.</p> <p>Processes source code by identifying structural boundaries (functions, classes, namespaces) and grouping content into token-limited chunks while preserving logical code units.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of code chunks with metadata. Each Box contains: - content (str): Code content - tree (str): Namespace hierarchy - start_line (int): Starting line in original source - end_line (int): Ending line in original source - span (tuple[int, int]): Character-level span (start and end offsets) in the original source. - source_path (str): Source file path or \"N/A\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid configuration parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = False,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n) -&gt; list[Box]:\n    \"\"\"\n    Extract semantic code chunks from source using structural analysis.\n\n    Processes source code by identifying structural boundaries (functions, classes,\n    namespaces) and grouping content into token-limited chunks while preserving\n    logical code units.\n\n    Args:\n        source (str | Path): Raw code string or file path to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable, optional): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        include_comments (bool): Include comments in output chunks. Default: False.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n\n    Returns:\n        list[Box]: List of code chunks with metadata. Each Box contains:\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path or \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid configuration parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    max_tokens, max_lines, max_functions = self._validate_constraints(\n        max_tokens, max_lines, max_functions, token_counter\n    )\n    token_counter = token_counter or self.token_counter\n\n    if self.verbose:\n        logger.info(\n            \"Starting chunk processing for {}\",\n            (\n                f\"source: {str(Path)}\"\n                if (isinstance(str, Path) or is_path_like(source))\n                else f\"code starting with:\\n```\\n{source[:100]}...\\n```\\n\"\n            ),\n        )\n\n    if not source.strip():\n        if self.verbose:\n            logger.info(\"Input source is empty. Returning empty list.\")\n        return []\n\n    snippet_dicts, cumulative_lengths = self._extract_code_structures(\n        source, include_comments, docstring_mode\n    )\n\n    if self.verbose:\n        logger.info(\n            \"Extracted {} structural blocks from source\", len(snippet_dicts)\n        )\n\n    # Grouping logic\n\n    merged_content = []\n    relations_list = []\n    start_line = None\n    end_line = None\n    token_count = 0\n    line_count = 0\n    function_count = 0\n    result_chunks = []\n\n    index = 0\n    while index &lt; len(snippet_dicts):\n        snippet_dict = snippet_dicts[index]\n        box_tokens = (\n            count_tokens(snippet_dict[\"content\"], token_counter)\n            if max_tokens != sys.maxsize\n            else 0\n        )\n        box_lines = snippet_dict[\"content\"].count(\"\\n\") + (\n            1 if snippet_dict[\"content\"] else 0\n        )\n        is_function = bool(snippet_dict.get(\"func_partial_signature\"))\n\n        # Check if adding this snippet exceeds any limits\n        token_limit_reached = token_count + box_tokens &gt; max_tokens\n        line_limit_reached = line_count + box_lines &gt; max_lines\n        function_limit_reached = is_function and (\n            function_count + 1 &gt; max_functions\n        )\n\n        if not (\n            token_limit_reached or line_limit_reached or function_limit_reached\n        ):\n            # Fits: merge normally\n            merged_content.append(snippet_dict[\"content\"])\n            relations_list.append(snippet_dict[\"relations\"])\n            token_count += box_tokens\n            line_count += box_lines\n            if is_function:\n                function_count += 1\n\n            if start_line is None:\n                start_line = snippet_dict[\"start_line\"]\n            end_line = snippet_dict[\"end_line\"]\n            index += 1\n\n        elif not merged_content:\n            # Too big and nothing merged yet: handle oversize\n            if strict:\n                raise TokenLimitError(\n                    f\"Structural block exceeds maximum limit (tokens: {box_tokens} &gt; {max_tokens}, \"\n                    f\"lines: {box_lines} &gt; {max_lines}, or functions: {int(is_function)} &gt; {max_functions}).\\n\"\n                    f\"Content starting with: \\n```\\n{snippet_dict['content'][:100]}...\\n```\\n\"\n                    \"Reason: Prevent splitting inside interest points (function, class, region, ...)\\n\"\n                    \"\ud83d\udca1Hint: Consider increasing 'max_tokens', 'max_lines', or 'max_functions', \"\n                    \"refactoring the oversized block, or setting 'strict=False' to allow automatic splitting of oversized blocks.\"\n                )\n            else:  # Else split further\n                if self.verbose:\n                    logger.warning(\n                        \"Splitting oversized block (tokens: {} lines: {}) into sub-chunks\",\n                        box_tokens,\n                        box_lines,\n                    )\n\n                sub_chunks = self._split_oversized(\n                    snippet_dict,\n                    max_tokens,\n                    max_lines,\n                    source,\n                    token_counter,\n                    cumulative_lengths,\n                )\n\n                for sub_chunk in sub_chunks:\n                    sub_chunk.metadata.chunk_num = len(result_chunks) + 1\n                    result_chunks.append(sub_chunk)\n                index += 1\n        else:\n            # Flush current merged content as a chunk\n            start_span = (\n                0 if start_line == 1 else cumulative_lengths[start_line - 2]\n            )\n            end_span = cumulative_lengths[end_line - 1]\n            merged_chunk = Box(\n                {\n                    \"content\": \"\\n\".join(merged_content),\n                    \"metadata\": {\n                        \"chunk_num\": len(result_chunks) + 1,\n                        \"tree\": self._merge_tree(relations_list),\n                        \"start_line\": start_line,\n                        \"end_line\": end_line,\n                        \"span\": (start_span, end_span),\n                        \"source\": (\n                            str(source)\n                            if (isinstance(source, Path) or is_path_like(source))\n                            else \"N/A\"\n                        ),\n                    },\n                }\n            )\n            result_chunks.append(merged_chunk)\n\n            # Reset for next chunk\n            merged_content.clear()\n            relations_list.clear()\n            start_line = None\n            end_line = None\n            token_count = 0\n            line_count = 0\n            function_count = 0\n\n    # Flush remaining content\n    if merged_content:\n        start_span = 0 if start_line == 1 else cumulative_lengths[start_line - 2]\n        end_span = cumulative_lengths[end_line - 1]\n        merged_chunk = Box(\n            {\n                \"content\": \"\\n\".join(merged_content),\n                \"metadata\": {\n                    \"chunk_num\": len(result_chunks) + 1,\n                    \"tree\": self._merge_tree(relations_list),\n                    \"start_line\": start_line,\n                    \"end_line\": end_line,\n                    \"span\": (start_span, end_span),\n                    \"source\": (\n                        str(source)\n                        if (isinstance(source, Path) or is_path_like(source))\n                        else \"N/A\"\n                    ),\n                },\n            }\n        )\n        result_chunks.append(merged_chunk)\n\n    if self.verbose:\n        logger.info(\n            \"Generated {} chunk(s) for the {}\",\n            len(result_chunks),\n            (\n                f\"source: {str(Path)}\"\n                if (isinstance(str, Path) or is_path_like(source))\n                else f\"code starting with:\\n```\\n{source[:100]}..\\n```\\n\"\n            ),\n        )\n\n    return result_chunks\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>Raw code string or file path to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Include comments in output chunks. Default: False.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy: - \"summary\": Include only first line of docstrings - \"all\": Include complete docstrings - \"excluded\": Remove all docstrings Defaults to \"all\"</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/helpers/","title":"helpers","text":""},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers","title":"chunklet.code_chunker.helpers","text":"<p>Functions:</p> <ul> <li> <code>is_binary_file</code>             \u2013              <p>Determine whether a file is binary or text.</p> </li> <li> <code>is_python_code</code>             \u2013              <p>Check if a source is written in Python.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_binary_file","title":"is_binary_file","text":"<pre><code>is_binary_file(file_path: str | Path) -&gt; bool\n</code></pre> <p>Determine whether a file is binary or text.</p> <p>First tries to guess the file type based on its MIME type derived from the file extension. If MIME type is unavailable or ambiguous, reads the first 1024 bytes of the file and checks for null bytes (<code>b'\u0000'</code>), which indicate binary content.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the file is likely binary, False if text.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/helpers.py</code> <pre><code>@validate_input\ndef is_binary_file(file_path: str | Path) -&gt; bool:\n    \"\"\"\n    Determine whether a file is binary or text.\n\n    First tries to guess the file type based on its MIME type derived from\n    the file extension. If MIME type is unavailable or ambiguous, reads the\n    first 1024 bytes of the file and checks for null bytes (`b'\\0'`), which\n    indicate binary content.\n\n    Args:\n        file_path (str | Path): Path to the file.\n\n    Returns:\n        bool: True if the file is likely binary, False if text.\n    \"\"\"\n    file_path = Path(file_path)\n    mime_type, _ = mimetypes.guess_type(file_path)\n    if mime_type:\n        return not mime_type.startswith(\"text\")\n\n    with open(file_path, \"rb\") as f:\n        chunk = f.read(1024)\n        return b\"\\0\" in chunk\n</code></pre>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_binary_file(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the file.</p>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_python_code","title":"is_python_code","text":"<pre><code>is_python_code(source: str | Path) -&gt; bool\n</code></pre> <p>Check if a source is written in Python.</p> <p>This function uses multiple indicators, prioritizing syntactic validity via the Abstract Syntax Tree (AST) parser for maximum confidence.</p> Indicators used <ul> <li>File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).</li> <li>Shebang line detection (e.g., \"#!/usr/bin/python\").</li> <li>Definitive syntax check using Python's <code>ast.parse()</code>.</li> <li>Fallback heuristic via Pygments lexer guessing.</li> </ul> Note <p>The function is definitive for complete, syntactically correct code blocks. It falls back to a Pygments heuristic only for short, incomplete, or ambiguous code snippets that fail AST parsing.</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the source is written in Python.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/helpers.py</code> <pre><code>@validate_input\ndef is_python_code(source: str | Path) -&gt; bool:\n    \"\"\"\n    Check if a source is written in Python.\n\n    This function uses multiple indicators, prioritizing syntactic validity\n    via the Abstract Syntax Tree (AST) parser for maximum confidence.\n\n    Indicators used:\n      - File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).\n      - Shebang line detection (e.g., \"#!/usr/bin/python\").\n      - Definitive syntax check using Python's `ast.parse()`.\n      - Fallback heuristic via Pygments lexer guessing.\n\n    Note:\n        The function is definitive for complete, syntactically correct code blocks.\n        It falls back to a Pygments heuristic only for short, incomplete, or\n        ambiguous code snippets that fail AST parsing.\n\n    Args:\n        source (str | Path): raw code string or Path to source file to check.\n\n    Returns:\n        bool: True if the source is written in Python.\n    \"\"\"\n    # Path-based check\n    if isinstance(source, Path) or (isinstance(source, str) and is_path_like(source)):\n        path = Path(source)\n        return path.suffix.lower() in {\".py\", \".pyi\", \".pyx\", \".pyw\"}\n\n    if isinstance(source, str):\n        # Shebang line check\n        if re.match(r\"#!/usr/bin/(env\\s+)?python\", source.strip()):\n            return True\n\n        # Definitive syntactic check (Highest confidence)\n        try:\n            ast.parse(source)\n            # If parsing succeeds, it's definitely Python code\n            return True\n        except Exception:\n            # If fails, it might still be Python code (e.g., incomplete snippet), so continue with heuristics\n            pass\n\n    # Pygments heuristic (Lowest confidence, last resort)\n    try:\n        lexer = guess_lexer(source)\n        return lexer.name.lower() == \"python\"\n    except ClassNotFound:\n        return False\n</code></pre>"},{"location":"reference/chunklet/code_chunker/helpers/#chunklet.code_chunker.helpers.is_python_code(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>raw code string or Path to source file to check.</p>"},{"location":"reference/chunklet/code_chunker/patterns/","title":"patterns","text":""},{"location":"reference/chunklet/code_chunker/patterns/#chunklet.code_chunker.patterns","title":"chunklet.code_chunker.patterns","text":"<p>regex_patterns.py</p> <p>Written by: Speedyk-005 Copyright 2025 License: MIT</p> <p>This module contains regular expressions for chunking and parsing source code across multiple programming languages. The patterns are designed to match:</p> <ul> <li>Single-line comments (Python, C/C++, Java, JavaScript, Lisp, etc.)</li> <li>Multi-line comments / docstrings (Python, C-style, Ruby, Lisp, etc.)</li> <li>Function or method definitions across various languages</li> <li>Namespaces, classes, modules, and interfaces</li> <li>Annotations / decorators (Python, C#, Java)</li> <li>Block-ending indicators ('}' or 'end')</li> </ul> <p>These regexes can be imported into a chunker or parser to identify logical sections of code for semantic analysis, tokenization, or processing.</p> Note <ul> <li>re.M = multiline (^,$ match each line)</li> <li>re.S = DOTALL (. matches newline)</li> </ul>"},{"location":"reference/chunklet/common/","title":"common","text":""},{"location":"reference/chunklet/common/#chunklet.common","title":"chunklet.common","text":"<p>Modules:</p> <ul> <li> <code>batch_runner</code>           \u2013            </li> <li> <code>path_utils</code>           \u2013            </li> <li> <code>token_utils</code>           \u2013            </li> <li> <code>validation</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/","title":"batch_runner","text":""},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner","title":"chunklet.common.batch_runner","text":"<p>Functions:</p> <ul> <li> <code>run_in_batch</code>             \u2013              <p>Processes a batch of items in parallel using multiprocessing.</p> </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch","title":"run_in_batch","text":"<pre><code>run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of items in parallel using multiprocessing. Splits the iterable into chunks and executes the function on each.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> Source code in <code>src/chunklet/common/batch_runner.py</code> <pre><code>def run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of items in parallel using multiprocessing.\n    Splits the iterable into chunks and executes the function on each.\n\n    Args:\n        func (Callable): The function to call for each argument.\n        iterable_of_args (Iterable): An iterable of inputs to process.\n        iterable_name: Name of the iterable. needed for logging and exception message.\n        n_jobs (int | None): Number of parallel workers to use.\n            If None, uses all available CPUs. Must be &gt;= 1 if specified.\n        show_progress (bool): Whether to display a progress bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to \"raise\".\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        verbose (bool): Whether to enable verbose logging.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n    \"\"\"\n    from mpire import WorkerPool\n\n    total, iterable_of_args = safely_count_iterable(iterable_name, iterable_of_args)\n\n    if verbose:\n        logger.info(\"Starting batch chunking for {} items.\", total)\n\n    if total == 0:\n        if verbose:\n            logger.info(\"Input {} is empty. Returning empty iterator.\", iterable_name)\n        return iter([])\n\n    # Wrapper to capture result/exception\n    def chunk_func(*args, **kwargs):\n        try:\n            res = func(*args, **kwargs)\n            return res, None\n        except Exception as e:\n            return None, e\n\n    failed_count = 0\n    try:\n        with WorkerPool(n_jobs=n_jobs) as pool:\n            imap_func = pool.imap if separator is not None else pool.imap_unordered\n\n            progress_bar_options = {\n                \"bar_format\": \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]\",\n                \"desc\": \"Chunking ...\",\n            }\n\n            task_iter = imap_func(\n                chunk_func,\n                iterable_of_args,\n                iterable_len=total,\n                progress_bar=show_progress,\n                progress_bar_options=progress_bar_options,\n            )\n\n            for res, error in task_iter:\n                if error:\n                    failed_count += 1\n                    if on_errors == \"raise\":\n                        raise error\n                    elif on_errors == \"break\":\n                        logger.error(\n                            \"A task for {} failed. Returning partial results.\\nReason: {}\",\n                            iterable_name,\n                            error,\n                        )\n                        break\n                    else:  # skip\n                        logger.warning(\"Skipping a failed task.\\nReason: {}\", error)\n                        continue\n\n                yield from res\n\n                if separator is not None:\n                    yield separator\n\n    finally:\n        if verbose:\n            logger.info(\n                \"Batch processing completed. {}/{} items processed successfully.\",\n                total - failed_count,\n                total,\n            )\n</code></pre>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(func)","title":"<code>func</code>","text":"(<code>Callable</code>)           \u2013            <p>The function to call for each argument.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_of_args)","title":"<code>iterable_of_args</code>","text":"(<code>Iterable</code>)           \u2013            <p>An iterable of inputs to process.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_name)","title":"<code>iterable_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the iterable. needed for logging and exception message.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display a progress bar.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to \"raise\".</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enable verbose logging.</p>"},{"location":"reference/chunklet/common/path_utils/","title":"path_utils","text":""},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils","title":"chunklet.common.path_utils","text":"<p>Functions:</p> <ul> <li> <code>is_path_like</code>             \u2013              <p>Check if a string looks like a filesystem path (file or folder),</p> </li> </ul>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like","title":"is_path_like","text":"<pre><code>is_path_like(text: str) -&gt; bool\n</code></pre> <p>Check if a string looks like a filesystem path (file or folder), including Unix/Windows paths, hidden files, and scripts without extensions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if string appears to be a filesystem path.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\nTrue\n&gt;&gt;&gt; is_path_like(\"C:\\Users\\User\\file.pdf\")\nTrue\n&gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\nTrue\n&gt;&gt;&gt; is_path_like(\".hidden_file\")\nTrue\n&gt;&gt;&gt; is_path_like(\"no_extension_script\")\nTrue\n&gt;&gt;&gt; is_path_like(\"path/with/newline\\nchar\")\nFalse\n&gt;&gt;&gt; is_path_like(\"string_with_null_byte\\x00\")\nFalse\n</code></pre> Source code in <code>src/chunklet/common/path_utils.py</code> <pre><code>@validate_input\ndef is_path_like(text: str) -&gt; bool:\n    \"\"\"\n    Check if a string looks like a filesystem path (file or folder),\n    including Unix/Windows paths, hidden files, and scripts without extensions.\n\n    Args:\n        text (str): text to check.\n\n    Returns:\n        bool: True if string appears to be a filesystem path.\n\n    Examples:\n        &gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\n        True\n        &gt;&gt;&gt; is_path_like(\"C:\\\\Users\\\\User\\\\file.pdf\")\n        True\n        &gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\n        True\n        &gt;&gt;&gt; is_path_like(\".hidden_file\")\n        True\n        &gt;&gt;&gt; is_path_like(\"no_extension_script\")\n        True\n        &gt;&gt;&gt; is_path_like(\"path/with/newline\\\\nchar\")\n        False\n        &gt;&gt;&gt; is_path_like(\"string_with_null_byte\\\\x00\")\n        False\n    \"\"\"\n    if not text or \"\\n\" in text or \"\\0\" in text:\n        return False\n    if sys.platform == \"win32\" and any(c in text for c in '&lt;&gt;:\"|?*'):\n        return False\n\n    try:\n        # Attempt to call is_file() to trigger OS-level path validation,\n        # especially for path length.\n        Path(text).is_file()\n    except OSError as e:\n        # If an OSError occurs, check if it's specifically due to the name being too long.\n        if e.errno == errno.ENAMETOOLONG:\n            return False\n        else:\n            # For other OSErrors (e.g., permission denied, invalid characters not caught by initial checks),\n            # we let the regex check proceed, as the focus is on structural validity, not existence or access.\n            pass\n\n    return bool(PATH_PATTERN.match(text))\n</code></pre>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>text to check.</p>"},{"location":"reference/chunklet/common/token_utils/","title":"token_utils","text":""},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils","title":"chunklet.common.token_utils","text":"<p>Functions:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count tokens in a string using a provided token counting function.</p> </li> </ul>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens","title":"count_tokens  <code>cached</code>","text":"<pre><code>count_tokens(\n    text: str, token_counter: Callable[[str], int]\n) -&gt; int\n</code></pre> <p>Count tokens in a string using a provided token counting function.</p> <p>Wraps the token counting function with error handling. Ensures the returned value is numeric and converts it to an integer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Number of tokens.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n...     return len(text.split())\n&gt;&gt;&gt; text = \"This is a sample sentence.\"\n&gt;&gt;&gt; count_tokens(text, simple_word_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n...     return len(text)\n&gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with a failing token counter\n&gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n...     raise ValueError(\"Something went wrong!\")\n&gt;&gt;&gt; try:\n...     count_tokens(\"test\", failing_counter)\n... except CallbackError as e:\n...     print(e)\nToken counter failed while processing text starting with: 'test...'.\n\ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\nDetails: Something went wrong!\n</code></pre> Source code in <code>src/chunklet/common/token_utils.py</code> <pre><code>@lru_cache(maxsize=1024)\ndef count_tokens(text: str, token_counter: Callable[[str], int]) -&gt; int:\n    \"\"\"\n    Count tokens in a string using a provided token counting function.\n\n    Wraps the token counting function with error handling. Ensures the returned\n    value is numeric and converts it to an integer.\n\n    Args:\n        text (str): Text to count tokens in.\n        token_counter (Callable[[str], int]): Function that returns the number of tokens.\n\n    Returns:\n        int: Number of tokens.\n\n    Raises:\n        CallbackError: If the token counter fails or returns an invalid type.\n\n    Examples:\n        &gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n        ...     return len(text.split())\n        &gt;&gt;&gt; text = \"This is a sample sentence.\"\n        &gt;&gt;&gt; count_tokens(text, simple_word_counter)\n        5\n\n        &gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n        ...     return len(text)\n        &gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n        5\n\n        &gt;&gt;&gt; # Example with a failing token counter\n        &gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n        ...     raise ValueError(\"Something went wrong!\")\n        &gt;&gt;&gt; try:\n        ...     count_tokens(\"test\", failing_counter)\n        ... except CallbackError as e:\n        ...     print(e)\n        Token counter failed while processing text starting with: 'test...'.\n        \ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\n        Details: Something went wrong!\n    \"\"\"\n    try:\n        token_count = token_counter(text)\n        if isinstance(token_count, (int, float)):\n            return int(token_count)\n        raise CallbackError(\n            f\"Token counter returned invalid type ({type(token_count).__name__}) \"\n            f\"for text starting with: '{text[:100]}'\"\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"Token counter failed while processing text starting with: '{text[:100]}...'.\\n\"\n            \"\ud83d\udca1 Hint: Please ensure the token counter function handles \"\n            f\"all edge cases and returns an integer. \\nDetails: {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>Text to count tokens in.</p>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>)           \u2013            <p>Function that returns the number of tokens.</p>"},{"location":"reference/chunklet/common/validation/","title":"validation","text":""},{"location":"reference/chunklet/common/validation/#chunklet.common.validation","title":"chunklet.common.validation","text":"<p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>restricted_iterable</code>             \u2013              <p>Creates a Pydantic Annotated type that represents a RestrictedIterable</p> </li> <li> <code>safely_count_iterable</code>             \u2013              <p>Counts elements in an iterable while preserving its state and forcing validation.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.restricted_iterable","title":"restricted_iterable","text":"<pre><code>restricted_iterable(*hints: Any) -&gt; Any\n</code></pre> <p>Creates a Pydantic Annotated type that represents a RestrictedIterable containing the specified hints (*hints), and applies a PlainValidator to reject str input.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def restricted_iterable(*hints: Any) -&gt; Any:\n    \"\"\"\n    Creates a Pydantic Annotated type that represents a RestrictedIterable\n    containing the specified hints (*hints), and applies a PlainValidator\n    to reject str input.\n    \"\"\"\n\n    def enforce_non_string(v: Any) -&gt; Any:\n        if isinstance(v, str):\n            # Pydantic-Core is sometimes pickier; using ValueError often works better\n            # with external validators than a raw TypeError\n            # Sliced to avoid overflowing screen\n            input_val = v if len(v) &lt; 500 else v[:500] + \"...\"\n            raise ValueError(\n                f\"Input cannot be a string.\\n  Found: (input={input_val!r}, type=str)\"\n            )\n        return v\n\n    ItemUnion = Union[hints] if len(hints) == 1 else hints[0]\n\n    # Build the Full Container Type\n    TargetType = (\n        list[ItemUnion]\n        | tuple[ItemUnion, ...]\n        | set[ItemUnion]\n        | frozenset[ItemUnion]\n        | Generator[ItemUnion, None, None]\n    )\n\n    # Create the Annotated Type\n    return Annotated[TargetType, PlainValidator(enforce_non_string)]\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable","title":"safely_count_iterable","text":"<pre><code>safely_count_iterable(\n    name: str, iterable: Iterable\n) -&gt; tuple[int, Iterable]\n</code></pre> <p>Counts elements in an iterable while preserving its state and forcing validation.</p> <p>If the input is an Iterator, it is duplicated using <code>itertools.tee</code> to prevent consumption during counting. The iteration simultaneously triggers any underlying Pydantic item validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[int, Iterable]</code>           \u2013            <p>tuple[int, Iterable]: The element count and the original (or preserved)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any element fails validation during the counting process.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With a list\n&gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 5\n&gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\nOriginal list is preserved: [1, 2, 3, 4, 5]\n</code></pre> <pre><code>&gt;&gt;&gt; # With an iterator (generator)\n&gt;&gt;&gt; my_iterator = (x for x in range(10))\n&gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 10\n&gt;&gt;&gt; # The iterator is preserved and can still be consumed\n&gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\nSum of preserved iterator: 45\n</code></pre> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>@validate_input\ndef safely_count_iterable(name: str, iterable: Iterable) -&gt; tuple[int, Iterable]:\n    \"\"\"\n    Counts elements in an iterable while preserving its state and forcing validation.\n\n    If the input is an Iterator, it is duplicated using `itertools.tee` to prevent\n    consumption during counting. The iteration simultaneously triggers any\n    underlying Pydantic item validation.\n\n    Args:\n        name (str): Descriptive name for the iterable (used in error context).\n        iterable (Iterable): The iterable or iterator to count and validate.\n\n    Returns:\n        tuple[int, Iterable]: The element count and the original (or preserved)\n\n    Raises:\n        InvalidInputError: If any element fails validation during the counting process.\n\n    Examples:\n        &gt;&gt;&gt; # With a list\n        &gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n        &gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 5\n        &gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\n        Original list is preserved: [1, 2, 3, 4, 5]\n\n        &gt;&gt;&gt; # With an iterator (generator)\n        &gt;&gt;&gt; my_iterator = (x for x in range(10))\n        &gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 10\n        &gt;&gt;&gt; # The iterator is preserved and can still be consumed\n        &gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\n        Sum of preserved iterator: 45\n    \"\"\"\n    try:  # If pydantic wrap it as ValidatorIterator object\n        # Tee if it's an iterator\n        if isinstance(iterable, Iterator):\n            iterable, copy_iterable = tee(iterable)\n            count = ilen(copy_iterable)\n        else:\n            count = len(iterable)\n    except ValidationError as e:\n        e.subtitle = name  # to be less generic\n        e.hint = \"\ud83d\udca1 Hint: Ensure all elements in the iterable are valid.\"\n        raise InvalidInputError(pretty_errors(e)) from None\n\n    return count, iterable\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Descriptive name for the iterable (used in error context).</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(iterable)","title":"<code>iterable</code>","text":"(<code>Iterable</code>)           \u2013            <p>The iterable or iterator to count and validate.</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker","title":"chunklet.document_chunker","text":"<p>Modules:</p> <ul> <li> <code>converters</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>processors</code>           \u2013            </li> <li> <code>registry</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> <ol> <li>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/","title":"converters","text":""},{"location":"reference/chunklet/document_chunker/converters/#chunklet.document_chunker.converters","title":"chunklet.document_chunker.converters","text":"<p>Modules:</p> <ul> <li> <code>html_2_md</code>           \u2013            </li> <li> <code>latex_2_md</code>           \u2013            </li> <li> <code>rst_2_md</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/","title":"html_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md","title":"chunklet.document_chunker.converters.html_2_md","text":"<p>Functions:</p> <ul> <li> <code>html_to_md</code>             \u2013              <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md","title":"html_to_md","text":"<pre><code>html_to_md(\n    file_path: str | Path = None,\n    raw_text: str | None = None,\n    max_url_length: int = 150,\n) -&gt; str\n</code></pre> <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/html_2_md.py</code> <pre><code>def html_to_md(\n    file_path: str | Path = None, raw_text: str | None = None, max_url_length: int = 150\n) -&gt; str:\n    \"\"\"\n    Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.\n\n    Args:\n        file_path (str | Path): Path to the html file.\n        raw_text (str, optional): Raw HTML text. If both file_path and raw_text is provided,\n            then raw_text will be used instead.\n        max_url_length (int): The maximum length of a URL. Defaults to 150.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if md is None:\n        raise ImportError(\n            \"The 'markdownify' library is not installed. \"\n            \"Please install it with 'pip install markdownify' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    if raw_text:\n        markdown_content = md(raw_text)\n    elif file_path:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            markdown_content = md(f.read())\n    else:\n        raise ValueError(\"Either file_path or raw_text must be provided.\")\n\n    # Normalize consecutive newlines that are more than 2\n    markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n    # Truncate long URLs in Markdown links or images\n    def truncate_url(match: re.Match) -&gt; str:\n        prefix, url = match.group(1), match.group(2)\n        if len(url) &gt; max_url_length:\n            url = url[: max_url_length - 3] + \"...\"\n        return f\"{prefix}({url})\"\n\n    return re.sub(r\"(!?\\[.*?\\])\\((.*?)\\)\", truncate_url, markdown_content)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>, default:                   <code>None</code> )           \u2013            <p>Path to the html file.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(raw_text)","title":"<code>raw_text</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Raw HTML text. If both file_path and raw_text is provided, then raw_text will be used instead.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(max_url_length)","title":"<code>max_url_length</code>","text":"(<code>int</code>, default:                   <code>150</code> )           \u2013            <p>The maximum length of a URL. Defaults to 150.</p>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/","title":"latex_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md","title":"chunklet.document_chunker.converters.latex_2_md","text":"<p>Functions:</p> <ul> <li> <code>latex_to_md</code>             \u2013              <p>Convert LaTeX code to Markdown-style plain text.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md","title":"latex_to_md","text":"<pre><code>latex_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Convert LaTeX code to Markdown-style plain text.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in markdown</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/latex_2_md.py</code> <pre><code>def latex_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Convert LaTeX code to Markdown-style plain text.\n\n    Args:\n        file_path (str | Path): Path to the latex file.\n\n    Returns:\n        str: The full text content in markdown\n    \"\"\"\n    if LatexNodes2Text is None:\n        raise ImportError(\n            \"The 'pylatexenc' library is not installed. \"\n            \"Please install it with 'pip install 'pylatexenc&gt;=2.10'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n        latex_code = f.read()\n\n    # Convert to text\n    latex_node = LatexNodes2Text()\n    text = latex_node.latex_to_text(latex_code)\n\n    # Replace \u00a7 by #\n    markdown_content = re.sub(r\"\u00a7\\.?\", \"#\", text)\n\n    # Normalize consecutive newlines more than two\n    return re.sub(r\"\\n{2,}\", \"\\n\\n\", markdown_content.strip())\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the latex file.</p>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/","title":"rst_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md","title":"chunklet.document_chunker.converters.rst_2_md","text":"<p>Functions:</p> <ul> <li> <code>rst_to_md</code>             \u2013              <p>Converts reStructuredText (RST) content into Markdown.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md","title":"rst_to_md","text":"<pre><code>rst_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Converts reStructuredText (RST) content into Markdown.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/rst_2_md.py</code> <pre><code>def rst_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Converts reStructuredText (RST) content into Markdown.\n\n    Args:\n        file_path (str | Path): Path to the rst file.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if publish_string is None:\n        raise ImportError(\n            \"The 'docutils' library is not installed. \"\n            \"Please install it with 'pip install 'docutils&gt;=0.21.2'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n\n    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        rst_content = f.read()\n\n    # Convert the rst content to HTML first\n    html_content = publish_string(source=rst_content, writer_name=\"html\").decode(\n        \"utf-8\"\n    )\n\n    # Now we can convert it to markdown\n    markdown_content = html_to_md(raw_text=html_content)\n    return markdown_content\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the rst file.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker","title":"chunklet.document_chunker.document_chunker","text":"<p>Classes:</p> <ul> <li> <code>DocumentChunker</code>           \u2013            <p>A comprehensive document chunker that handles various file formats.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker","title":"DocumentChunker","text":"<pre><code>DocumentChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>A comprehensive document chunker that handles various file formats.</p> <p>This class provides a high-level interface to chunk text from different document types. It automatically detects the file format and uses the appropriate method to extract content before passing it to an underlying <code>PlainTextChunker</code> instance.</p> <p>Key Features: - Multi-Format Support: Chunks text from PDF, TXT, MD, and RST files. - Metadata Enrichment: Automatically adds source file path and other   document-level metadata (e.g., PDF page numbers) to each chunk. - Bulk Processing: Efficiently chunks multiple documents in a single call. - Pluggable Document processors: Integrate custom processors allowing definition of specific logic for extracting text from various file types.</p> <p>Initializes the DocumentChunker.</p> <p>Parameters:</p> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Chunks multiple documents from a list of file paths.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunks a single document from a given path.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>supported_extensions</code>           \u2013            <p>Get the supported extensions, including the custom ones.</p> </li> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>def __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initializes the DocumentChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter | None): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    # Explicit type validation for sentence_splitter\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    self.plain_text_chunker = PlainTextChunker(\n        sentence_splitter=sentence_splitter,\n        verbose=self._verbose,\n        continuation_marker=self.continuation_marker,\n        token_counter=self.token_counter,\n    )\n\n    self.processors = {\n        \".pdf\": pdf_processor.PDFProcessor,\n        \".epub\": epub_processor.EpubProcessor,\n        \".docx\": docx_processor.DocxProcessor,\n    }\n    self.converters = {\n        \".html\": html_2_md.html_to_md,\n        \".hml\": html_2_md.html_to_md,\n        \".rst\": rst_2_md.rst_to_md,\n        \".tex\": latex_2_md.latex_to_md,\n    }\n    self.processor_registry = CustomProcessorRegistry()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.supported_extensions","title":"supported_extensions  <code>property</code>","text":"<pre><code>supported_extensions\n</code></pre> <p>Get the supported extensions, including the custom ones.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Chunks multiple documents from a list of file paths.</p> <p>This method is a memory-efficient generator that yields chunks as they are processed, without loading all documents into memory at once. It handles various file types.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Chunks multiple documents from a list of file paths.\n\n    This method is a memory-efficient generator that yields chunks as they\n    are processed, without loading all documents into memory at once. It\n    handles various file types.\n\n    Args:\n        paths (restricted_iterable[str | Path]): A restricted iterable of paths to the document files.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n               Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors: How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    sentinel = object()\n\n    # Validate all paths upfront\n    sucess_count = 0\n    validated_paths = []\n    for i, path in enumerate(paths):\n        path = Path(path)\n        try:\n            ext = self._validate_and_get_extension(path)\n            validated_paths.append((path, ext, None))\n            sucess_count += 1\n        except Exception as e:\n            validated_paths.append((path, None, e))\n\n    gathered_data = self._gather_all_data(validated_paths, on_errors)\n\n    all_chunks_gen = self.plain_text_chunker.batch_chunk(\n        texts=gathered_data[\"all_texts_gen\"],\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n        separator=sentinel,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n    )\n\n    all_chunk_groups = split_at(all_chunks_gen, lambda x: x is sentinel)\n    path_section_counts = gathered_data[\"path_section_counts\"]\n    all_metadata = gathered_data[\"all_metadata\"]\n\n    # HACK: Since a sentinel is always at the end of the gen,\n    # the last list of the groups will be an empty one.\n    # The only work-around to add a sentinel at paths\n    paths = list(path_section_counts.keys()) + [None]\n\n    doc_count = 0\n    curr_path = paths[0]\n    for chunks in all_chunk_groups:\n        if path_section_counts[curr_path] == 0:\n            if separator is not None:\n                yield separator\n\n            doc_count += 1\n            curr_path = paths[doc_count]\n            if curr_path is None:\n                return\n\n        for i, ch in enumerate(chunks, start=1):\n            doc_metadata = all_metadata[doc_count]\n            doc_metadata[\"section_count\"] = path_section_counts[curr_path]\n            doc_metadata[\"curr_section\"] = i\n\n            ch[\"metadata\"].update(doc_metadata)\n            yield ch\n\n        path_section_counts[curr_path] -= 1\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(paths)","title":"<code>paths</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of paths to the document files.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs.    Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single document from a given path.</p> <p>This method automatically detects the file type and uses the appropriate processor to extract text before chunking. It then adds document-level metadata to each resulting chunk.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each representing</p> </li> <li> <code>list[Box]</code>           \u2013            <p>a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single document from a given path.\n\n    This method automatically detects the file type and uses the appropriate\n    processor to extract text before chunking. It then adds document-level\n    metadata to each resulting chunk.\n\n    Args:\n        path (str | Path): The path to the document file.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each representing\n        a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    path = Path(path)\n    ext = self._validate_and_get_extension(path)\n\n    text_content_or_generator, document_metadata = self._extract_data(path, ext)\n\n    if not isinstance(text_content_or_generator, str):\n        raise UnsupportedFileTypeError(\n            f\"File type '{ext}' is not supported by the general chunk method.\\n\"\n            \"Reason: The processor for this file returns iterable, \"\n            \"so it must be processed in parallel for efficiency.\\n\"\n            \"\ud83d\udca1 Hint: use `chunker.batch_chunk()` for this file type.\"\n        )\n\n    if self.verbose:\n        logger.info(\"Starting chunk processing for path: {}.\", path)\n\n    text_content = text_content_or_generator\n\n    # Process as a single block of text\n    chunk_boxes = self.plain_text_chunker.chunk(\n        text=text_content,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n        base_metadata=document_metadata,\n    )\n\n    if self.verbose:\n        logger.info(\"Generated {} chunks for {}.\", len(chunk_boxes), path)\n\n    return chunk_boxes\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>The path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/processors/","title":"processors","text":""},{"location":"reference/chunklet/document_chunker/processors/#chunklet.document_chunker.processors","title":"chunklet.document_chunker.processors","text":"<p>Modules:</p> <ul> <li> <code>base_processor</code>           \u2013            </li> <li> <code>docx_processor</code>           \u2013            </li> <li> <code>epub_processor</code>           \u2013            </li> <li> <code>pdf_processor</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/","title":"base_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor","title":"chunklet.document_chunker.processors.base_processor","text":"<p>Classes:</p> <ul> <li> <code>BaseProcessor</code>           \u2013            <p>Abstract base class for document processors, providing a unified interface</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor","title":"BaseProcessor","text":"<pre><code>BaseProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for document processors, providing a unified interface for extracting text and metadata from documents.</p> <p>Initializes the processor with the path to the document.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the document.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields text content from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_metadata","title":"extract_metadata  <code>abstractmethod</code>","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the document.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: Dictionary containing document metadata.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts metadata from the document.\n\n    Returns:\n        dict[str, Any]: Dictionary containing document metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_text","title":"extract_text  <code>abstractmethod</code>","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields text content from the document.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Text content chunks from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields text content from the document.\n\n    Yields:\n        str: Text content chunks from the document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/","title":"docx_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor","title":"chunklet.document_chunker.processors.docx_processor","text":"<p>Classes:</p> <ul> <li> <code>DocxProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from DOCX files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DocxProcessor","title":"DocxProcessor","text":"<pre><code>DocxProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from DOCX files.</p> <p>Text content is extracted, images are replaced with a placeholder, and the resulting text is formatted using Markdown conversion.</p> <p>This class extracts metadata which typically uses a mix of Open Packaging Conventions (OPC) properties and elements that align with Dublin Core standards.</p> <p>For more details on the DOCX core properties processed, refer to the <code>python-docx</code> documentation: https://python-docx.readthedocs.io/en/latest/dev/analysis/features/coreprops.html</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Extracts the text content from the DOCX file in Markdown format.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DocxProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - publisher - last_modified_by - created - modified - rights - version</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - publisher\n            - last_modified_by\n            - created\n            - modified\n            - rights\n            - version\n    \"\"\"\n    try:\n        from docx import Document\n    except ImportError as e:\n        raise ImportError(\n            \"The 'python-docx' library is not installed. \"\n            \"Please install it with 'pip install 'python-docx&gt;=1.2.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n\n    doc = Document(self.file_path)\n    props = doc.core_properties\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        value = getattr(props, field, \"\")\n        if value:\n            metadata[field] = str(value)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DocxProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Extracts the text content from the DOCX file in Markdown format.</p> <p>Images are replaced with a placeholder \"[Image - num]\". Text is yielded in blocks of approximately 10 paragraphs each.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A block of Markdown text, approximately 10 paragraphs each.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Extracts the text content from the DOCX file in Markdown format.\n\n    Images are replaced with a placeholder \"[Image - num]\".\n    Text is yielded in blocks of approximately 10 paragraphs each.\n\n    Yields:\n        str: A block of Markdown text, approximately 10 paragraphs each.\n    \"\"\"\n    try:  # Lazy import\n        import mammoth\n    except ImportError as e:\n        raise ImportError(\n            \"The 'mammoth' library is not installed. \"\n            \"Please install it with 'pip install 'mammoth&gt;=1.9.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n\n    count = 0\n\n    def placeholder_images(image):\n        \"\"\"Replace all images with a placeholder text.\"\"\"\n        nonlocal count\n        count += 1\n        return [mammoth.html.text(f\"[Image - {count}]\")]\n\n    with open(self.file_path, \"rb\") as docx_file:\n        # Convert DOCX to HTML first\n        result = mammoth.convert_to_html(\n            docx_file, convert_image=placeholder_images\n        )\n        html_content = result.value\n\n    # Now we can convert it to markdown\n    markdown_content = html_to_md(raw_text=html_content)\n\n    # Chunk its paragraphs into groups of 8 for faster processing.\n    paragraphs = markdown_content.split(\"\\n\\n\")\n    for paragraph_chunk in chunked(paragraphs, 8):\n        yield \"\\n\\n\".join(paragraph_chunk)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/","title":"epub_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor","title":"chunklet.document_chunker.processors.epub_processor","text":"<p>Classes:</p> <ul> <li> <code>EpubProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from EPUB files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EpubProcessor","title":"EpubProcessor","text":"<pre><code>EpubProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from EPUB files.</p> <p>Text content is extracted by concatenating the text from all HTML content documents within the EPUB container.</p> <p>This processor focuses on extracting core metadata following the Dublin Core Metadata Initiative (DCMI) standard, which is the common practice in EPUB files. Not all available metadata fields are extracted.</p> <p>For more details on EPUB metadata and the Dublin Core standard, refer to the <code>ebooklib</code> tutorial:</p> <p>https://docs.sourcefabric.org/projects/ebooklib/en/latest/tutorial.html</p> <p>Initializes the EpubProcessor with a path to the EPUB file and reads the EPUB book into memory.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts Dublin Core metadata from the EPUB file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields Markdown-converted text from all document items in the EPUB file.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the EpubProcessor with a path to the EPUB file\n    and reads the EPUB book into memory.\n\n    Args:\n        file_path (str): Path to the EPUB file.\n    \"\"\"\n    if not epub:\n        raise ImportError(\n            \"The 'ebooklib' library is not installed. \"\n            \"Please install it with 'pip install 'ebooklib&gt;=0.19'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        )\n    self.file_path = file_path\n    self.book = epub.read_epub(file_path)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EpubProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the EPUB file.</p>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EpubProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts Dublin Core metadata from the EPUB file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields. - title - creator - contributor - publisher - date - rights</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts Dublin Core metadata from the EPUB file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields.\n            - title\n            - creator\n            - contributor\n            - publisher\n            - date\n            - rights\n    \"\"\"\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        values = [v[0] for v in self.book.get_metadata(\"DC\", field)]\n        if values:\n            metadata[field] = \", \".join(values)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EpubProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields Markdown-converted text from all document items in the EPUB file.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown-formatted text of each document item.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields Markdown-converted text from all document items in the EPUB file.\n\n    Yields:\n        str: Markdown-formatted text of each document item.\n    \"\"\"\n    for idref, _ in self.book.spine:\n        item = self.book.get_item_with_id(idref)\n        html_content = item.get_body_content().decode(\"utf-8\", errors=\"ignore\")\n        md_content = html_to_md(raw_text=html_content)\n        yield md_content.strip()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/","title":"pdf_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor","title":"chunklet.document_chunker.processors.pdf_processor","text":"<p>Classes:</p> <ul> <li> <code>PDFProcessor</code>           \u2013            <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor","title":"PDFProcessor","text":"<pre><code>PDFProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> <p>Provides methods to extract text and metadata from PDF files, while cleaning and normalizing the extracted text using regex patterns.</p> <p>This processor extracts metadata from the PDF document's information dictionary, focusing on core metadata rather than all available fields.</p> <p>For more details on PDF metadata extraction using <code>pdfminer.six</code>, refer to this relevant Stack Overflow discussion:</p> <p>https://stackoverflow.com/questions/75591385/extract-metadata-info-from-online-pdf-using-pdfminer-in-python</p> <p>Initialize the PDFProcessor.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the PDF document's information dictionary.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yield cleaned text from each PDF page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize the PDFProcessor.\n\n    Args:\n        file_path (str): Path to the PDF file.\n    \"\"\"\n    try:\n        from pdfminer.layout import LAParams\n    except ImportError as e:\n        raise ImportError(\n            \"The 'pdfminer.six' library is not installed. \"\n            \"Please install it with 'pip install 'pdfminer.six&gt;=20250324'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[document]''\"\n        ) from e\n    self.file_path = file_path\n    self.laparams = LAParams(\n        line_margin=0.5,\n    )\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the PDF file.</p>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the PDF document's information dictionary.</p> <p>Includes source path, page count, and PDF info fields.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - creator - producer - publisher - created - modified</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts metadata from the PDF document's information dictionary.\n\n    Includes source path, page count, and PDF info fields.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - creator\n            - producer\n            - publisher\n            - created\n            - modified\n    \"\"\"\n    from pdfminer.pdfpage import PDFPage\n    from pdfminer.pdfparser import PDFParser\n    from pdfminer.pdfdocument import PDFDocument\n\n    metadata = {\"source\": str(self.file_path), \"page_count\": 0}\n    with open(self.file_path, \"rb\") as f:\n        # Initialize parser on the file stream\n        parser = PDFParser(f)\n\n        # The PDFDocument constructor reads file structure and advances the pointer\n        doc = PDFDocument(parser)\n\n        # Count pages: Reset pointer to the start of the file stream to count pages correctly\n        f.seek(0)\n\n        metadata[\"page_count\"] = ilen(PDFPage.get_pages(f))\n\n        # Extract info fields from the document object\n        if hasattr(doc, \"info\") and doc.info:\n            for info in doc.info:\n                for k, v in info.items():\n\n                    # To keep metadata uniform\n                    k = \"created\" if k == \"CreationDate\" else k\n                    k = \"modified\" if k == \"ModDate\" else k\n\n                    if k.lower() in self.METADATA_FIELDS:\n                        if isinstance(k, bytes):\n                            k = k.decode(\"utf-8\", \"ignore\")\n                        if isinstance(v, bytes):\n                            v = v.decode(\"utf-8\", \"ignore\")\n                        metadata[k.lower()] = v\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yield cleaned text from each PDF page.</p> <p>Uses pdfminer.high_level.extract_text for efficient page-by-page extraction.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown-formatted text of each page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Yield cleaned text from each PDF page.\n\n    Uses pdfminer.high_level.extract_text for efficient page-by-page extraction.\n\n    Yields:\n        str: Markdown-formatted text of each page.\n    \"\"\"\n    from pdfminer.high_level import extract_text\n    from pdfminer.pdfpage import PDFPage\n\n    with open(self.file_path, \"rb\") as fp:\n        page_count = ilen(PDFPage.get_pages(fp))\n\n        for page_num in range(page_count):\n            # Call extract_text on the file path, specifying the page number.\n            # This is efficient as it avoids repeated file seeks/parsing\n            # within the loop that was present in the old `extract_text_to_fp` approach.\n            raw_text = extract_text(\n                self.file_path,\n                page_numbers=[page_num],\n                laparams=self.laparams,\n            )\n            yield self._cleanup_text(raw_text)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/","title":"registry","text":""},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry","title":"chunklet.document_chunker.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> <ol> <li>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/exceptions/","title":"exceptions","text":""},{"location":"reference/chunklet/exceptions/#chunklet.exceptions","title":"chunklet.exceptions","text":"<p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the `chunk` method, like `chunker.chunk(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p>"},{"location":"reference/chunklet/plain_text_chunker/","title":"plain_text_chunker","text":""},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker","title":"chunklet.plain_text_chunker","text":"<p>Classes:</p> <ul> <li> <code>PlainTextChunker</code>           \u2013            <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker","title":"PlainTextChunker","text":"<pre><code>PlainTextChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> <p>Key Features: - Flexible Constraint-Based Chunking: Segment text by specifying limits on sentence count, token count and section breaks or combination of them. - Clause-Level Overlap: Ensures semantic continuity between chunks by overlapping at natural clause boundaries with Customizable continuation marker. - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Pluggable Token Counters: Integrate custom token counting functions (e.g., for specific LLM tokenizers). - Parallel Processing: Efficiently handles batch chunking of multiple texts using multiprocessing. - Memory friendly batching: Yields chunks one at a time, reducing memory usage, especially for very large documents.</p> <p>Initialize The PlainTextChunker.</p> <p>Parameters:</p> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Processes a batch of texts in parallel, splitting each into chunks.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunks a single text into smaller pieces based on specified parameters.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize The PlainTextChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter, optional): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int], optional): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    # Initialize SentenceSplitter\n    self.sentence_splitter = sentence_splitter or SentenceSplitter()\n    self.sentence_splitter.verbose = self._verbose\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of texts in parallel, splitting each into chunks. Leverages multiprocessing for efficient batch chunking.</p> <p>If a task fails, <code>chunklet</code> will now stop processing and return the results of the tasks that completed successfully, preventing wasted work.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If <code>texts</code> is not an iterable of strings, or if <code>n_jobs</code> is less than 1.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of texts in parallel, splitting each into chunks.\n    Leverages multiprocessing for efficient batch chunking.\n\n    If a task fails, `chunklet` will now stop processing and return the results\n    of the tasks that completed successfully, preventing wasted work.\n\n    Args:\n        texts (restricted_iterable[str]): A restricted iterable of input texts to be chunked.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): The token counting function.\n            Required if `max_tokens` is set.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n            Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]): How to handle errors during processing.\n            Defaults to 'raise'.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n\n    Raises:\n        InvalidInputError: If `texts` is not an iterable of strings, or if `n_jobs` is less than 1.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting\n            or token counting within a chunking task.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        overlap_percent=overlap_percent,\n        max_section_breaks=max_section_breaks,\n        offset=offset,\n        base_metadata=base_metadata,\n        token_counter=token_counter or self.token_counter,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=texts,\n        iterable_name=\"texts\",\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        separator=separator,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(texts)","title":"<code>texts</code>","text":"(<code>restricted_iterable[str]</code>)           \u2013            <p>A restricted iterable of input texts to be chunked.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>The token counting function. Required if <code>max_tokens</code> is set.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single text into smaller pieces based on specified parameters. Supports flexible constraint-based chunking, clause-level overlap, and custom token counters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each containing the chunk content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any chunking configuration parameter is invalid.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/plain_text_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single text into smaller pieces based on specified parameters.\n    Supports flexible constraint-based chunking, clause-level overlap,\n    and custom token counters.\n\n    Args:\n        text (str): The input text to chunk.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-75). Defaults to 20\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): Optional token counting function.\n            Required for token-based modes only.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each containing the chunk content and metadata.\n\n    Raises:\n        InvalidInputError: If any chunking configuration parameter is invalid.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting or token counting within a chunking task.\n    \"\"\"\n    # Validate that at least one limit is provided\n    if not any((max_tokens, max_sentences, max_section_breaks)):\n        raise InvalidInputError(\n            \"At least one of 'max_tokens', 'max_sentences', or 'max_section_break' must be provided.\"\n        )\n\n    # If token_counter is required but not provided\n    if max_tokens is not None and not (token_counter or self.token_counter):\n        raise MissingTokenCounterError()\n\n    if self.verbose:\n        logger.info(\n            \"Starting chunk processing for text starting with: {}.\",\n            f\"{text[:100]}...\",\n        )\n\n    # Adjust limits for _group_by_chunk's internal use\n    if max_tokens is None:\n        max_tokens = sys.maxsize\n    if max_sentences is None:\n        max_sentences = sys.maxsize\n    if max_section_breaks is None:\n        max_section_breaks = sys.maxsize\n\n    if not text.strip():\n        if self.verbose:\n            logger.info(\"Input text is empty. Returning empty list.\")\n        return []\n\n    try:\n        sentences = self.sentence_splitter.split(\n            text,\n            lang,\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"An error occurred during the sentence splitting process.\\nDetails: {e}\\n\"\n            \"\ud83d\udca1 Hint: This may be due to an issue with the underlying sentence splitting library.\"\n        ) from e\n\n    if not sentences:\n        return []\n\n    offset = round(offset)\n    if offset &gt;= len(sentences):\n        logger.warning(\n            \"Offset {} &gt;= total sentences {}. Returning empty list.\",\n            offset,\n            len(sentences),\n        )\n        return []\n\n    chunks = self._group_by_chunk(\n        sentences[offset:],\n        token_counter=token_counter or self.token_counter,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n    )\n\n    if base_metadata is None:\n        base_metadata = {}\n\n    return self._create_chunk_boxes(chunks, base_metadata, text)\n</code></pre>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to chunk.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-75). Defaults to 20</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required for token-based modes only.</p>"},{"location":"reference/chunklet/plain_text_chunker/#chunklet.plain_text_chunker.PlainTextChunker.chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter","title":"chunklet.sentence_splitter","text":"<p>Modules:</p> <ul> <li> <code>languages</code>           \u2013            <p>This module contains the language sets for the supported sentence splitters.</p> </li> <li> <code>registry</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Abstract base class for sentence splitting.</p> </li> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sentence splitting. Defines the interface that all sentence splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str, lang: str) -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>text (str): The input text to be split.     lang (str): The language of the text (e.g., 'en', 'fr', 'auto').</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MySplitter(BaseSplitter):\n...     def split(self, text: str, lang: str) -&gt; list[str]:\n...         return text.split(\".\")\n&gt;&gt;&gt; splitter = MySplitter()\n&gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n['Hello', ' World']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, text: str, lang: str) -&gt; list[str]:\n    \"\"\"\n    Splits the given text into a list of sentences.\n\n    text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n\n    Examples:\n        &gt;&gt;&gt; class MySplitter(BaseSplitter):\n        ...     def split(self, text: str, lang: str) -&gt; list[str]:\n        ...         return text.split(\".\")\n        &gt;&gt;&gt; splitter = MySplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n        ['Hello', ' World']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> <ol> <li>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; List[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        List[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.custom_splitter_registry = CustomSplitterRegistry()\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for langid\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    if self.verbose:\n        logger.info(\n            \"Language detection: '{}' with confidence {}.\",\n            lang_detected,\n            f\"{round(confidence)  * 10}/10\",\n        )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        if self.verbose:\n            logger.info(\"Input text is empty. Returning empty list.\")\n        return []\n    sentences = []\n\n    if lang == \"auto\":\n        if self.verbose:\n            logger.warning(\n                \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n            )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if self.custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = self.custom_splitter_registry.split(text, lang)\n        if self.verbose:\n            logger.info(\"Using registered splitter: {}\", splitter_name)\n    elif lang in PYSBD_SUPPORTED_LANGUAGES:\n        sentences = Segmenter(language=lang).segment(text)\n    elif lang in SENTSPLIT_UNIQUE_LANGUAGES:\n        sentences = SentSplit(lang).segment(text)\n    elif lang in INDIC_NLP_UNIQUE_LANGUAGES:\n        sentences = sentence_tokenize.sentence_split(text, lang)\n    elif lang in SENTENCEX_UNIQUE_LANGUAGES:\n        sentences = segment(lang, text)\n    else:\n        if self.verbose:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n        sentences = self.fallback_splitter.split(text)\n\n    # Apply post-processing filter\n    processed_sentences = self._filter_sentences(sentences)\n\n    if self.verbose:\n        logger.info(\n            \"Text splitted into sentences. Total sentences detected: {}\",\n            len(processed_sentences),\n        )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            f\"{ind}) {formatted_loc} {msg}.\\n\"\n            f\"  Found: (input={input_value!r}, type={input_type})\"\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/","title":"_fallback_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter","title":"chunklet.sentence_splitter._fallback_splitter","text":"<p>Classes:</p> <ul> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; List[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        List[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/languages/","title":"languages","text":""},{"location":"reference/chunklet/sentence_splitter/languages/#chunklet.sentence_splitter.languages","title":"chunklet.sentence_splitter.languages","text":"<p>This module contains the language sets for the supported sentence splitters. Each set is filtered to contain only the languages truly unique to that library.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/","title":"registry","text":""},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry","title":"chunklet.sentence_splitter.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways: 1. As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> <ol> <li>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter","title":"chunklet.sentence_splitter.sentence_splitter","text":"<p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Abstract base class for sentence splitting.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sentence splitting. Defines the interface that all sentence splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str, lang: str) -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>text (str): The input text to be split.     lang (str): The language of the text (e.g., 'en', 'fr', 'auto').</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MySplitter(BaseSplitter):\n...     def split(self, text: str, lang: str) -&gt; list[str]:\n...         return text.split(\".\")\n&gt;&gt;&gt; splitter = MySplitter()\n&gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n['Hello', ' World']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@abstractmethod\ndef split(self, text: str, lang: str) -&gt; list[str]:\n    \"\"\"\n    Splits the given text into a list of sentences.\n\n    text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n\n    Examples:\n        &gt;&gt;&gt; class MySplitter(BaseSplitter):\n        ...     def split(self, text: str, lang: str) -&gt; list[str]:\n        ...         return text.split(\".\")\n        &gt;&gt;&gt; splitter = MySplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello. World.\", \"en\")\n        ['Hello', ' World']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.custom_splitter_registry = CustomSplitterRegistry()\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for langid\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    if self.verbose:\n        logger.info(\n            \"Language detection: '{}' with confidence {}.\",\n            lang_detected,\n            f\"{round(confidence)  * 10}/10\",\n        )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        if self.verbose:\n            logger.info(\"Input text is empty. Returning empty list.\")\n        return []\n    sentences = []\n\n    if lang == \"auto\":\n        if self.verbose:\n            logger.warning(\n                \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n            )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if self.custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = self.custom_splitter_registry.split(text, lang)\n        if self.verbose:\n            logger.info(\"Using registered splitter: {}\", splitter_name)\n    elif lang in PYSBD_SUPPORTED_LANGUAGES:\n        sentences = Segmenter(language=lang).segment(text)\n    elif lang in SENTSPLIT_UNIQUE_LANGUAGES:\n        sentences = SentSplit(lang).segment(text)\n    elif lang in INDIC_NLP_UNIQUE_LANGUAGES:\n        sentences = sentence_tokenize.sentence_split(text, lang)\n    elif lang in SENTENCEX_UNIQUE_LANGUAGES:\n        sentences = segment(lang, text)\n    else:\n        if self.verbose:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n        sentences = self.fallback_splitter.split(text)\n\n    # Apply post-processing filter\n    processed_sentences = self._filter_sentences(sentences)\n\n    if self.verbose:\n        logger.info(\n            \"Text splitted into sentences. Total sentences detected: {}\",\n            len(processed_sentences),\n        )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/terminators/","title":"terminators","text":""},{"location":"reference/chunklet/sentence_splitter/terminators/#chunklet.sentence_splitter.terminators","title":"chunklet.sentence_splitter.terminators","text":""}]}