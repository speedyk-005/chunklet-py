{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Chunklet-py Documentation!","text":""},{"location":"#what-is-chunklet-anyway-and-why-should-you-care","title":"What is Chunklet, Anyway? (And Why Should You Care?)","text":"<p>So, you've got a mountain of text and a tiny little pickaxe? Fear not! You've stumbled upon the official (and slightly quirky) documentation for Chunklet-py, your new heavy machinery for demolishing text into perfectly sized, context-aware chunks. </p> <p>At its core, Chunklet is a smart text chunking utility. Whether you're preparing data for Large Language Models (LLMs), building a Retrieval-Augmented Generation (RAG) system, or just need to break down a long document, Chunklet has your back. It handles the messy business of text segmentation so you don't have to.</p> <p>Chunklet is a Python library for multilingual, context-aware text chunking optimized for large language model (LLM) and retrieval-augmented generation (RAG) pipelines. It splits long documents into manageable segments while preserving semantic boundaries, enabling efficient indexing, embedding, and inference.</p>"},{"location":"#did-you-know","title":"Did You Know?","text":"<p>\ud83d\udca1 Tip: Chunklet's <code>overlap_percent</code> works at the clause level, not just sentence or token boundaries! This means it intelligently preserves semantic flow across chunks, making your LLMs smarter and your RAG pipelines more effective.</p>"},{"location":"#why-bother-with-fancy-chunking","title":"Why Bother with Fancy Chunking?","text":"<p>Look, you could just split your text by character count or paragraphs. But let's be honest, that's like performing surgery with a butter knife. Standard splitting methods often:</p> <ul> <li>Commit literary butchery: They'll chop sentences right in the middle of a thought.</li> <li>Get lost in translation: They don't care about the rules of non-English languages.</li> <li>Have the memory of a goldfish: They forget the context of the previous chunk, leaving you with a mess of disconnected ideas.</li> </ul> <p>Chunklet is the smart surgeon. It understands the structure of your text, using fancy tricks like clause-level overlapping to keep the meaning intact. It's like a linguistic artist, carefully preserving the masterpiece that is your data.</p>"},{"location":"#why-chunklet","title":"\ud83e\udd14 Why Chunklet?","text":"Feature Why it\u2019s elite \u26d3\ufe0f Hybrid Mode Combines token + sentence limits with guaranteed overlap \u2014 rare even in commercial stacks. \ud83c\udf10 Multilingual Fallbacks Pysbd &gt; SentenceSplitter &gt; Regex, with dynamic confidence detection. \u27bf Clause-Level Overlap `overlap_percent operates at the clause level, preserving semantic flow across chunks using logic. \u26a1 Parallel Batch Processing Efficient parallel processing with <code>ThreadPoolExecutor</code>, optimized for low overhead on small batches. \u267b\ufe0f LRU Caching Smart memoization via <code>functools.lru_cache</code>. \ud83e\ude84 Pluggable Token Counters Swap in GPT-2, BPE, or your own tokenizer. \u2702\ufe0f Pluggable Sentence splitters Integrate custom splitters for more specific languages."},{"location":"#ready-to-dive-in","title":"Ready to Dive In?","text":"<p>Here's how to get your hands dirty:</p> <ul> <li>Installation: Get Chunklet on your machine. We've made it as painless as possible.</li> <li>Getting Started (CLI &amp; Programmatic): Whether you're a command-line cowboy or a Python purist, we've got you covered.</li> </ul>"},{"location":"#the-grand-tour","title":"The Grand Tour","text":"<p>Wanna know what's under the hood?</p> <ul> <li>Models: Check out the different ways you can configure Chunklet.</li> <li>Supported Languages: See which languages Chunklet speaks fluently.</li> <li>Internal Flow: For those who like to know how the sausage is made.</li> <li>Utility Functions: The secret sauce that makes Chunklet so powerful.</li> <li>Exceptions and Warnings: Because sometimes, things go wrong. Here's what to do when they do.</li> </ul> <p>## Keeping Up-to-Date </p> <p>Stay informed about Chunklet's evolution:</p> <ul> <li>Changelog: See what's new, what's fixed, and what's been improved in recent versions.</li> <li>Benchmarks: Curious about performance? Check out how Chunklet stacks up.</li> </ul> <p>## Project Information &amp; Contributing For the serious stuff (and if you want to join the fun):</p> <ul> <li>GitHub Repository: The main hub for all things Chunklet.</li> <li>License Information: All the necessary bits and bobs about Chunklet's license.</li> <li>Contributing: Want to help make Chunklet even better? Find out how you can contribute!                                            </li> </ul>"},{"location":"exceptions-and-warnings/","title":"Exceptions and Warnings: When Chunklet Gets a Bit Grumpy","text":"<p>Even the most robust tools have their moments, and Chunklet is no exception (pun intended!). This page is your guide to understanding the various hiccups and murmurs you might encounter while using Chunklet. Don't worry, most of them are easily fixed, and some are just friendly nudges.</p>"},{"location":"exceptions-and-warnings/#exceptions-when-things-go-sideways-and-stop","title":"Exceptions: When Things Go Sideways (and Stop)","text":"<p>These are the big ones. When Chunklet throws an exception, it means something went wrong enough to halt the process. But fear not, we'll tell you why!</p>"},{"location":"exceptions-and-warnings/#chunkleterror","title":"ChunkletError","text":"<p>This is the grand-daddy of all Chunklet-specific errors. If you see this, it means something fundamental went wrong within Chunklet itself. It's usually a sign that a deeper issue occurred, often related to a custom function you provided.</p> <ul> <li>Common Scenario: Your custom <code>token_counter</code> function decided to take a coffee break (i.e., it raised an exception).</li> <li>What to do: Check your custom <code>token_counter</code> or any other custom callbacks you've provided. Make sure they're robust and handle all possible inputs gracefully.</li> </ul>"},{"location":"exceptions-and-warnings/#invalidinputerror","title":"InvalidInputError","text":"<p>Chunklet is a stickler for rules, especially when it comes to your input. This error means you've given Chunklet something it just can't work with.</p> <ul> <li>Common Scenarios:<ul> <li>You tried to initialize Chunklet with some funky <code>custom_splitters</code> that didn't quite fit the mold.</li> <li>Your chunking configuration (like <code>max_tokens</code> or <code>mode</code>) was a bit off.</li> <li>In batch processing, you forgot to provide a list of texts, or you asked for a negative number of parallel jobs (we're good, but not that good).</li> </ul> </li> <li>What to do: Double-check your input parameters against the documentation. Make sure everything is in the right format and within the expected ranges.</li> </ul>"},{"location":"exceptions-and-warnings/#tokennotprovidederror","title":"TokenNotProvidedError","text":"<p>This one's pretty self-explanatory, but we'll explain it anyway. If you're trying to chunk by tokens (or in hybrid mode, which also needs token awareness) but haven't told Chunklet how to count tokens, it'll politely (or not so politely) refuse to proceed.</p> <ul> <li>Common Scenario: You set <code>mode=\"token\"</code> or <code>mode=\"hybrid\"</code> but didn't provide a <code>token_counter</code> when initializing Chunklet or in your <code>chunk()</code> call.</li> <li>What to do: Provide a <code>token_counter</code> function. You can use a simple word counter, or for more accuracy, integrate a library like <code>tiktoken</code>. Check the Programmatic Usage documentation for examples.</li> </ul>"},{"location":"exceptions-and-warnings/#warnings-chunklets-friendly-nudges-and-occasional-grumbles","title":"Warnings: Chunklet's Friendly Nudges (and Occasional Grumbles)","text":"<p>Warnings are Chunklet's way of saying, \"Hey, I did what you asked, but you might want to know this...\" They don't stop the process, but they often indicate something you could optimize or be aware of.</p>"},{"location":"exceptions-and-warnings/#the-language-is-set-to-auto-consider-setting-the-lang-parameter-to-a-specific-language-to-improve-performance","title":"\"The language is set to <code>auto</code>. Consider setting the <code>lang</code> parameter to a specific language to improve performance.\"","text":"<ul> <li>What it means: You've let Chunklet guess the language of your text. While it's pretty good at it, explicitly telling it the language (<code>lang='en'</code>, <code>lang='fr'</code>, etc.) can sometimes speed things up and improve accuracy, especially for shorter texts.</li> <li>What to do: If you know the language of your text, set the <code>lang</code> parameter. If not, no worries, Chunklet will do its best!</li> </ul>"},{"location":"exceptions-and-warnings/#low-confidence-in-language-detected-detected-lang_detected-with-confidence-confidence2f","title":"\"Low confidence in language detected. Detected: '{lang_detected}' with confidence {confidence:.2f}.\"","text":"<ul> <li>What it means: Chunklet tried its best to detect the language, but it's not super confident about its guess. This often happens with very short texts or texts that mix multiple languages.</li> <li>What to do: If you know the language, set the <code>lang</code> parameter explicitly. If the text is genuinely ambiguous, just be aware that the sentence splitting might not be perfect.</li> </ul>"},{"location":"exceptions-and-warnings/#language-not-supported-or-detected-with-low-confidence-universal-regex-splitter-was-used","title":"\"Language not supported or detected with low confidence. Universal regex splitter was used.\"","text":"<ul> <li>What it means: Chunklet couldn't find a specialized sentence splitter for your language (or it wasn't confident enough in its detection), so it fell back to its trusty universal regex splitter. This splitter is robust but might not be as linguistically nuanced as the specialized ones.</li> <li>What to do: If you need highly accurate sentence splitting for an unsupported language, consider implementing a Custom Splitter. Otherwise, the universal splitter will still get the job done!</li> </ul>"},{"location":"exceptions-and-warnings/#using-batch-with-file-is-deprecated-cli-warning","title":"\"Using <code>--batch</code> with <code>--file</code> is deprecated.\" (CLI Warning)","text":"<ul> <li>What it means: You're using an older way of batch processing. While it still works, we've introduced a more streamlined approach.</li> <li>What to do: For batch processing multiple files, use <code>--input-dir</code>. If you're processing a single file, just provide it directly without <code>--batch</code>. Check the CLI Usage documentation for the latest methods.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>So you've decided to take the plunge and install Chunklet. Good for you. Here's how you do it.</p>"},{"location":"installation/#the-easy-way","title":"The Easy Way","text":"<p>The easiest way to install Chunklet is with <code>pip</code>:</p> <pre><code># Install and verify version\npip install chunklet-py\nchunklet --version\n</code></pre> <p>That's it. You're done. Go have a cookie.</p>"},{"location":"installation/#the-hard-way","title":"The Hard Way","text":"<p>If you want to be difficult, you can clone the repository and install it from source:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install .\n</code></pre> <p>But why would you want to do that? The easy way is so much easier.</p>"},{"location":"installation/#the-i-want-to-make-chunklet-even-better-way","title":"The \"I want to make Chunklet even better\" Way","text":"<p>So you think you can improve upon perfection? Bold move. Here's how you can get a development environment set up.</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install -e \".[dev]\"\n</code></pre> <p>This will install Chunklet in \"editable\" mode, which means any changes you make to the source code will be immediately available. The <code>[dev]</code> part installs all the extra stuff you'll need to run the tests and build the documentation.</p> <p>Now go forth and code, you magnificent developer. And don't forget to write tests. Kindly Provide Outstanding JavaScript Examples. (I know it's a Python project, but that's the joke).</p>"},{"location":"internal-flow/","title":"Internal Flow","text":"<p>Ever wondered what happens under the hood when you call <code>chunklet.chunk()</code>? Well, wonder no more. Here's a peek into the magical, mystical world of Chunklet's internal flow. It's not as complicated as it looks. Mostly.</p> <p>Here's a high-level overview of Chunklet's internal processing flow.</p> <pre><code>graph TD\n    A[Start: chunk()] --&gt; B{Validate Input};\n    B --&gt; C{Use Cache?};\n    C --&gt;|Yes| D[Check Cache for Result];\n    D --&gt; E{Result in Cache?};\n    E --&gt;|Yes| F[Return Cached Result];\n    E --&gt;|No| G[Call _chunk()];\n    C --&gt;|No| G;\n    G --&gt; H[Split Text into Sentences];\n    H --&gt; I[Group Sentences into Chunks];\n    I --&gt; J[Return Chunks];\n    J --&gt; K[Store Result in Cache];\n    K --&gt; F;\n    F --&gt; Z[End];</code></pre>"},{"location":"internal-flow/#the-gory-details","title":"The Gory Details","text":"<ol> <li>Validate Input: First, we check if you've provided valid parameters. If not, we'll yell at you with a <code>ValidationError</code>.</li> <li>Check Cache: If you've enabled caching (which is on by default, you're welcome), we'll check if we've already chunked this exact text with these exact parameters. If so, we'll just return the cached result. We're efficient like that.</li> <li>Split Text into Sentences: If we don't have a cached result, we'll have to do the work. The first step is to split the text into sentences. We use a combination of language-specific libraries and a universal regex to do this, so we can handle pretty much anything you throw at us.</li> <li> <p>Group Sentences into Chunks: Once we have the sentences, we group them into chunks based on the mode you've selected (<code>sentence</code>, <code>token</code>, or <code>hybrid</code>). This is where the real magic happens.</p> </li> </ol>"},{"location":"internal-flow/#sentence-mode-grouping","title":"Sentence Mode Grouping","text":"<pre><code>graph TD\n    I_S[Group Sentences into Chunks (Sentence Mode)] --&gt; S1[Initialize current_chunk, sentence_count=0];\n    S1 --&gt; S2{Loop through sentences};\n    S2 --&gt; S3{If sentence_count + 1 &gt; max_sentences?};\n    S3 --&gt; |Yes| S4[Add current_chunk to final_chunks];\n    S4 --&gt; S5[Reset current_chunk, sentence_count];\n    S3 --&gt; |No| S6[Add sentence to current_chunk];\n    S6 --&gt; S7[Increment sentence_count];\n    S7 --&gt; S2;\n    S5 --&gt; S2;\n    S2 --&gt; S8[Add any remaining sentences to final_chunks];\n    S8 --&gt; J_S[Return Chunks];</code></pre>"},{"location":"internal-flow/#token-mode-grouping","title":"Token Mode Grouping","text":"<pre><code>graph TD\n    I_T[Group Sentences into Chunks (Token Mode)] --&gt; T1[Initialize current_chunk, token_count=0];\n    T1 --&gt; T2{Loop through sentences};\n    T2 --&gt; T3[Calculate sentence_tokens];\n    T3 --&gt; T4{If token_count + sentence_tokens &gt; max_tokens?};\n    T4 --&gt; |Yes| T5[Add current_chunk to final_chunks];\n    T5 --&gt; T6[Reset current_chunk, token_count];\n    T4 --&gt; |No| T7[Add sentence to current_chunk];\n    T7 --&gt; T8[Increment token_count];\n    T8 --&gt; T2;\n    T6 --&gt; T2;\n    T2 --&gt; T9[Add any remaining sentences to final_chunks];\n    T9 --&gt; J_T[Return Chunks];</code></pre>"},{"location":"internal-flow/#hybrid-mode-grouping","title":"Hybrid Mode Grouping","text":"<p><pre><code>graph TD\n    I_H[Group Sentences into Chunks (Hybrid Mode)] --&gt; H1[Initialize current_chunk, token_count=0, sentence_count=0];\n    H1 --&gt; H2{Loop through sentences};\n    H2 --&gt; H3[Calculate sentence_tokens];\n    H3 --&gt; H4{If token_count + sentence_tokens &gt; max_tokens OR sentence_count + 1 &gt; max_sentences?};\n    H4 --&gt; |Yes| H5{If token_count + sentence_tokens &gt; max_tokens?};\n    H5 --&gt; |Yes| H6[Fit clauses within remaining tokens];\n    H6 --&gt; H7[Add fitted clauses to current_chunk];\n    H7 --&gt; H8[Add current_chunk to final_chunks];\n    H8 --&gt; H9[Prepare overlap clauses];\n    H9 --&gt; H10[Reset current_chunk, token_count, sentence_count with overlap];\n    H5 --&gt; |No| H11[Add current_chunk to final_chunks];\n    H11 --&gt; H9;\n    H4 --&gt; |No| H12[Add sentence to current_chunk];\n    H12 --&gt; H13[Increment token_count, sentence_count];\n    H13 --&gt; H2;\n    H10 --&gt; H2;\n    H2 --&gt; H14[Add any remaining sentences to final_chunks];\n    H14 --&gt; J_H[Return Chunks];</code></pre> 5.  Return Chunks: Finally, we return the chunks to you. If caching is enabled, we'll also store the result in the cache for next time.</p>"},{"location":"models/","title":"Models (Don't Worry, You Won't Be Tested)","text":"<p>Ever wondered what's going on under the hood when you're configuring <code>chunklet</code>? It's all handled by these nifty Pydantic models. They're like the diligent, behind-the-scenes roadies of our rockstar chunking library, making sure everything is set up correctly and safely. You don't need to interact with them directly, but for the curious minds, here's a peek behind the curtain.</p>"},{"location":"models/#chunkletinitconfig","title":"ChunkletInitConfig","text":"<p>This is the blueprint for creating a <code>Chunklet</code> instance. Think of it as the soundcheck before the big show.</p> <p>Settings:</p> <ul> <li><code>verbose</code> (bool): Want to see every little detail of what <code>chunklet</code> is doing? Set this to <code>True</code>. Defaults to <code>False</code>.</li> <li><code>use_cache</code> (bool): If you're chunking the same text over and over, this will save you time by caching the results. It's like having a photographic memory for chunking. Defaults to <code>True</code>.</li> <li><code>token_counter</code> (Optional[Callable[[str], int]]): Got your own way of counting tokens? Plug it in here. This is a must-have if you're using <code>token</code> or <code>hybrid</code> mode. Defaults to <code>None</code>.</li> <li><code>custom_splitters</code> (Optional[CustomSplitterConfig]): If you have a special way of splitting sentences, you can add your own custom splitters here. More on this below. Defaults to <code>None</code>.</li> </ul>"},{"location":"models/#customsplitterconfig","title":"CustomSplitterConfig","text":"<p>This is for when you want to bring your own sentence-splitting party to <code>chunklet</code>. <code>CustomSplitterConfig</code> is just a list of <code>CustomSplitter</code> objects.</p> <p><code>CustomSplitter</code> Settings:</p> <ul> <li><code>name</code> (str): Give your splitter a cool name, like \"The Sentence Slicer 3000\".</li> <li><code>languages</code> (Union[str, Iterable[str]]): Tell <code>chunklet</code> which language or languages your splitter works with (e.g., \"en\" or [\"fr\", \"es\"]).</li> <li><code>callback</code> (Callable[[str], List[str]]): This is the actual function that does the splitting. It takes a string and returns a list of sentences.</li> </ul>"},{"location":"models/#chunkingconfig","title":"ChunkingConfig","text":"<p>This model is the director of a single chunking operation. It's created internally every time you call <code>.chunk()</code> or <code>.batch_chunk()</code>, so you don't need to worry about it. It's just here to make sure everything goes smoothly.</p> <p>Settings:</p> <ul> <li><code>text</code> (str): The text you want to chunk. The star of the show!</li> <li><code>lang</code> (str): The language of the text. If you're not sure, just leave it as <code>\"auto\"</code>.</li> <li><code>mode</code> (str): The chunking strategy. Choose from <code>\"sentence\"</code>, <code>\"token\"</code>, or <code>\"hybrid\"</code>. Defaults to <code>\"sentence\"</code>.</li> <li><code>max_tokens</code> (int): The maximum number of tokens per chunk. Only for <code>token</code> and <code>hybrid</code> modes.</li> <li><code>max_sentences</code> (int): The maximum number of sentences per chunk. Only for <code>sentence</code> and <code>hybrid</code> modes.</li> <li><code>overlap_percent</code> (Union[int, float]): The percentage of overlap between chunks. A little overlap can help maintain context. Must be between 0 and 85. Defaults to <code>20</code>.</li> <li><code>offset</code> (int): Want to skip the first few sentences? This is the setting for you. Defaults to <code>0</code>.</li> <li><code>token_counter</code> (Optional[Callable[[str], int]]): You can provide a token counter here to override the one in the <code>Chunklet</code> instance.</li> <li><code>verbose</code> (bool): Want to get chatty for just one chunking operation? Set this to <code>True</code>.</li> <li><code>use_cache</code> (bool): You can override the instance's cache setting for a single operation.</li> </ul>"},{"location":"supported-languages/","title":"Supported Languages","text":"<p>Chunklet leverages powerful third-party libraries and flexible customization options to provide robust multilingual support. While we list the languages explicitly supported by our integrated libraries below, for the most comprehensive and up-to-date information, we highly recommend checking their official documentation.</p> <p>If your language isn't explicitly listed, or if you require specialized handling, you have two powerful options:</p> <ol> <li>Custom Splitters: Integrate your own sentence splitting logic. This allows you to define precise rules for specific languages or domains. Learn more about Custom Splitters in our Models documentation. For exemple usage check.getting-started/programmatic.md#custom-sentence-splitter</li> <li>Universal Regex Fallback: Chunklet will automatically fall back to a smart regex-based splitter, ensuring text can always be segmented, even without specific language support.</li> </ol>"},{"location":"supported-languages/#languages-supported-by-pysbd","title":"Languages supported by <code>pysbd</code>","text":"<p><code>pysbd</code> (Python Sentence Boundary Disambiguation) offers highly accurate sentence boundary detection for a wide array of languages. For the complete and most current list, please refer to the official pysbd documentation.</p> Language Code Language Name en English mr Marathi hi Hindi bg Bulgarian es Spanish ru Russian ar Arabic am Amharic hy Armenian fa Persian (Farsi) ur Urdu pl Polish zh Chinese (Mandarin) nl Dutch da Danish fr French it Italian el Greek my Burmese (Myanmar) ja Japanese de German kk Kazakh"},{"location":"supported-languages/#languages-unique-to-sentence-splitter-not-supported-by-pysbd","title":"Languages Unique to <code>sentence-splitter</code> (NOT Supported by <code>pysbd</code>)","text":"<p><code>sentence-splitter</code> provides support for additional languages that are not covered by <code>pysbd</code>, thus complementing its coverage. For the complete and most current list, please refer to the official sentence-splitter documentation.</p> Language Code Language Name ca Catalan cs Czech fi Finnish hu Hungarian is Icelandic lv Latvian lt Lithuanian no Norwegian pt Portuguese ro Romanian sk Slovak sl Slovenian sv Swedish tr Turkish"},{"location":"supported-languages/#fallback-universal-regex-splitter","title":"Fallback: Universal Regex Splitter","text":"<p>For any language not explicitly supported by <code>pysbd</code> or <code>sentence-splitter</code>, Chunklet employs a robust, smart regex-based splitter. This ensures that text can always be segmented, even if language-specific rules are not available. You can find more details about this in the Utility Functions documentation.</p>"},{"location":"utils/","title":"Utils","text":"<p>These are the unsung heroes of Chunklet, the utility functions that work behind the scenes to make everything tick. You might not interact with them directly often, but they're crucial for Chunklet's multilingual and robust operation.</p>"},{"location":"utils/#detect_text_language","title":"<code>detect_text_language</code>","text":"<p>This function is Chunklet's linguistic detective. It automatically identifies the language of a given text, which is vital for applying the correct sentence splitting rules. It's smart enough to give you a confidence score, so you know how sure it is.</p> <p>Source Code: src/chunklet/utils/detect_text_language.py</p> <p>Function Signature:</p> <pre><code>def detect_text_language(text: str) -&gt; Tuple[str, float]:\n</code></pre> <p>Description:</p> <p>This function uses the <code>py3langid</code> library to classify the input text and determine its language. It returns a tuple containing the detected language code (e.g., \"en\", \"fr\") and a normalized probability score representing the confidence of the detection.</p> <p>Key Features:</p> <ul> <li>Numerical Stability: Uses the log-sum-exp trick to ensure stable calculations.</li> <li>Focused Ranking: Considers the top 10 language candidates for improved accuracy.</li> </ul>"},{"location":"utils/#universalsplitter","title":"<code>UniversalSplitter</code>","text":"<p>When all else fails, the <code>UniversalSplitter</code> steps in. This is Chunklet's reliable fallback for languages not explicitly supported by <code>pysbd</code> or <code>sentence-splitter</code>, or when language detection is uncertain. It uses a smart regex-based approach to split text into sentences, ensuring that even the most obscure texts get chunked.</p> <p>Source Code: src/chunklet/utils/universal_splitter.py</p> <p>This class provides a rule-based sentence splitter that works as a fallback when language-specific splitters are not available.</p> <p>Class: <code>UniversalSplitter</code></p> <p>Description:</p> <p>The <code>UniversalSplitter</code> uses a series of regular expressions to split text into sentences. It is designed to handle various edge cases, such as abbreviations, acronyms, numbered lists, and different types of punctuation.</p> <p>Key Features:</p> <ul> <li>Robust Splitting: Handles complex sentence boundaries and avoids common errors like splitting on decimals or after abbreviations.</li> <li>Multi-stage Processing:<ol> <li>An initial split is performed based on sentence-ending punctuation.</li> <li>A post-processing stage merges sentences that were incorrectly split (e.g., after an abbreviation).</li> <li>A final cleanup stage normalizes whitespace and removes empty sentences.</li> </ol> </li> </ul>"},{"location":"getting-started/","title":"Getting Started: Your Journey into the World of Chunklet","text":"<p>Welcome, brave adventurer, to the Chunklet starter pack! You've taken the first step towards taming unruly texts and transforming them into bite-sized, manageable chunks. No more wrestling with massive strings of words \u2013 Chunklet is here to make your life (and your LLMs' lives) a whole lot easier.</p> <p>As you already known (or guessed), it's key Features of Chunklet:</p> <ul> <li>Intelligent Text Segmentation: Breaks down text into semantically meaningful chunks.</li> <li>Multilingual Support: Handles various languages with smart fallbacks.</li> <li>Flexible Chunking Modes: Offers sentence, token, and hybrid chunking to suit diverse needs.</li> <li>Context Preservation: Uses advanced techniques like clause-level overlapping to maintain meaning across chunks.</li> </ul>"},{"location":"getting-started/#choose-your-own-adventure-cli-or-code","title":"Choose Your Own Adventure: CLI or Code?","text":"<p>Chunklet offers two main paths to text-chunking glory. Pick the one that suits your style:</p> <ul> <li> <p>Command Line Interface (CLI): For the terminal aficionados and those who prefer quick, direct action. If you like typing commands and seeing immediate results, this is your jam.</p> <ul> <li>Dive into CLI Usage</li> </ul> </li> <li> <p>Programmatic Usage: For the developers, the scripters, and those who want to integrate Chunklet's power directly into their Python applications. If you prefer writing code and building custom workflows, this path is for you.</p> <ul> <li>Explore Programmatic Usage</li> </ul> </li> </ul> <p>No matter which path you choose, Chunklet is designed to make text chunking as painless (and perhaps, as entertaining) as possible. Let's get chunking!</p>"},{"location":"getting-started/cli/","title":"CLI Usage: Your Command-Line Companion","text":"<p>So, you're ready to get your hands dirty with some command-line action? Excellent! <code>chunklet-py</code> isn't just a fancy library; it's also a handy CLI tool. A quick <code>pip install chunklet-py</code> and you'll have <code>chunklet</code> at your beck and call, ready to slice and dice your text.</p> <p>Run chunklet --help for full options.</p>"},{"location":"getting-started/cli/#basic-chunking-getting-started-with-the-essentials","title":"Basic Chunking: Getting Started with the Essentials","text":"<p>Specifying Chunking Mode and Parameters: Because One Size Rarely Fits All</p> <p>You're about to discover the various ways <code>chunklet</code> can dissect your text. We'll cover the core arguments that let you control how your text gets chopped up, so you can get exactly what you need (or at least, something close).</p> <p>Note: The following examples showcase individual flags for clarity. However, most flags can be combined to create more complex and specific chunking behaviors.</p> Flag Alias Description Default <code>text</code> The input text to chunk. <code>--file</code> <code>--input-file</code> Path to a text file to read input from. <code>--output-file</code> Path to a file to write the output chunks to. <code>--input-dir</code> Path to a directory to read input files from. <code>--output-dir</code> Path to a directory to write the output chunks to. <code>--mode</code> Chunking mode: 'sentence', 'token', or 'hybrid'. <code>sentence</code> <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). <code>auto</code> <code>--max-tokens</code> Maximum number of tokens per chunk. <code>512</code> <code>--max-sentences</code> Maximum number of sentences per chunk. <code>100</code> <code>--overlap-percent</code> Percentage of overlap between chunks (0-85). <code>10</code> <code>--offset</code> Starting sentence offset for chunking. <code>0</code> <code>-v</code> <code>--verbose</code> Enable verbose logging. <code>--no-cache</code> Disable LRU caching. <code>--batch</code> Process input as a list of texts for batch chunking. <code>--n-jobs</code> Number of parallel jobs for batch chunking. <code>None</code> <code>--tokenizer-command</code> A shell command to use for token counting. <code>--version</code> Show program's version number and exit. <p>\u26a0\ufe0f Note: Language will be detected automatically if not set explicitly, but you'll see this warning in output. <pre><code>2025-08-30 22:09:09.355 | WARNING | chunklet.core:chunk:481 -\nFound 1 unique warning(s) during chunking:\n- The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n</code></pre></p>"},{"location":"getting-started/cli/#sentence-mode-keeping-your-thoughts-intact","title":"Sentence Mode: Keeping Your Thoughts Intact","text":"<p>This mode is for those who appreciate the elegance of a complete sentence. <code>chunklet</code> will diligently break down your text based on sentence boundaries, ensuring each chunk is a coherent thought. It's perfect for when you want to maintain the natural flow of your content, just in smaller, more manageable pieces.</p> <pre><code>chunklet \"She loves cooking. He studies AI. The weather is great.\" \\\n    --max-sentences 2 \\\n    --lang \"en\"\n</code></pre> Output <pre><code>--- Chunk 1 ---\nShe loves cooking. He studies AI.\n\n--- Chunk 2 ---\nThe weather is great.\n</code></pre>"},{"location":"getting-started/cli/#token-mode-when-every-character-counts-almost","title":"Token Mode: When Every Character Counts (Almost)","text":"<p>For the detail-oriented among us, Token Mode focuses on the raw building blocks of your text. Whether you're counting words, characters, or something more exotic, this mode lets you define chunk sizes by the number of tokens. You can even hook up your own custom tokenizer if you're feeling adventurous \u2013 because who are we to tell you how to count your tokens?</p> <pre><code># Example using 'wc -w' as a simple word counter (approximation of tokens)\nchunklet \"Hello world! You see that? This is a sample text for token counting.\" \\\n    --mode token \\\n    --max-tokens 10 \\\n    --tokenizer-command \"wc -w\"\n</code></pre> Output <pre><code>2025-08-30 22:09:09.355 | WARNING | chunklet.core:chunk:481 -\nFound 1 unique warning(s) during chunking:\n- The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n--- Chunk 1 ---\nHello world! You see that?\n\n--- Chunk 2 ---\nThis is a sample text for token counting.\n</code></pre> <p>For more accurate tokenization that matches OpenAI's models, you can use the <code>tiktoken</code> library.</p> <ol> <li>Install <code>tiktoken</code>:</li> </ol> <pre><code>pip install tiktoken\n</code></pre> <ol> <li>Create a tokenizer script (<code>my_tokenizer.py</code>):</li> </ol> <pre><code># my_tokenizer.py\nimport tiktoken\nimport sys\n\n# Note: tiktoken will download the selected encoding models on first usage.\ndef count_tokens(text):\n    # Using cl100k_base encoding, suitable for gpt-3.5-turbo and gpt-4\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))\n\nif __name__ == \"__main__\":\n    input_text = sys.stdin.read()\n    token_count = count_tokens(input_text)\n    print(token_count)\n</code></pre> <ol> <li>Use the script with <code>chunklet</code>:</li> </ol> <pre><code>chunklet \"Your long text here...\" \\\n --mode token \\\n --max-tokens 100 \\\n --tokenizer-command \"python my_tokenizer.py\"\n</code></pre>"},{"location":"getting-started/cli/#hybrid-mode-the-smart-compromise","title":"Hybrid Mode: The Smart Compromise","text":"<p>Hybrid Mode is <code>chunklet</code>'s attempt at being the smartest kid in the chunking class. It starts by trying to split your text into sentences using its advanced, language-aware splitters. If your language isn't explicitly supported, or if your text is a bit... unconventional, it gracefully falls back to a robust, universal regex-based sentence splitter. So, you'll always get something that looks like sentences.</p> <p>Once it has these sentences, Hybrid Mode then groups them into chunks, but with a keen eye on both your <code>max-sentences</code> and <code>max-tokens</code> limits. It tries its best to keep sentences whole, but if adding a full sentence would push a chunk over the <code>max-tokens</code> limit, it gets clever. It'll then try to fit just parts of that sentence (clauses) into the remaining space, ensuring your chunks stay within budget while still being as semantically complete as possible. It's the perfect choice when you want readable chunks but also need to strictly control their size, especially useful for LLM contexts where token limits are king.</p> <pre><code>chunklet \"Bonjou tout moun! Byenvini anko. Mesye dam, Kitem prezante nou 'hybrid mode'. Li vreman bon.\" \\\n     --max-sentences 3 \\\n     --mode hybrid \\\n     --max-tokens 10 \\\n     --tokenizer-command \"wc -w\" \\\n     --overlap-percent 30 \\\n     --lang \"ht\"\n</code></pre> Output <pre><code>2025-08-30 22:52:23.951 | WARNING | chunklet.core:chunk:481 -\nFound 1 unique warning(s) during chunking:\n- Language not supported or detected with low confidence. Universal regex splitter was used.\n--- Chunk 1 ---\nBonjou tout moun! Byenvini anko. Mesye dam,\n\n--- Chunk 2 ---\nMesye dam, Kitem prezante nou 'hybrid mode'. Li vreman bon.\n</code></pre>"},{"location":"getting-started/cli/#advanced-usage-beyond-the-command-line-files-and-directories","title":"Advanced Usage: Beyond the Command Line (Files and Directories)","text":"<p>For the following examples, let's assume we have a directory named <code>my_documents</code> with the following two files:</p> <p>my_documents/story.txt <pre><code>In a quiet village nestled between rolling hills and a whispering forest, lived a clockmaker named Elias. His hands, though old and worn, moved with a grace that defied his age. He didn't just make clocks; he crafted time itself, each tick a heartbeat, each tock a breath. One day, a mysterious traveler arrived, carrying a pocket watch that didn't tell time, but rather, seemed to hold it. The watch, the traveler claimed, could show glimpses of what was, what is, and what could be. Intrigued, Elias traded his finest creation for the enigmatic timepiece, a decision that would unravel the very fabric of his reality.\n</code></pre></p> <p>my_documents/poem.md <pre><code>The wind, a restless poet, writes verses on the leaves,\nOf summer's fleeting sonnet, and autumn's golden eves.\nIt sings a mournful ballad, a lament for the fallen snow,\nAnd whispers tales of springtime, in the gentle seeds that grow.\n</code></pre></p>"},{"location":"getting-started/cli/#chunking-from-a-file-when-your-text-lives-in-a-document","title":"Chunking from a File: When Your Text Lives in a Document","text":"<p>Got a text file you need to chunk? <code>chunklet</code> can handle that. Just point it to your file, and it'll process the content. You can even tell it to save the output to another file, keeping your console tidy. Remember, <code>--file</code> and <code>--input-file</code> are just two ways to say the same thing \u2013 we like options!</p> <p>\u26a0\ufe0f Deprecation Notice: While you can use <code>--file</code> (or <code>--input-file</code>) with <code>--batch</code> for processing multiple lines from a single file, this approach is deprecated and will be removed in future releases. Please use <code>--input-dir</code> for batch processing multiple files instead. For more details on this and other friendly nudges from Chunklet, check out the Exceptions and Warnings documentation.</p> <pre><code># Chunk from my_documents/story.txt and print to console\nchunklet --file my_documents/story.txt \\\n    --mode sentence \\\n    --max-sentences 2\n</code></pre> Output <pre><code>--- Chunk 1 ---\nIn a quiet village nestled between rolling hills and a whispering forest, lived a clockmaker named Elias. His hands, though old and worn, moved with a grace that defied his age.\n\n--- Chunk 2 ---\nHe didn't just make clocks; he crafted time itself, each tick a heartbeat, each tock a breath. One day, a mysterious traveler arrived, carrying a pocket watch that didn't tell time, but rather, seemed to hold it.\n\n--- Chunk 3 ---\n... seemed to hold it. The watch, the traveler claimed, could show glimpses of what was, what is, and what could be.\n\n--- Chunk 4 ---\n... and what could be. Intrigued, Elias traded his finest creation for the enigmatic timepiece, a decision that would unravel the very fabric of his reality.\n\n2025-08-30 21:39:55.197 | WARNING | chunklet.core:chunk:481 -\nFound 1 unique warning(s) during chunking:\n- The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n</code></pre> <pre><code># Chunk from my_documents/story.txt and save to output.txt\nchunklet --input-file my_documents/story.txt \\\n    --output-file output.txt \\\n    --mode sentence \\\n    --max-sentences 2\n</code></pre> <p>This command will create a file named <code>output.txt</code> containing the output.</p>"},{"location":"getting-started/cli/#chunking-from-a-directory-for-when-you-have-a-whole-folder-of-fun","title":"Chunking from a Directory: For When You Have a Whole Folder of Fun","text":"<p>Why chunk one file when you can chunk them all? If your texts are neatly organized in a directory, <code>chunklet</code> can sweep through it, processing all <code>.txt</code> and <code>.md</code> files (and it's smart enough to find them even in subfolders!). It's batch processing, <code>chunklet</code> style.</p> <pre><code># Process all text files in 'my_documents/' and print chunks to console\nchunklet --input-dir my_documents/ \\\n    --mode token \\\n    --max-tokens 10 \\\n    --tokenizer-command \"wc -w\"\n</code></pre> Output <pre><code>## Source: my_documents/story.txt\n\n--- Chunk 1 ---\nIn a quiet village nestled between rolling hills and a whispering forest, lived a clockmaker named Elias.\n\n--- Chunk 2 ---\nHis hands, though old and worn, moved with a grace that defied his age.\n\n--- Chunk 3 ---\nHe didn't just make clocks; he crafted time itself, each tick a heartbeat, each tock a breath.\n\n--- Chunk 4 ---\nOne day, a mysterious traveler arrived, carrying a pocket watch that didn't tell time, but rather, seemed to hold it.\n\n--- Chunk 5 ---\nThe watch, the traveler claimed, could show glimpses of what was, what is, and what could be.\n\n--- Chunk 6 ---\nIntrigued, Elias traded his finest creation for the enigmatic timepiece, a decision that would unravel the very fabric of his reality.\n\n## Source: my_documents/poem.md\n\n--- Chunk 1 ---\nThe wind, a restless poet, writes verses on the leaves,\n\n--- Chunk 2 ---\nOf summer's fleeting sonnet, and autumn's golden eves.\n\n--- Chunk 3 ---\nIt sings a mournful ballad, a lament for the fallen snow,\n\n--- Chunk 4 ---\nAnd whispers tales of springtime, in the gentle seeds that grow.\n\n2025-08-30 21:54:37.662 | WARNING | chunklet.core:batch_chunk:596 -\nFound 1 unique warning(s) during batch processing of 2 texts:\n- (2/2) The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n</code></pre>"},{"location":"getting-started/cli/#saving-chunks-to-a-directory-because-sometimes-you-need-more-than-just-console-output","title":"Saving Chunks to a Directory: Because Sometimes You Need More Than Just Console Output","text":"<p>Printing chunks to your terminal is cool and all, but what if you need those chunks as actual files? <code>chunklet</code> understands. Just tell it an output directory, and it will dutifully save each generated chunk as its own separate file. Perfect for when you're building something bigger and need those individual pieces.</p> <pre><code># Process 'input.txt' and save each chunk as a separate file in 'output_chunks/'\nchunklet --file input.txt \\\n    --output-dir output_chunks/ \\\n    --mode token \\\n    --max-tokens 50\n# Example output files: output_chunks/input_chunk_1.txt, output_chunks/input_chunk_2.txt\n</code></pre> <p>You should see a message like this: <pre><code>Successfully processed 1 file(s) and wrote 6 chunk file(s) to output_chunks/\n</code></pre></p> <p>This will create the directory with the following files:</p> <pre><code>output_chunks/\n\u251c\u2500\u2500 story_chunk_1.txt\n\u251c\u2500\u2500 story_chunk_2.txt\n\u251c\u2500\u2500 story_chunk_3.txt\n\u251c\u2500\u2500 story_chunk_4.txt\n\u251c\u2500\u2500 story_chunk_5.txt\n\u2514\u2500\u2500 story_chunk_6.txt\n</code></pre>"},{"location":"getting-started/cli/#combined-directory-input-and-output-the-full-automation-experience","title":"Combined Directory Input and Output: The Full Automation Experience","text":"<p>For the true power users (or just the really lazy ones), <code>chunklet</code> offers the ultimate convenience: process an entire input directory and save all the resulting chunks into a separate output directory. It's like setting up your own personal chunking factory \u2013 just point, click (well, type), and let <code>chunklet</code> handle the rest. Your hands will thank you.</p> <pre><code># Process all files in 'my_documents/' and save individual chunks to 'processed_chunks/'\nchunklet --input-dir my_documents/ \\\n    --output-dir processed_chunks/ \\\n    --mode hybrid \\\n    --max-sentences 3 \\\n    --max-tokens 100 \\\n    --tokenizer-command \"wc -w\"\n</code></pre> <p>You should see a message like this: <pre><code>Successfully processed 2 file(s) and wrote 5 chunk file(s) to processed_chunks/\n</code></pre></p> <p>This will create the directory with the following files:</p> <pre><code>processed_chunks/\n\u251c\u2500\u2500 poem_chunk_1.txt\n\u251c\u2500\u2500 poem_chunk_2.txt\n\u251c\u2500\u2500 story_chunk_1.txt\n\u251c\u2500\u2500 story_chunk_2.txt\n\u2514\u2500\u2500 story_chunk_3.txt\n</code></pre>"},{"location":"getting-started/cli/#speeding-up-batch-processing-with-n-jobs","title":"Speeding Up Batch Processing with <code>--n-jobs</code>","text":"<p>When you're processing a large number of files in a directory, you can speed things up by using the <code>--n-jobs</code> argument. This allows <code>chunklet</code> to process multiple files in parallel, taking advantage of multiple CPU cores.</p> <pre><code># Process all files in 'my_documents/' using 4 parallel jobs\nchunklet --input-dir my_documents/ \\\n    --output-dir processed_chunks/ \\\n    --n-jobs 4\n</code></pre> <p>By default, <code>chunklet</code> will use all available CPU cores. You can specify a number to limit the number of parallel jobs.</p> <p> \u26a0\ufe0f Note: Language will be detected automatically if not set explicitly, but you'll see this warning in output. </p> <pre>\n2025-08-30 22:09:09.355 | WARNING | chunklet.core:chunk:481 -\nFound 1 unique warning(s) during chunking:\n- The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n</pre>"},{"location":"getting-started/cli/#best-practices-for-cli-usage","title":"Best Practices for CLI Usage","text":"<p>To get the most out of the <code>chunklet</code> CLI, consider these best practices:</p> <ul> <li> <p>Explicit Language Setting: While <code>chunklet</code>'s <code>--lang auto</code> detection is robust, explicitly setting the <code>--lang</code> parameter when you know the language of your text can significantly improve performance and accuracy, especially for shorter texts or less common languages.</p> <p>Tip: For consistent results and to avoid language detection warnings, always specify <code>--lang</code> if your text's language is known.</p> </li> <li> <p>Leverage Batch Processing for Multiple Files: For processing multiple documents, always prefer <code>--input-dir</code> over iterating and calling <code>chunklet</code> individually for each file. <code>--input-dir</code> is optimized for parallel processing and will significantly speed up your workflow, especially when combined with <code>--n-jobs</code>.</p> <p>Tip: Use <code>--n-jobs</code> with <code>--input-dir</code> to utilize multiple CPU cores for even faster batch processing.</p> </li> <li> <p>Understand Output Options: Decide whether you need console output, a single output file (<code>--output-file</code>), or individual chunk files in an output directory (<code>--output-dir</code>). Choose the option that best suits your downstream processing needs.</p> </li> <li> <p>Monitor Warnings: Pay attention to the warnings <code>chunklet</code> emits. They often provide valuable insights into potential optimizations (e.g., language detection confidence) or inform you about fallback mechanisms being used.</p> </li> <li> <p>Use a Custom Tokenizer for LLM Alignment: If you're preparing text for a specific Large Language Model (LLM), integrate its tokenizer using <code>--tokenizer-command</code>. This ensures your chunks align perfectly with the LLM's tokenization strategy, preventing truncation issues and optimizing token usage.</p> </li> </ul>"},{"location":"getting-started/programmatic/","title":"Programmatic Usage","text":"<p>This section delves into the various ways you can wield Chunklet programmatically. Prepare to be amazed (or at least mildly impressed).</p>"},{"location":"getting-started/programmatic/#preview-sentences","title":"Preview Sentences","text":"<p>You can use the <code>preview_sentences</code> method to quickly split a text into sentences without performing any chunking. This is useful for inspecting the sentence boundaries detected by Chunklet.</p> <p>Parameters: - text (str): input document. - lang (str): ISO language code.</p> <p>Returns: - Tuple[List[str], List[str]]:   - first element: list of sentences.   - second element: list of warning messages.</p> <p>Raises: - InvalidInputError : if text is invalid.</p> <pre><code>from chunklet import Chunklet\n\n# Sample text\ntext = \"\"\"\nThis is a sample text to preview sentences. It has multiple sentences.\n\"You are a Dr.\", she said. The weather is great. We play chess.\n\"\"\"\n\nchunker = Chunklet()\n\n# Preview the sentences in the text\nsentences, warnings = chunker.preview_sentences(text, lang=\"en\")\n\nprint(\"---\" + \"-\" * 10 + \" Sentences Preview \" + \"-\" * 10 + \"---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings:\n    print(\"\\n---\" + \"-\" * 10 + \" Warnings \" + \"-\" * 10 + \"---\")\n    for warning in warnings:\n        print(warning)\n</code></pre> Output <pre><code>---          Sentences Preview          ---\nSentence 1: This is a sample text to preview sentences.\nSentence 2: It has multiple sentences.\nSentence 3: \"You are a Dr.\", she said.\nSentence 4: The weather is great.\nSentence 5: We play chess.\n</code></pre>"},{"location":"getting-started/programmatic/#chunk-api","title":"Chunk api","text":"<p>Parameters: - <code>text</code> (str): The input text to chunk. - <code>lang</code> (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\". - <code>mode</code> (str): Chunking mode ('sentence', 'token', or 'hybrid'). Defaults to \"sentence\". - <code>max_tokens</code> (int): Maximum number of tokens per chunk. Defaults to 512. - <code>max_sentences</code> (int): Maximum number of sentences per chunk. Defaults to 100. - <code>overlap_percent</code> (Union[int, float]): Percentage of overlap between chunks (0-85). Defaults to 20. - <code>offset</code> (int): Starting sentence offset for chunking. Defaults to 0. - <code>token_counter</code> (Optional[Callable[[str], int]]): A function to count tokens in a string.</p> <p>Returns: - <code>List[str]</code>: A list of text chunks.</p> <p>Raises: - <code>InvalidInputError</code>: If any chunking configuration parameter is invalid. - <code>TokenNotProvidedError</code>: If <code>mode</code> is \"token\" or \"hybrid\" but no <code>token_counter</code> is provided. - <code>ChunkletError</code>: If the provided <code>token_counter</code> callable raises an exception during token counting.</p>"},{"location":"getting-started/programmatic/#sentence-mode","title":"Sentence mode","text":"<p>Note: The timestamps and logging messages you see in the following examples are because <code>verbose=True</code> is set when initializing <code>Chunklet</code>. This is not part of the actual return value of the functions.</p> <p>In this example, we'll take a small text and split it into chunks, with each chunk containing a maximum of two sentences. This is a common use case when you want to process a text sentence by sentence, but still want to group a few sentences together for context.</p> <ul> <li>Parameters per call: max_sentences, overlap_sentences.</li> </ul> <pre><code>from chunklet import Chunklet\n\n# Sample text\ntext = \"\"\"\nThis is a sample text for sentence mode chunking. It has multiple sentences. Each sentence will be considered as a unit. The chunker will group sentences based on the maximum number of sentences per chunk. This mode is useful when you want to preserve the integrity of sentences within your chunks.\n\"\"\"\n\nchunker = Chunklet(verbose=True)  # Set verbose to true to see logging, `False` by default.\n\n# Chunk the text by sentences, with a maximum of 2 sentences per chunk\nchunks = chunker.chunk(\n    text=text,\n    mode=\"sentence\",         # Chunking mode. Default is 'sentence'\n    lang=\"auto\",             # Language of the text. Default is 'auto'\n    max_sentences=2,         # Max sentences per chunk. Default is 100\n    overlap_percent=0,       # Overlap between chunks. Default is 20\n    offset=0                 # Start offset. Default is 0\n)\n\nprint(\"---\" + \" Sentence Mode Chunks \" + \"---\")\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}: {chunk}\")\n</code></pre> Output <pre><code>2025-08-30 16:22:56.205 | DEBUG    | chunklet.core:__init__:116 - Chunklet initialized with verbose=True, use_cache=True. Default token counter is not provided.\n2025-08-30 16:22:56.208 | INFO     | chunklet.core:chunk:436 - Processing text - single run\n2025-08-30 16:22:56.370 | DEBUG    | chunklet.core:_split_by_sentence:157 - Attempting language detection.\n2025-08-30 16:22:56.371 | DEBUG    | chunklet.core:_split_by_sentence:173 - Using pysbd for language: en.\n2025-08-30 16:22:56.400 | DEBUG    | chunklet.core:_chunk:364 - Text splitted into sentences. Total sentences detected: 5\n2025-08-30 16:22:56.401 | INFO     | chunklet.core:_chunk:385 - Finished chunking text. Generated 3 chunks.\n\n2025-08-30 16:22:56.401 | WARNING  | chunklet.core:chunk:481 - \nFound 1 unique warning(s) during chunking:\n  - The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n--- Sentence Mode Chunks ---\nChunk 1: This is a sample text for sentence mode chunking. It has multiple sentences.\nChunk 2: Each sentence will be considered as a unit. The chunker will group sentences based on the maximum number of sentences per chunk.\nChunk 3: This mode is useful when you want to preserve the integrity of sentences within your chunks.\n</code></pre>"},{"location":"getting-started/programmatic/#token-mode","title":"Token Mode","text":"<p>This example shows how to use a custom function to count tokens, which is essential for token-based chunking. We will set <code>lang</code> to a specific language instead of auto. This will remove the warning we saw earlier.</p> <ul> <li>Parameters: max_tokens, overlap_tokens.</li> </ul> <pre><code>import regex as re\nfrom chunklet import Chunklet\n\n# Sample text\ntext = \"\"\"\nShe loves cooking. He studies AI. \"You are a Dr.\", she said. The weather is great. We play chess. Books are fun, aren't they?\n\nThe Playlist contains:\n  - two videos\n  - one image\n  - one music\n\nRobots are learning. It's raining. Let's code. Mars is red. Sr. sleep is rare. Consider item 1. This is a test. The year is 2025. This is a good year since N.A.S.A. reached 123.4 light year more.\n\"\"\"\n\n# Define a custom token counter for demonstration purpose.\ndef simple_token_counter(text: str) -&gt; int:\n    return len(re.findall(r\"\\b\\p{L}+\\b\", text))\n\n# Initialize Chunklet with the custom counter (this will be the default for the instance)\nchunker = Chunklet(token_counter=simple_token_counter)\n\n# Chunk by tokens, using the token_counter set during Chunklet initialization\nchunks_default = chunker.chunk(\n    text=text,\n    mode=\"token\",\n    lang=\"en\",\n    max_tokens=10\n)\nfor i, chunk in enumerate(chunks_default):\n    print(f\"Chunk {i+1}: {chunk}\")\n\n\n# You can override the token_counter for a specific call too\n\n# chunks_override = chunker.chunk(text, mode=\"token\", max_tokens=10, token_counter=another_token_counter)\n# for i, chunk in enumerate(chunks_override):\n#     print(f\"Chunk {i+1}: {chunk}\")\n</code></pre> Output <pre><code>Chunk 1: She loves cooking. He studies AI. \"You are a Dr.\",\nChunk 2: \"You are a Dr.\", she said. The weather is great.\nChunk 3: The weather is great. We play chess. Books are fun,\nChunk 4: Books are fun, aren't they? The Playlist contains:\nChunk 5: The Playlist contains: - two videos - one image - one music\nChunk 6: ... - one music Robots are learning. It's raining.\nChunk 7: It's raining. Let's code. Mars is red.\nChunk 8: Mars is red. Sr. sleep is rare. Consider item 1.\nChunk 9: Consider item 1. This is a test. The year is 2025.\nChunk 10: The year is 2025. This is a good year since N.A.S.A. reached 123.4 light year more.\n</code></pre>"},{"location":"getting-started/programmatic/#hybrid-mode","title":"Hybrid Mode","text":"<p>Combine sentence and token limits with overlap to maintain context between chunks.</p> <pre><code>import regex as re\nfrom chunklet import Chunklet\n\n# Define a custom token counter for demonstration purpose.\ndef simple_token_counter(text: str) -&gt; int:\n    return len(re.findall(r\"\\b\\p{L}+\\b\", text))\n\ntext = \"\"\"\nThis is a long text to demonstrate hybrid chunking. It combines both sentence and token limits for flexible chunking. Overlap helps maintain context between chunks by repeating some clauses. This mode is very powerful for maintaining semantic coherence. It is ideal for applications like RAG pipelines where context is crucial.\n\"\"\"\n\nchunker = Chunklet(token_counter=simple_token_counter)\n\n# Chunk with both sentence and token limits, and 20% overlap\nchunks = chunker.chunk(\n    text=text,\n    mode=\"hybrid\",\n    max_sentences=2,\n    max_tokens=15\n)\n\nprint(\"--- Hybrid Mode Chunks ---\")\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}: {chunk}\")\n</code></pre> Output <pre><code>2025-08-30 16:32:55.709 | WARNING  | chunklet.core:chunk:481 - \nFound 1 unique warning(s) during chunking:\n  - The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n--- Hybrid Mode Chunks ---\nChunk 1: This is a long text to demonstrate hybrid chunking.\nChunk 2: It combines both sentence and token limits for flexible chunking.\nChunk 3: Overlap helps maintain context between chunks by repeating some clauses.\nChunk 4: This mode is very powerful for maintaining semantic coherence.\nChunk 5: It is ideal for applications like RAG pipelines where context is crucial.\n</code></pre>"},{"location":"getting-started/programmatic/#batch-chunk-api","title":"Batch Chunk api","text":"<p>Parameters: - <code>texts</code> (List[str]): List of input texts to be chunked. - <code>lang</code> (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\". - <code>mode</code> (str): Chunking mode ('sentence', 'token', or 'hybrid'). Defaults to \"sentence\". - <code>max_tokens</code> (int): Maximum number of tokens per chunk. Defaults to 512. - <code>max_sentences</code> (int): Maximum number of sentences per chunk. Defaults to 100. - <code>overlap_percent</code> (Union[int, float]): Percentage of overlap between chunks (0-85). Defaults to 20. - <code>offset</code> (int): Starting sentence offset for chunking. Defaults to 0. - <code>n_jobs</code> (Optional[int]): Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified. - <code>token_counter</code> (Optional[Callable[[str], int]]): Optional token counting function. Required for token-based modes.</p> <p>Returns: - <code>List[List[str]]</code>: A list of lists, where each inner list contains the chunks for the corresponding input text.</p> <p>Raises: - <code>InvalidInputError</code>: If <code>texts</code> is not a list or if <code>n_jobs</code> is less than 1.</p>"},{"location":"getting-started/programmatic/#batch-processing","title":"Batch Processing","text":"<p>Process multiple documents in parallel for improved performance.</p> <pre><code>from chunklet import Chunklet\n\ntexts = [\n    \"This is the first document. It has multiple sentences for chunking.\",\n    \"Here is the second document. It is a bit longer to test batch processing effectively.\",\n    \"And this is the third document. Short and sweet, but still part of the batch.\",\n    \"The fourth document. Another one to add to the collection for testing purposes.\",\n]\n\n# Initialize Chunklet\nchunker = Chunklet(verbose=True)\n\nresults = chunker.batch_chunk(\n    texts=texts,\n    max_sentences=2,\n    n_jobs=2\n)\n\nfor i, doc_chunks in enumerate(results):\n    print(f\"--- Document {i+1} ---\")\n    for j, chunk in enumerate(doc_chunks):\n        print(f\"Chunk {j+1}: {chunk}\")\n    print(\"\\n\")  # Add a newline between documents for readability\n</code></pre> Output <pre><code>2025-08-30 16:37:44.568 | WARNING  | chunklet.core:batch_chunk:596 - \nFound 1 unique warning(s) during batch processing of 4 texts:\n  - (4/4) The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve performance.\n--- Document 1 ---\nChunk 1: This is the first document. It has multiple sentences for chunking.\nChunk 1: This is the 3rd sentences.\n\n\n--- Document 2 ---\nChunk 2: Here is the second document. It is a bit longer to test batch processing effectively, yup I really mean longer.\n\n\n--- Document 3 ---\nChunk 3: And this is the third document. Short and sweet, but still part of the batch.\n\n\n--- Document 4 ---\nChunk 4: The fourth document. Another one to add to the collection for testing purposes.\nChunk 4: Guess we needed it.\n</code></pre>"},{"location":"getting-started/programmatic/#custom-sentence-splitter","title":"Custom Sentence Splitter","text":"<p>You can provide your own custom sentence splitting functions to Chunklet. This is useful if you have a specialized splitter for a particular language or domain that you want to prioritize over Chunklet's built-in splitters.</p> <p>To use a custom splitter, initialize <code>Chunklet</code> with the <code>custom_splitters</code> parameter. This parameter expects a list of dictionaries, where each dictionary defines a splitter:</p> <ul> <li><code>name</code> (str): A unique name for your splitter.</li> <li><code>languages</code> (str or Iterable[str]): The language code(s) this splitter supports (e.g., \"en\", or [\"fr\", \"es\"]).</li> <li><code>callback</code> (Callable[[str], List[str]]): A function that takes the input text (string) and returns a list of sentences (list of strings).</li> </ul> <p>Custom splitters are checked before Chunklet's default <code>pysbd</code> and <code>sentence-splitter</code> implementations. If multiple custom splitters support the same language, the first one in the provided list will be used.</p> <pre><code>import re\nfrom chunklet import Chunklet\nfrom typing import List\n\n# Define a simple custom sentence splitter\ndef my_custom_splitter(text: str) -&gt; List[str]:\n    # This is a very basic splitter for demonstration\n    # In a real scenario, this would be a more sophisticated function\n    return [s.strip() for s in re.split(r'(?&lt;=\\.)\\\\s+', text) if s.strip()]\n\n# Initialize Chunklet with the custom splitter\nchunker = Chunklet(\n    custom_splitters=[\n        {\n            \"name\": \"MyCustomEnglishSplitter\",\n            \"languages\": \"en\",\n            \"callback\": my_custom_splitter,\n        }\n    ]\n)\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences, warnings = chunker.preview_sentences(text=text, lang=\"en\")\n\nprint(\"---\" + \" Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings:\n    print(\"\\n---\" + \" Warnings ---\")\n    for warning in warnings:\n        print(warning)\n\n# Example with a custom splitter for multiple languages\ndef multi_lang_splitter(text: str) -&gt; List[str]:\n    # A more complex splitter that might handle specific rules for French and Spanish\n    return [s.strip() for s in re.split(r'(?&lt;=!)\\s+', text) if s.strip()]\n\nchunker_multi = Chunklet(\n    custom_splitters=[\n        {\n            \"name\": \"MultiLangExclamationSplitter\",\n            \"languages\": [\"fr\", \"es\"],\n            \"callback\": multi_lang_splitter,\n        }\n    ]\n)\n\ntext_fr = \"Bonjour! Comment \u00e7a va? C'est super. Au revoir!\"\nsentences_fr, warnings_fr = chunker_multi.preview_sentences(text=text_fr, lang=\"fr\")\nprint(\"\\n--- Sentences using Multi-language Custom Splitter (French) ---\")\nfor i, sentence in enumerate(sentences_fr):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings_fr:\n    print(\"\\n--- Warnings (French) ---\")\n    for warning in warnings_fr:\n        print(warning)\n\ntext_es = \"Hola. Qu\u00e9 tal? Muy bien! Adi\u00f3s.\"\nsentences_es, warnings_es = chunker_multi.preview_sentences(text=text_es, lang=\"es\")\nprint(\"\\n--- Sentences using Multi-language Custom Splitter (Spanish) ---\")\nfor i, sentence in enumerate(sentences_es):\n    print(f\"Sentence {i+1}: {sentence}\")\n\nif warnings_es:\n    print(\"\\n--- Warnings (Spanish) ---\")\n    for warning in warnings_es:\n        print(warning)\n</code></pre> Output <pre><code>--- Sentences using Custom Splitter ---\nSentence 1: This is the first sentence.\nSentence 2: This is the second sentence.\nSentence 3: And the third.\nSentence 4: This is the fourth sentence, making the text longer.\nSentence 5: We are adding more content to demonstrate the chunking with max_sentences=1.\nSentence 6: This should result in more chunks.\n\n--- Sentences using Multi-language Custom Splitter (French) ---\nSentence 1: Bonjour!\nSentence 2: Comment \u00e7a va?\nSentence 3: C'est super.\nSentence 4: Au revoir!\n\n--- Sentences using Multi-language Custom Splitter (Spanish) ---\nSentence 1: Hola.\nSentence 2: Qu\u00e9 tal?\nSentence 3: Muy bien!\nSentence 4: Adi\u00f3s.\n</code></pre>"},{"location":"getting-started/programmatic/#pdf-chunking-exemple","title":"Pdf chunking exemple","text":"<p>So, you want to chunk a PDF? You've come to the right place. Well, not exactly. Chunklet doesn't have native PDF support... yet. But don't worry, we've got a workaround for you. Below is a cool wrapper you can build yourself to extract text from a PDF and chunk it with Chunklet. It's not perfect, but it gets the job done. And hey, it's better than nothing, right?</p> <p>Before you start, you'll need to download a sample PDF file. You can find one here. Just click the \"Download\" button, and you're good to go.</p>"},{"location":"getting-started/programmatic/#first-make-sure-you-installed-pypdf","title":"First make sure you installed pypdf","text":"<pre><code>pip install pypdf\n</code></pre>"},{"location":"getting-started/programmatic/#lets-get-rolling","title":"Let's get rolling!","text":"<pre><code>import regex as re\ntry:\n    from pypdf import PdfReader\nexcept ImportError:\n    print(\"pypdf not found. Please install it with: pip install pypdf\")\n    exit()\nfrom chunklet import Chunklet\n\nclass PDFProcessor:\n    \"\"\"\n    Extract, clean, and chunk PDF text by sentences.\n    Preserves meaningful newlines like headings and numbered lists,\n    removes standalone numbers, and chunks pages using Chunklet.\n    \"\"\"\n\n    def __init__(self, pdf_path: str):\n        self.pdf_path = pdf_path\n        self.chunker = Chunklet(verbose=False, use_cache=True)  # These are the default values.\n\n    def extract_text(self):\n        \"\"\"Extracts and cleans text from all pages of the PDF.\"\"\"\n        reader = PdfReader(self.pdf_path)\n        return [\n            self._cleanup_text(page.extract_text())\n            for page in reader.pages\n            if page.extract_text()\n        ]\n\n    def _cleanup_text(self, text: str):\n        \"\"\"Clean text, preserving meaningful newlines and removing standalone numbers.\"\"\"\n        if not text:\n            return \"\"\n\n        # Normalize 3+ consecutive newlines to 2\n        text = re.sub(r\"(?:\\n\\s*){3,}\", \"\\n\\n\", text)\n\n        # Remove lines that contain only numbers\n        text = re.sub(r\"\\n\\s*\\p{N}+\\s*\\n\", \"\\n\", text)\n\n        # Patterns for numbered lists and headings\n        numbered = re.compile(r\"\\n(\\d+\\) .+?)\\n\")\n        heading = re.compile(r\"\\n\\s*#*\\s*[\\p{L}\\p{N}].*?\\n\")\n\n        # Merge accidental line breaks that are NOT numbered lists or headings\n        text = re.sub(r\"(?&lt;!\\n)\\n(?!\\d+\\||[\\p{L}\\p{N}])\", \" \", text)\n\n        # Reapply patterns to preserve newlines\n        text = numbered.sub(r\"\\n\\1\\n\", text)\n        text = heading.sub(lambda m: \"\\n\" + m.group(0).strip() + \"\\n\", text)\n\n        return text\n\n    def batch_chunk_pages(self, max_sentences=5):\n        \"\"\"Extract text and chunk all pages using sentence mode (safe for mobile).\"\"\"\n        pages_text = self.extract_text()  # list of page texts\n        all_chunks = self.chunker.batch_chunk(\n            texts=pages_text,\n            mode=\"sentence\",\n            lang=\"fr\",\n            max_sentences=max_sentences,\n            overlap_percent=20,     # This is the default\n            offset=0,               # This is the default\n            n_jobs=1                # Default is None\n        )\n        return all_chunks\n\n\n# --- Usage example ---\npdf_path = \"examples/Sample.pdf\"\nprocessor = PDFProcessor(pdf_path)\n\n# Chunk all pages by sentences\npages_chunks = processor.batch_chunk_pages(max_sentences=15)\n\n# Print the chunks per page\nfor i, page_chunks in enumerate(pages_chunks, start=1):\n    print(f\"--- Page {i} Chunks ---\")\n    for j, chunk in enumerate(page_chunks, start=1):\n        print(f\"Chunk {j}: {chunk}\\n\")\n    print(\"=\"*50 + \"\\n\")\n</code></pre> Output <pre><code>--- Page 1 Chunks ---\nChunk 1: Les Bases de la Th\u00e9orie Musicale Bonjour \u00e0 tous et bienvenue dans cette formation sur les bases de la th\u00e9orie musicale ! Nous allons, \u00e0 l\u2019aide de cette formation vous apprendre \u00e0 d\u00e9velopper vos comp\u00e9tences en termes de production musicale. Aujourd'hui, la musique \u00e9lectronique touche une communaut\u00e9 de plus en plus large. Cela est d\u00fb en partie au nouveaux genres et sous genres qui \u00e9mergent chaque ann\u00e9e. Gr\u00e2ce \u00e0 cela, et au fait qu\u2019aujourd\u2019hui la plupart des gens poss\u00e8dent un ordinateur, de plus en plus de personnes se mettent \u00e0 la production. Cependant une grande partie de ces derniers ne sont pas des musiciens \u00e0 la base et n\u2019ont aucune notion en th\u00e9orie musicale. Nous avons donc d\u00e9cid\u00e9 de mettre en ligne gratuitement sur notre site une formation dans laquelle nous allons aborder les diff\u00e9rents th\u00e8mes existants en th\u00e9orie musicale. Dans un\n\nChunk 2: ... derniers ne sont pas des musiciens \u00e0 la base et n\u2019ont aucune notion en th\u00e9orie musicale. Nous avons donc d\u00e9cid\u00e9 de mettre en ligne gratuitement sur notre site une formation dans laquelle nous allons aborder les diff\u00e9rents th\u00e8mes existants en th\u00e9orie musicale. Dans un premier temps nous allons faire une introduction \u00e0 la th\u00e9orie musicale en vous expliquant son int\u00e9r\u00eat et en faisant une petite r\u00e9trospective sur la musique et ses formes. Puis, nous vous expliquerons comment la musique est lue et \u00e9crite et aborderons \u00e9galement la notion d\u2019harmonie. Il y aura ensuite une partie th\u00e9orique o\u00f9 nous \u00e9tudierons le tempo, les notes, les intervalles, les gammes et les accords. Et pour finir, nous passerons \u00e0 la partie pratique sur un logiciel de MAO dans laquelle nous parlerons du format MIDI, des partitions, de la cr\u00e9ation d\u2019accords sur un s\u00e9quenceur et nous vous d\u00e9voilerons enfin quelques astuces pour pouvoir facilement utiliser les gammes dans vos tracks.\n\nChunk 3: ... des partitions, de la cr\u00e9ation d\u2019accords sur un s\u00e9quenceur et nous vous d\u00e9voilerons enfin quelques astuces pour pouvoir facilement utiliser les gammes dans vos tracks. Cette formation convient \u00e0 tout type de producteur que vous soyez d\u00e9butant ou confirm\u00e9. Donc, quel que soit votre niveau n'h\u00e9sitez pas \u00e0 participer \u00e0 la formation car cela va grandement am\u00e9liorer votre qualit\u00e9 de production. Bonne lecture !\n==================================================\n\n--- Page 2 Chunks ---\nChunk 1: I. Introduction \u00e0 la Th\u00e9orie Musicale a. Quel est son int\u00e9r\u00eat ? 1. Lecture et \u00e9criture Tablature Partition Solf\u00e8ge Partition Midi (sous Fl Studio) 2. L\u2019harmonie 3. En quoi ceci est utile? b. Historique 1. La musique classique 2. La musique \u00e9lectronique II. Formation aux bases de la th\u00e9orie musicale a. Le tempo d\u2019un morceau\n\nChunk 2: II. Formation aux bases de la th\u00e9orie musicale a. Le tempo d\u2019un morceau b. Les notes Les tonalit\u00e9s Les diff\u00e9rents types de notes c. Les intervalles Notion de tons et demi tons Les alt\u00e9rations d. Les gammes 1. Gammes Majeures Alt\u00e9rations avec # 2. Gammes Mineures e. Les accords 1. Qu\u2019est ce qu\u2019un accord\n\nChunk 3: ... 2. Gammes Mineures e. Les accords 1. Qu\u2019est ce qu\u2019un accord 2. Comment les cr\u00e9er \u00c0 l\u2019aide d\u2019une gamme : \u00c0 l\u2019aide de la m\u00e9thode universelle En quoi consiste cette m\u00e9thode ? Comment l\u2019utiliser? III. Mise en application sur un s\u00e9quenceur a. Partitions MIDI D'o\u00f9 vient le MIDI? Comment l\u2019utiliser ? b. Tips utiles pour les gammes 1. Retrouver facilement des gammes sur internet\n\nChunk 4: Comment l\u2019utiliser ? b. Tips utiles pour les gammes 1. Retrouver facilement des gammes sur internet Fondamentale Accord Gamme 2. R\u00e9cup\u00e9rer une gamme sur le s\u00e9quenceur c. M\u00e9thodes de cr\u00e9ation d\u2019une suite d\u2019accords Mise en place Suite d\u2019accord simple Pour conclure\n==================================================\n\n--- Page 3 Chunks ---\nChunk 1: I. Introduction \u00e0 la Th\u00e9orie Musicale a. Quel est son int\u00e9r\u00eat ? Vous qui \u00eates en train de lire cette formation, devez s\u00fbrement vous poser la question   \u00ab Mais qu\u2019est ce donc la th\u00e9orie musicale, et pourquoi, en tant que producteur de musique \u00e9lectronique, en aurais-je besoin ?! \u00bb. Et bien c\u2019est ce que nous allons voir dans cette formation. Tout d\u2019abord, par ...\n\n[... And more]\n</code></pre>"},{"location":"getting-started/programmatic/#caching","title":"Caching","text":"<p>Chunklet uses an in-memory LRU (Least Recently Used) cache to speed up repeated chunking operations with the same parameters. This is enabled by default.</p>"},{"location":"getting-started/programmatic/#disabling-the-cache","title":"Disabling the Cache","text":"<p>You can disable the cache by setting <code>use_cache=False</code> when initializing <code>Chunklet</code>:</p> <pre><code>chunker = Chunklet(use_cache=False)\n</code></pre>"},{"location":"getting-started/programmatic/#clearing-the-cache","title":"Clearing the Cache","text":"<p>There is no public method to clear the cache of a <code>Chunklet</code> instance. If you need to clear the cache, you can simply create a new instance of the <code>Chunklet</code> class.</p>"},{"location":"getting-started/programmatic/#best-practices","title":"Best Practices","text":"<p>To get the most out of Chunklet and ensure efficient and accurate text processing, consider the following best practices:</p> <ul> <li>Explicit Language Setting: While Chunklet's <code>lang=\"auto\"</code> detection is robust, explicitly setting the <code>lang</code> parameter when you know the language of your text can significantly improve performance and accuracy, especially for shorter texts or less common languages.</li> <li>Optimize <code>overlap_percent</code>: The <code>overlap_percent</code> parameter is crucial for maintaining context between chunks. Experiment with different values (typically between 10-30%) to find the sweet spot for your specific use case. Too little overlap might break semantic continuity, while too much can lead to redundant information.</li> <li>Custom Token Counters for LLMs: If you're preparing text for a specific Large Language Model (LLM), it's highly recommended to integrate its tokenizer as a <code>token_counter</code>. This ensures that your chunks align perfectly with the LLM's tokenization strategy, preventing truncation issues and optimizing token usage.</li> <li>Leverage Batch Processing: For processing multiple documents, always prefer <code>batch_chunk()</code> over iterating and calling <code>chunk()</code> individually. <code>batch_chunk()</code> is optimized for parallel processing and will significantly speed up your workflow.</li> <li>Monitor Warnings: Pay attention to the warnings Chunklet emits. They often provide valuable insights into potential optimizations (e.g., language detection confidence) or inform you about fallback mechanisms being used.</li> <li>Consider Custom Splitters for Niche Cases: If you're working with highly specialized text (e.g., legal documents, medical transcripts) or languages not fully supported by <code>pysbd</code> or <code>sentence-splitter</code>, implementing a <code>custom_splitter</code> can provide superior sentence boundary detection tailored to your needs.</li> </ul>"}]}