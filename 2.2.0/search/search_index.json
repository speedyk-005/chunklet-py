{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Chunklet-py Docs","text":"<p>\u201cOne library to split them all: Sentence, Code, Docs.\u201d</p> <p>Hey! Welcome to the Chunklet-py docs. Let's make some text chunking magic happen.</p>"},{"location":"#why-smart-chunking-or-why-not-just-split-on-character-count","title":"Why Smart Chunking? (Or: Why Not Just Split on Character Count?)","text":"<p>You could split your text by character count or random line breaks. But that's like trying to cut a wedding cake with a chainsaw. \ud83c\udf82</p> <p>Dumb splitting causes problems:</p> <ul> <li>Mid-sentence surprises: Your thoughts get chopped mid-way, losing all meaning</li> <li>Language confusion: Non-English text and code structures get treated the same</li> <li>Lost context: Each chunk forgets what came before</li> </ul> <p>Smart chunking solves this by:</p> <ul> <li>Smart limits \u2014 Respects both natural boundaries (sentences, paragraphs, sections) AND configurable limits (tokens, lines, functions)</li> <li>Language-aware \u2014 Detects language automatically and applies the right rules (50+ languages supported)</li> <li>Context preservation \u2014 Overlap between chunks, rich metadata (source, span, document structure)</li> </ul>"},{"location":"#so-whats-chunklet-py-anyway-and-why-should-you-care","title":"\ud83e\udd14 So What's Chunklet-py Anyway? (And Why Should You Care?)","text":"<p>Chunklet-py is a developer-friendly text splitting library designed to be the most versatile chunking solution \u2014 for devs, researchers, and AI engineers. It goes way beyond basic character counting. I built this because I was tired of terrible chunking options. Chunklet-py intelligently chunks text, documents, and code into meaningful, context-aware pieces \u2014 perfect for RAG pipelines and LLM applications.</p> <p>Key features:</p> <ul> <li>Composable constraints \u2014 Mix and match limits (sentences, tokens, sections) to get exactly the chunks you need</li> <li>Pluggable architecture \u2014 Swap in custom tokenizers, sentence splitters, or processors  </li> <li>Rich metadata \u2014 Every chunk comes with source references, spans, and structural info</li> <li>Multi-format support \u2014 PDF, DOCX, EPUB, Markdown, HTML, LaTeX, ODT, CSV, Excel, and plain text</li> </ul> <p>Available tools:</p> <ul> <li><code>SentenceSplitter</code> \u2014 Lightweight sentence tokenization</li> <li><code>DocumentChunker</code> \u2014 Natural language with semantic boundaries</li> <li><code>CodeChunker</code> \u2014 Language-aware code chunking</li> <li><code>ChunkVisualizer</code> \u2014 Interactive web-based exploration</li> </ul> <p>Perfect for prepping data for LLMs, building RAG systems, or powering AI search - Chunklet-py gives you the precision and flexibility you need across tons of formats and languages.</p> <ul> <li> <p> Blazingly Fast</p> <p>Leverages efficient parallel processing to chunk large volumes of content with remarkable speed.</p> </li> <li> <p> Featherlight Footprint</p> <p>Designed to be lightweight and memory-efficient, ensuring optimal performance without unnecessary overhead.</p> </li> <li> <p> Rich Metadata for RAG</p> <p>Enriches chunks with valuable, context-aware metadata (source, span, document properties, code AST details) crucial for advanced RAG and LLM applications.</p> </li> <li> <p> Infinitely Customizable</p> <p>Offers extensive customization options, from pluggable token counters to custom sentence splitters and processors.</p> </li> <li> <p> Multilingual Mastery</p> <p>Supports over 50 natural languages for text and document chunking with intelligent detection and language-specific algorithms.</p> </li> <li> <p> Code-Aware Intelligence</p> <p>Language-agnostic code chunking that understands and preserves the structural integrity of your source code.</p> </li> <li> <p> Precision Chunking</p> <p>Flexible chunking with configurable limits based on sentences, tokens, sections, lines, and functions.</p> </li> <li> <p> Triple Interface: CLI, Library &amp; Web</p> <p>Use it as a command-line tool, import as a library for deep integration, or launch the interactive web visualizer for real-time chunk exploration and parameter tuning.</p> </li> </ul>"},{"location":"#how-does-chunklet-py-stack-up","title":"How Does Chunklet-py Stack Up?","text":"<p>Wondering how we compare to other chunking tools? Here's the quick comparison:</p> Library Key Differentiator Focus chunklet-py All-in-one, lightweight, multilingual, language-agnostic with specialized algorithms. Text, Code, Docs LangChain Full LLM framework with basic splitters (e.g., RecursiveCharacterTextSplitter, Markdown, HTML, code splitters). Good for prototyping but basic for complex docs or multilingual needs. Full Stack Chonkie All-in-one pipeline (chunking + embeddings + vector DB). Uses <code>tree-sitter</code> for code. Multilingual. Pipelines Semchunk Text-only, fast semantic splitting. Built-in tiktoken/HuggingFace support. 85% faster than alternatives. Text CintraAI Code Chunker Code-specific, uses <code>tree-sitter</code>. Initially supports Python, JS, CSS only. Code <p>Chunklet-py is a specialized, drop-in replacement for the chunking step in any RAG pipeline. It handles text, documents, and code without heavy dependencies, while keeping your project lightweight.</p>"},{"location":"#ready-lets-go","title":"Ready? Let's Go!","text":"<p>Pick your path:</p> <ul> <li>Installation: Get Chunklet-py running in minutes</li> <li>CLI Fan? The command line interface is perfect for quick tasks.<ul> <li>CLI Usage</li> </ul> </li> <li>Code Ninja? Want to integrate chunking into your Python projects?<ul> <li>Programmatic Usage</li> </ul> </li> </ul>"},{"location":"#the-full-tour","title":"The Full Tour","text":"<p>Curious about all the features?</p> <ul> <li>Supported Languages: See which languages Chunklet speaks fluently.</li> <li>Exceptions and Warnings: Because sometimes, things go wrong. Here's what to do when they do.</li> <li>Metadata: Understand the rich context <code>chunklet</code> attaches to your chunks.</li> <li>Troubleshooting: Solutions to common issues you might encounter.</li> </ul>"},{"location":"#stay-in-the-loop","title":"Stay in the Loop","text":"<p>Want to keep up with Chunklet-py's latest adventures?</p> <ul> <li>What's New: Discover all the exciting new features and improvements in Chunklet 2.2.0.</li> <li> <p>Migration Guide: Learn how to smoothly transition from previous versions to Chunklet 2.x.x.</p> </li> <li> <p>Changelog: See what's new, what's fixed, and what's been improved in recent versions.</p> </li> </ul>"},{"location":"#project-details-join-the-fun","title":"Project Details &amp; Join the Fun","text":"<p>For the behind-the-scenes info and if you're thinking of contributing:</p> <ul> <li>GitHub Repository: The main hub for all things Chunklet.</li> <li>License Information: All the necessary bits and bobs about Chunklet-py's license.</li> <li>Contributing: Want to help make Chunklet even better? Find out how you can contribute!                                            </li> </ul>"},{"location":"exceptions-and-warnings/","title":"Exceptions and Warnings \ud83d\uded1","text":"<p>Sometimes things break. Sometimes they almost break. Here's what it means.</p>"},{"location":"exceptions-and-warnings/#exceptions","title":"Exceptions \ud83d\udca5","text":""},{"location":"exceptions-and-warnings/#chunkleterror","title":"<code>ChunkletError</code>","text":"<p>The base exception. Catch this if you want to catch all the things. One ring to rule them all. All other exceptions in this library inherit from this, so catching this one catches everything.</p>"},{"location":"exceptions-and-warnings/#invalidinputerror","title":"<code>InvalidInputError</code>","text":"<p>You passed something wrong. Wrong type, missing required field, bad file extension, etc. We tried to work with it but couldn't. This is our way of saying \"that doesn't look right\" \u2014 we validate inputs on the way in so you find out early.</p> <p>Fix: Check the error message. It usually tells you what's up. Yes, actually read it.</p>"},{"location":"exceptions-and-warnings/#missingtokencountererror","title":"<code>MissingTokenCounterError</code> \ud83d\udd22","text":"<p>You tried to chunk by tokens but forgot to give us a <code>token_counter</code>. We can't read minds... yet. Without it, we have no idea how many tokens your text has, so we can't enforce <code>max_tokens</code>. This only applies to token-based chunking \u2014 if you're using sentences or lines, you're fine.</p> <p>Fix: Pass one:</p> <pre><code>chunker.chunk_text(text, token_counter=some_counter)\n# or\nchunker = SomeChunker(token_counter=some_counter)\n</code></pre>"},{"location":"exceptions-and-warnings/#fileprocessingerror","title":"<code>FileProcessingError</code> \ud83d\udcc1","text":"<p>File couldn't be read. Missing, permissions, encoding issues, corrupted \u2014 something went wrong and we gave up. This happens when we try to open a file and it fails, whether the file doesn't exist, you don't have permission, it's binary garbage, or it's just broken.</p> <p>Fix: Check if the file exists and actually opens.</p>"},{"location":"exceptions-and-warnings/#unsupportedfiletypeerror","title":"<code>UnsupportedFileTypeError</code> \ud83d\udcc4","text":"<p>We don't support that file extension. We checked our list of supported formats and yours wasn't on it. This can also happen if:</p> <ul> <li>The processor for that file type returns an iterable (not a string), which requires batch processing \ud83d\udd04</li> <li>The file has no extension, so we don't know how to handle it</li> </ul> <p>The list of supported formats includes things like <code>.txt</code>, <code>.md</code>, <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.html</code>, <code>.rst</code>, <code>.rtf</code>, <code>.tex</code>, <code>.odt</code>, <code>.csv</code>, and <code>.xlsx</code>. If your file isn't one of these, you'll see this error.</p> <p>Fix: Use a supported format, register a custom processor, or use <code>chunk_files([path])</code> for formats that require batch processing.</p>"},{"location":"exceptions-and-warnings/#tokenlimiterror","title":"<code>TokenLimitError</code> \ud83d\udccf","text":"<p>A code block is too fat for <code>max_tokens</code> and you're in <code>strict</code> mode. We refuse to split it because that would break the code. This happens in <code>CodeChunker</code> when a function or class is larger than your token limit \u2014 splitting it would result in broken, unrunnable code, so we error instead.</p> <p>Fix: Bump <code>max_tokens</code> or set <code>strict=False</code> (which will split it anyway, even if it breaks).</p>"},{"location":"exceptions-and-warnings/#callbackerror","title":"<code>CallbackError</code> \ud83d\udd0c","text":"<p>Your custom callback (token_counter, splitter, processor) threw an error. Whatever you wrote in that function crashed. We wrap this so you know the error came from your code, not ours \u2014 the traceback will point you to exactly where things went sideways.</p> <p>Fix: Debug your callback. The stack trace has the answers.</p>"},{"location":"exceptions-and-warnings/#warnings","title":"Warnings \u26a0\ufe0f","text":""},{"location":"exceptions-and-warnings/#language-auto-detect","title":"Language auto-detect \ud83c\udf0d","text":"<pre><code>The language is set to `auto`. Consider setting `lang` explicitly.\n</code></pre> <p>What it means: We don't know what language your text is. Auto-detect works, but explicit is faster and more reliable, especially with short texts. Language detection is a guess \u2014 short texts have less signal, so the guess is less confident.</p> <p>Fix: Pass <code>lang='en'</code> (or whatever) if you know it.</p>"},{"location":"exceptions-and-warnings/#universal-splitter-fallback","title":"Universal splitter fallback \ud83c\udf10","text":"<pre><code>Using universal rule-based splitter. Language not supported or detected with low confidence.\n</code></pre> <p>What it means: No specialized splitter for your language exists in pysbd. We're using the generic regex fallback instead. Some languages have complex punctuation rules that the specialized splitters handle \u2014 the fallback is a one-size-fits-all that works okay but isn't great for anything.</p> <p>Fix: Either provide a custom splitter, or don't worry \u2014 the fallback is decent enough.</p>"},{"location":"exceptions-and-warnings/#offset-out-of-range","title":"Offset out of range \ud83d\udccd","text":"<pre><code>Offset {} &gt;= total sentences {}. Returning empty list.\n</code></pre> <p>What it means: Your offset is bigger than the text. There's nothing left to split so we return nothing. This is just informing you \u2014 it's not an error, you just asked for more sentences than exist.</p> <p>Fix: Use a smaller offset.</p>"},{"location":"exceptions-and-warnings/#skipping-failed-task","title":"Skipping failed task \ud83c\udfc3","text":"<pre><code>Skipping failed task. Reason: {error}\n</code></pre> <p>What it means: One file in your batch choked and you set <code>on_errors='skip'</code>. We logged the error and kept going. This is intentional \u2014 you told us to continue on error, so we do. Check the logs to see what actually failed.</p> <p>Fix: Check the reason. Fix the file or change <code>on_errors</code>.</p>"},{"location":"exceptions-and-warnings/#validation-failure","title":"Validation failure \u2705","text":"<pre><code>Skipping document {path} due to validation failure. Reason: {reason}\n</code></pre> <p>What it means: File failed validation. Bad extension, corrupted, or some other issue \u2014 we couldn't process it. This happens before we even try to extract text \u2014 the file just doesn't look right.</p> <p>Fix: Check the reason.</p>"},{"location":"exceptions-and-warnings/#no-valid-files","title":"No valid files \ud83d\udcc2","text":"<pre><code>No valid files found after validation. Returning empty generator.\n</code></pre> <p>What it means: None of your input files passed the vibe check. Wrong paths, unsupported formats, or all of them failed validation. We validated every file and found nothing worth processing.</p> <p>Fix: Check your paths and file types.</p>"},{"location":"exceptions-and-warnings/#oversized-code-block","title":"Oversized code block \ud83d\udcaa","text":"<pre><code>Splitting oversized block ({token_count} tokens) into sub-chunks\n</code></pre> <p>What it means: A code block exceeded <code>max_tokens</code>. In non-strict mode, we split it anyway to avoid throwing an error. This is us being helpful \u2014 we could have errored, but instead we broke it into smaller chunks even though they're not valid code anymore.</p> <p>Fix: Set <code>strict=True</code> if you want it to error instead.</p>"},{"location":"exceptions-and-warnings/#path-not-found","title":"Path not found \ud83d\udd0d","text":"<pre><code>{path} is path-like but was not found. Skipping.\n</code></pre> <p>What it means: That file? Doesn't exist. We checked the filesystem and it's not there. Either you typo'd the path, the file was moved, or it's a symlink pointing to nowhere.</p> <p>Fix: Check the path.</p>"},{"location":"exceptions-and-warnings/#path-doesnt-look-like-a-path","title":"Path doesn't look like a path \ud83e\uddd0","text":"<pre><code>{path} does not resemble a valid file system path. Skipping.\n</code></pre> <p>What it means: You passed something that doesn't look like a file path. We use heuristics to guess, and this one failed the test. It's probably a URL, a glob pattern, or just a random string.</p> <p>Fix: Double-check your input.</p>"},{"location":"exceptions-and-warnings/#no-processable-files","title":"No processable files \ud83d\udced","text":"<pre><code>No processable files found in the specified source(s). Exiting.\n</code></pre> <p>What it means: No files with supported extensions found in your input. We scanned the directories and came up empty. Either the directory is empty, contains only unsupported file types, or you pointed us at the wrong place.</p> <p>Fix: Check your paths.</p>"},{"location":"exceptions-and-warnings/#no-chunks-generated","title":"No chunks generated \ud83e\uded9","text":"<pre><code>No chunks were generated. Input might be empty.\n</code></pre> <p>What it means: Nothing to chunk. The input is empty or we couldn't extract any text from it. This can happen with binary files, image-only PDFs, or literally empty files.</p> <p>Fix: Check your input.</p>"},{"location":"exceptions-and-warnings/#deprecation-warning","title":"Deprecation warning \u23f3","text":"<pre><code>{object} was deprecated since v{version}. Use {replacement} instead.\n</code></pre> <p>What it means: You're using old stuff. It works now, but we'll remove it in a future version so you should migrate eventually. We're not removing it yet, but we're telling you now so you're not surprised later.</p> <p>Fix: Switch to the replacement when you get a chance.</p>"},{"location":"migration/","title":"Migrating from v1 to v2: Everything Changed (But You'll Be Fine)","text":"<p>Python Version Bump</p> <p>v2.x.x dropped Python 3.8 and 3.9. Minimum is now 3.10. Update your env if you're stuck on ancient Python.</p> <p>So you upgraded to v2 and things broke. That's normal. Let me walk you through what changed and how to fix it.</p>"},{"location":"migration/#the-breaking-stuff","title":"The Breaking Stuff","text":"<p>Here's what blew up between v1 and v2:</p>"},{"location":"migration/#chunklet-is-now-documentchunker","title":"<code>Chunklet</code> is now <code>DocumentChunker</code>","text":"<p>I renamed the main class. Why? Because we now have two chunkers (<code>DocumentChunker</code> and <code>CodeChunker</code>), and calling one of them just \"Chunklet\" was confusing. Now the names actually make sense.</p> <p>Fix:</p> Before (v1.4.0)After (v2.x.x) <pre><code>from chunklet import Chunklet\n\nchunker = Chunklet()\n</code></pre> <pre><code>from chunklet import DocumentChunker\n\nchunker = DocumentChunker()\n</code></pre>"},{"location":"migration/#use_cache-is-gone","title":"<code>use_cache</code> is gone","text":"<p>I removed the <code>use_cache</code> flag. It was doing internal stuff that didn't need your attention anyway. Now caching just works without you having to think about it.</p> <p>Fix: Delete <code>use_cache=False</code> from your calls.</p> Before (v1.4.0)After (v2.x.x) <pre><code>chunker = Chunklet()\nchunks = chunker.chunk(text, use_cache=False, ...)\n</code></pre> <pre><code>chunker = DocumentChunker()\nchunks = chunker.chunk_text(text, ...)\n</code></pre>"},{"location":"migration/#the-mode-argument-is-gone","title":"The <code>mode</code> argument is gone","text":"<p>This was confusing. Instead of saying <code>mode=\"sentence\"</code> or <code>mode=\"hybrid\"</code>, now you just pass the limits you want. Whatever you pass determines how it chunks. Simple.</p> <p>What's different: - No more <code>mode</code> parameter - No more default values for <code>max_tokens</code> or <code>max_sentences</code> - you have to pick - New toy: <code>max_section_breaks</code> lets you chunk by headings, horizontal rules (<code>---</code>, <code>***</code>, <code>___</code>), and <code>&lt;details&gt;</code> tags</p> <p>Fix: Stop using <code>mode</code>. Just pass your limits.</p> Before (v1.4.0) - hybrid modeAfter (v2.x.x) <pre><code>chunks = chunker.chunk(text, mode=\"hybrid\", max_sentences=5, max_tokens=512)\n</code></pre> <pre><code>chunks = chunker.chunk_text(text, max_sentences=5, max_tokens=512)\n</code></pre>"},{"location":"migration/#chunk-is-now-chunk_text","title":"<code>chunk()</code> is now <code>chunk_text()</code>","text":"<p>The method was renamed to be clear about what it takes: strings.</p> <p>Fix:</p> Before (v1.4.0)After (v2.x.x) <pre><code>chunks = chunker.chunk(text, mode=\"sentence\", max_sentences=5)\n</code></pre> <pre><code>chunks = chunker.chunk_text(text, max_sentences=5)\n</code></pre>"},{"location":"migration/#batch_chunk-is-now-chunk_texts","title":"<code>batch_chunk()</code> is now `chunk_texts()","text":"<p>For multiple texts use <code>chunk_texts()</code>. The name actually describes what it does now.</p> <p>Fix:</p> Before (v1.4.0)After (v2.x.x) <pre><code>texts = [\"text1\", \"text2\"]\nchunks = chunker.batch_chunk(texts, mode=\"sentence\", max_sentences=5)\n</code></pre> <pre><code>texts = [\"text1\", \"text2\"]\nchunks = chunker.chunk_texts(texts, max_sentences=5)\n</code></pre>"},{"location":"migration/#language-detection-moved","title":"Language detection moved","text":"<p>The old <code>detect_text_language.py</code> file is gone. Language detection now lives inside <code>SentenceSplitter</code> directly. Most people won't notice because it happens automatically, but if you were calling it directly, here's the fix:</p> Before (v1.4.0)After (v2.x.x) <pre><code>from chunklet.utils.detect_text_language import detect_text_language\n\nlang, confidence = detect_text_language(text)\n</code></pre> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nsplitter = SentenceSplitter()\nlang, confidence = splitter.detected_top_language(text)\n</code></pre>"},{"location":"migration/#custom-splitters-use-a-registry-now","title":"Custom splitters use a registry now","text":"<p>The old <code>custom_splitters</code> parameter in the constructor is gone. Instead, there's a global registry you register with. This means your custom splitters work across all chunker instances, not just one.</p> <p>Fix:</p> Before (v1.4.0)After (v2.x.x) <pre><code>import re\nfrom chunklet import Chunklet\nfrom typing import List\n\ndef my_custom_splitter(text: str) -&gt; List[str]:\n    return [s.strip() for s in re.split(r'(?&lt;=\\.)\\\\s+', text) if s.strip()]\n\nchunker = Chunklet(\n    custom_splitters=[\n        {\n            \"name\": \"MyCustomEnglishSplitter\",\n            \"languages\": \"en\",\n            \"callback\": my_custom_splitter,\n        }\n    ]\n)\n</code></pre> <pre><code>import re\nfrom chunklet import DocumentChunker\nfrom chunklet.sentence_splitter import custom_splitter_registry\n\n@custom_splitter_registry.register(\"en\", name=\"MyAwesomeEnglishSplitter\")\ndef my_awesome_splitter(text: str) -&gt; list[str]:\n    return [s.strip() for s in re.split(r'[.!?]\\s+', text) if s.strip()]\n\nchunker = DocumentChunker()\nchunks = chunker.chunk_text(text, lang=\"en\", max_sentences=1)\n</code></pre> <p>Check the docs for more details.</p>"},{"location":"migration/#exception-name-changes","title":"Exception name changes","text":"<p>A couple exceptions got renamed to be less confusing:</p> <ul> <li><code>TokenNotProvidedError</code> -&gt; <code>MissingTokenCounterError</code> (clearer about what you forgot)</li> <li>Custom callback errors now throw <code>CallbackError</code> instead of generic <code>ChunkletError</code> (so you know it's your code that broke, not ours)  </li> </ul>"},{"location":"migration/#cli-changed","title":"CLI changed","text":"<p>The CLI got a new structure. Instead of just <code>chunklet \"text\"</code>, you now use <code>chunklet chunk \"text\"</code>. </p> <ul> <li>Text as argument = DocumentChunker</li> <li>File with <code>--source</code> = DocumentChunker (handles PDFs, DOCX, etc.)</li> <li>Add <code>--code</code> flag = CodeChunker</li> </ul> Before (v1.4.0)After (v2.x.x) <pre><code>chunklet \"Your text here.\" --mode sentence --max-sentences 5\n</code></pre> <pre><code># Chunking a string uses DocumentChunker\nchunklet chunk \"Your text here.\" --max-sentences 5\n\n# Chunking a file from a path uses DocumentChunker by default\nchunklet chunk --source your_text.txt --max-sentences 5\n</code></pre> <p>See CLI docs for the full breakdown.</p>"},{"location":"migration/#automated-migration-checker","title":"Automated migration checker","text":"<p>I wrote a script that scans your code for old v1 patterns. It'll point out exactly what needs changing.</p> <pre><code>curl -O https://raw.githubusercontent.com/speedyk-005/chunklet-py/main/audit_migration.py\npython audit_migration.py /path/to/your/project\n</code></pre> <p>That's it. Go forth and migrate.</p>"},{"location":"supported-languages/","title":"Supported Languages: A World Tour \ud83c\udf0d","text":"<p>So you want to know if Chunklet-py speaks your language? Short answer: probably yes. Long answer: keep reading!</p> <p>I've built Chunklet-py to be quite the polyglot. Thanks to some fantastic third-party libraries, it can handle over 50 languages out of the box. And if your language isn't on the list? Don't sweat it \u2014 I've got a fallback splitter that's like that friend who kind of understands every language at the party.</p> <p>We use ISO 639-1 codes (those handy two-letter shortcuts like <code>en</code>, <code>fr</code>, <code>es</code>). Check out Wikipedia's full list if you're hunting for a specific code.</p>"},{"location":"supported-languages/#the-all-stars-languages-where-chunklet-py-truly-shines","title":"The All-Stars: Languages Where Chunklet-py Truly Shines \u2b50","text":"<p>Here's where we bring out the big guns. These languages have dedicated, high-quality splitters \u2014 think of them as the VIP section of our language support. If your language is here, you're in good hands.</p> <p>And if it's not? No worries \u2014 the Fallback Splitter at the bottom of this page has your back.</p> <p>Let me introduce you to the libraries making this magic happen:</p>"},{"location":"supported-languages/#the-headliner-pysbd","title":"The Headliner: <code>pysbd</code>","text":"<p>This is our workhorse. <code>pysbd</code> (Python Sentence Boundary Detection) is incredibly good at figuring out where sentences end \u2014 even in tricky situations. It's the reason we can handle 40+ languages without making a mess of your text.</p> Language Code Language Name Flag en English \ud83c\uddec\ud83c\udde7 mr Marathi \ud83c\uddee\ud83c\uddf3 hi Hindi \ud83c\uddee\ud83c\uddf3 bg Bulgarian \ud83c\udde7\ud83c\uddec es Spanish \ud83c\uddea\ud83c\uddf8 ru Russian \ud83c\uddf7\ud83c\uddfa ar Arabic \ud83c\uddf8\ud83c\udde6 am Amharic \ud83c\uddea\ud83c\uddf9 hy Armenian \ud83c\udde6\ud83c\uddf2 fa Persian (Farsi) \ud83c\uddee\ud83c\uddf7 ur Urdu \ud83c\uddf5\ud83c\uddf0 pl Polish \ud83c\uddf5\ud83c\uddf1 zh Chinese (Mandarin) \ud83c\udde8\ud83c\uddf3 nl Dutch \ud83c\uddf3\ud83c\uddf1 da Danish \ud83c\udde9\ud83c\uddf0 fr French \ud83c\uddeb\ud83c\uddf7 it Italian \ud83c\uddee\ud83c\uddf9 el Greek \ud83c\uddec\ud83c\uddf7 my Burmese (Myanmar) \ud83c\uddf2\ud83c\uddf2 ja Japanese \ud83c\uddef\ud83c\uddf5 de German \ud83c\udde9\ud83c\uddea kk Kazakh \ud83c\uddf0\ud83c\uddff sk Slovak \ud83c\uddf8\ud83c\uddf0"},{"location":"supported-languages/#special-guest-sentsplit","title":"Special Guest: <code>sentsplit</code>","text":"<p>A few more languages needed a home, so <code>sentsplit</code> stepped in. Think of these as the opening act \u2014 still great, just a smaller crowd.</p> Language Code Language Name Flag ko Korean \ud83c\uddf0\ud83c\uddf7 lt Lithuanian \ud83c\uddf1\ud83c\uddf9 pt Portuguese \ud83c\uddf5\ud83c\uddf9 tr Turkish \ud83c\uddf9\ud83c\uddf7"},{"location":"supported-languages/#the-indian-subcontinent-squad-indic-nlp-library","title":"The Indian Subcontinent Squad: <code>Indic NLP Library</code>","text":"<p>The <code>Indic NLP Library</code> handles 11 languages from the Indian subcontinent. These languages have some pretty complex scripts, so specialized support is a must.</p> Language Code Language Name Flag as Assamese \ud83c\uddee\ud83c\uddf3 bn Bengali \ud83c\uddee\ud83c\uddf3 gu Gujarati \ud83c\uddee\ud83c\uddf3 kn Kannada \ud83c\uddee\ud83c\uddf3 ml Malayalam \ud83c\uddee\ud83c\uddf3 ne Nepali \ud83c\uddf3\ud83c\uddf5 or Odia \ud83c\uddee\ud83c\uddf3 pa Punjabi \ud83c\uddee\ud83c\uddf3 sa Sanskrit \ud83c\uddee\ud83c\uddf3 ta Tamil \ud83c\uddee\ud83c\uddf3 te Telugu \ud83c\uddee\ud83c\uddf3"},{"location":"supported-languages/#the-wildcard-sentencex","title":"The Wildcard: <code>Sentencex</code>","text":"<p><code>Sentencex</code> from Wikimedia adds even more languages to the mix. It's a bit more relaxed about things \u2014 uses fallbacks when it doesn't have a perfect match for your language.</p> <p>Wait, what's a fallback?</p> <p>Good question! If <code>Sentencex</code> doesn't have a perfect splitter for your language, it falls back to a similar one. Like using Spanish rules for Galician \u2014 close enough, usually gets the job done.</p> <p>I've filtered the list below to only show languages that are actually useful and reliable. No point showing you 200 languages if half of them are just \"eh, good enough\" \u2014 right?</p> Language Code Language Name Flag an Aragonese \ud83c\uddea\ud83c\uddf8 ca Catalan \ud83c\uddea\ud83c\uddf8 co Corsican \ud83c\uddeb\ud83c\uddf7 cs Czech \ud83c\udde8\ud83c\uddff fi Finnish \ud83c\uddeb\ud83c\uddee gl Galician \ud83c\uddea\ud83c\uddf8 io Ido \ud83c\udff3\ufe0f jv Javanese \ud83c\uddee\ud83c\udde9 li Limburgish \ud83c\uddf3\ud83c\uddf1 mo Moldovan \ud83c\uddf2\ud83c\udde9 nds Low German \ud83c\udde9\ud83c\uddea nn Norwegian Nynorsk \ud83c\uddf3\ud83c\uddf4 oc Occitan \ud83c\uddeb\ud83c\uddf7 su Sundanese \ud83c\uddee\ud83c\udde9 wa Walloon \ud83c\udde7\ud83c\uddea"},{"location":"supported-languages/#the-universal-translator-fallback-splitter","title":"The Universal Translator: Fallback Splitter \ud83d\udd04","text":"<p>So your language isn't on the list? That's okay \u2014 this is where things get interesting.</p> <p>The Fallback Splitter is my \"when in doubt\" solution. It's a rule-based regex splitter that takes a reasonable shot at sentence segmentation for... well, anything. Is it as smart as the dedicated libraries above? Nope. But it'll work when you need it to.</p> <p>Think of it as that friend at the karaoke bar who doesn't know the song but will still give it their best shot. \ud83e\udd64</p> <p>API Reference</p> <p>For the nerds who want the full details, check out the <code>FallbackSplitter</code> API docs.</p>"},{"location":"supported-languages/#teaching-chunklet-new-tricks-custom-splitters","title":"Teaching Chunklet New Tricks: Custom Splitters \ud83d\udee0\ufe0f","text":"<p>What if none of this works for you? Maybe you have a weird edge case, or you're working with something really niche. That's where custom splitters come in \u2014 you bring your own splitting logic, and Chunklet-py will use it like a boss.</p> <p>Here's how you can add your own splitter:</p> <p>Option A: Register Directly (The No-Nonsense Way)</p> <pre><code>from chunklet.sentence_splitter import custom_splitter_registry\n\ndef my_custom_splitter(text: str) -&gt; list[str]:\n    # Your brilliant, custom splitting logic here\n    return text.split('.')\n\n# Teach Chunklet your new trick for English\ncustom_splitter_registry.register(my_custom_splitter, \"en\", name=\"MyCustomSplitter\")\n</code></pre> <p>Option B: Use a Decorator (The Fancy Way)</p> <pre><code>from chunklet.sentence_splitter import custom_splitter_registry\n\n@custom_splitter_registry.register(\"fr\", name=\"MyFrenchSplitter\")\ndef my_french_splitter(text: str) -&gt; list[str]:\n    # Your magnifique splitting logic for French\n    return text.split('!')\n</code></pre> <p>Go Global with 'xx'</p> <p>Register a splitter with the language code <code>xx</code> and it'll become your universal fallback. Just set <code>lang='xx'</code> when chunking and boom \u2014 your splitter runs the show.</p>"},{"location":"troubleshooting/","title":"Troubleshooting \ud83d\udee0\ufe0f","text":"<p>Things break. Here's how to fix them.</p>"},{"location":"troubleshooting/#batch-processing-hangs-on-exit","title":"Batch Processing Hangs on Exit","text":"Why does <code>batch_chunk</code> hang, then throw a <code>TypeError</code> when I Ctrl+C? <p>You broke out of the loop early with a <code>break</code>, or didn't finish iterating through all chunks. </p> <p>Batch methods use multiprocessing. When you bail out early, the background processes don't get cleaned up properly. Hit Ctrl+C and Python has a breakdown.</p> <p>Fix:</p> <p>Option 1: Close the generator</p> <pre><code>from chunklet import DocumentChunker\n\npaths = [\"doc1.pdf\", \"doc2.txt\"]\nchunker = DocumentChunker()\nchunks = chunker.chunk_files(paths)\n\ntry:\n    for chunk in chunks:\n        if some_condition:\n            break\n        print(chunk.content)\nfinally:\n    chunks.close()\n</code></pre> <p>Option 2: Convert to list</p> <pre><code>all_chunks = list(chunker.chunk_files(paths))\n# pool is closed once list is built\n</code></pre> <p>Simple tradeoff: memory vs peace of mind.</p>"},{"location":"troubleshooting/#visualizer-showing-oldcached-stuff","title":"Visualizer Showing Old/Cached Stuff","text":"Why does the visualizer look broken after I updated? <p>Browser cache. The classic \"it worked yesterday\" problem.</p> <p>Fix:</p> <ul> <li>Hard refresh: <code>Ctrl+Shift+R</code> (Windows/Linux) or <code>Cmd+Shift+R</code> (Mac)</li> <li>Or just open incognito \u2014 caches don't follow you there</li> </ul>"},{"location":"troubleshooting/#something-broke-or-warned","title":"Something Broke or Warned","text":"Something threw an exception. Now what? <p>First: actually read the error message. We know, we know \u2014 \"just read the error\" sounds obvious, but sometimes it actually tells you what's wrong.</p> <p>If it's a warning, you're probably fine \u2014 just a heads up. If it's an exception, something actually broke.</p> <p>What to do:</p> <ol> <li>Read the message \u2014 it usually tells you what's up</li> <li>Check exceptions-and-warnings.md \u2014 we explain what each one means</li> <li>Check What's New \u2014 breaking changes and new stuff live there</li> <li>Open an issue \u2014 if it's genuinely broken and not covered, let us know. But check first.</li> </ol>"},{"location":"whats-new/","title":"What's New","text":"<p>What's on This Page</p> <p>The big stuff. The shiny new things. The stuff we got tired of fixing. For everything else, there's the changelog.</p>"},{"location":"whats-new/#chunklet-v220","title":"Chunklet v2.2.0","text":""},{"location":"whats-new/#simpler-chunking-api","title":"\u2728 Simpler Chunking API","text":"<p>We renamed some methods. Yes, we're those people who rename things. But honestly, the old names were confusing \u2014 even to us:</p> <ul> <li><code>chunk_text()</code> \u2014 chunk a string</li> <li><code>chunk_file()</code> \u2014 chunk a file directly  </li> <li><code>chunk_texts()</code> \u2014 batch strings</li> <li><code>chunk_files()</code> \u2014 batch files</li> </ul> <p>The old <code>chunk</code> and <code>batch_chunk</code> still work. They'll whine at you with a deprecation warning. Deal with it or migrate \u2014 your choice.</p>"},{"location":"whats-new/#plaintextchunker-got-absorbed","title":"\ud83d\udd17 PlainTextChunker Got Absorbed","text":"<p><code>PlainTextChunker</code> is now part of <code>DocumentChunker</code>. We know \u2014 having two chunkers was weird. Just use <code>chunk_text()</code> or <code>chunk_texts()</code> like a normal person. The old import still works, technically, with a deprecation warning.</p>"},{"location":"whats-new/#sentencesplitter-now-does-split_text","title":"\u2702\ufe0f SentenceSplitter Now Does <code>split_text()</code>","text":"<p><code>split()</code> is out. <code>split_text()</code> is in. We renamed it because apparently \"split\" was too short. There's also now <code>split_file()</code> if you're the type who likes skipping steps.</p>"},{"location":"whats-new/#visualizer-makeover","title":"\ud83c\udfa8 Visualizer Makeover","text":"<p>The chunk visualizer finally got some love:</p> <ul> <li>Fullscreen mode \u2014 for when you want to pretend you're doing something important</li> <li>3-row layout \u2014 less cluttered, more clickable</li> <li>Smoother hovers \u2014 no more seizure-inducing animations</li> <li>Smarter buttons \u2014 they stay enabled because, honestly, disabling them was stupid</li> </ul>"},{"location":"whats-new/#shorter-cli-flags","title":"\u2328\ufe0f Shorter CLI Flags","text":"<p>Finally, stuff you can actually type without wrist strain:</p> <ul> <li><code>-l</code> for <code>--lang</code></li> <li><code>-h</code> for <code>--host</code> </li> <li><code>-m</code> for <code>--metadata</code></li> </ul> <p>You're welcome.</p>"},{"location":"whats-new/#code-chunking-less-broken","title":"\ud83e\uddd1\u200d\ud83d\udcbb Code Chunking, Less Broken","text":"<p>Code chunking got slightly less terrible:</p> <ul> <li>Cleaner output \u2014 fixed weird artifacts in chunks from comment handling (we know, it was annoying)</li> <li>More languages \u2014 Forth, PHP 8 attributes, VB.NET, ColdFusion, and Pascal. Yes, really.</li> <li>String protection \u2014 multi-line strings and triple-quotes won't get mangled anymore</li> </ul>"},{"location":"whats-new/#the-boring-but-necessary-stuff","title":"\ud83d\udd27 The Boring But Necessary Stuff","text":"<ul> <li>Tokenizer timeout \u2014 new <code>--tokenizer-timeout</code> / <code>-t</code> flag so custom tokenizers don't hang forever</li> <li>Direct imports \u2014 <code>from chunklet import DocumentChunker</code> now works without making things slow</li> <li>Fewer crashes \u2014 fixed dependency issues with <code>setuptools&lt;81</code> in CI (sentsplit and pkg_resources, long story)</li> <li>Global registries \u2014 <code>custom_splitter_registry</code> and <code>custom_processor_registry</code> exist now</li> <li>Error messages \u2014 slightly less cryptic when things explode</li> </ul>"},{"location":"whats-new/#chunklet-v211","title":"Chunklet v2.1.1","text":""},{"location":"whats-new/#visualizer-was-broken","title":"\ud83d\udc1b Visualizer Was Broken","text":"<p>The visualizer didn't work after installing from PyPI. Static files were MIA. Fixed now, obviously.</p>"},{"location":"whats-new/#chunklet-v210","title":"Chunklet v2.1.0","text":""},{"location":"whats-new/#visualizer-10","title":"\ud83c\udf10 Visualizer 1.0","text":"<p>We built an actual UI. Because sometimes you want to click buttons instead of writing code:</p> <ul> <li>Interactive web interface for parameter tuning</li> <li>Launch with <code>chunklet visualize</code></li> <li>Works with all chunker types</li> </ul>"},{"location":"whats-new/#more-file-formats","title":"\ud83d\udcc1 More File Formats","text":"<p>ODT, CSV, and Excel (.xlsx) \u2014 added in this release. Because apparently plain text wasn't enough for some people.</p>"},{"location":"whats-new/#chunklet-v200","title":"Chunklet v2.0.0","text":""},{"location":"whats-new/#the-big-rewrite-aka-we-broke-everything","title":"\ud83d\ude80 The Big Rewrite (aka \"We Broke Everything\")","text":"<p>We rewrote the whole thing. You're welcome? Here's what changed:</p> <ul> <li>\ud83d\uddc3 New classes \u2014 PlainTextChunker, DocumentChunker, CodeChunker</li> <li>\ud83c\udf0d 50+ languages \u2014 because the world has more than English</li> <li>\ud83d\udcc4 Document formats \u2014 PDF, DOCX, EPUB, HTML, etc.</li> <li>\ud83d\udcbb Code understanding \u2014 actual code chunking, not just \"split by lines like a savage\"</li> <li>\ud83c\udfaf New constraints \u2014 <code>max_section_breaks</code> and <code>max_lines</code> for finer control</li> <li>\u26a1 Memory efficient batch \u2014 generators in batch methods so your RAM doesn't cry</li> </ul>"},{"location":"whats-new/#want-more-details","title":"\ud83d\uddfa\ufe0f Want More Details?","text":"<p>The changelog has everything. We're not gonna repeat it here.</p>"},{"location":"getting-started/cli/","title":"Chunklet CLI","text":"<p>Meet <code>chunklet</code>, your CLI companion for text processing! From sentence splitting to smart chunking to interactive visualization \u2014 it's all here.</p> <p><code>chunklet</code> vs <code>chunklet-py</code></p> <p>The CLI command is <code>chunklet</code> (kept for backward compatibility), while the Python package is named <code>chunklet-py</code> to avoid naming conflicts with other packages.</p> <p>Quick help:</p> <pre><code>chunklet --version\nchunklet --help\nchunklet split --help\nchunklet chunk --help\nchunklet visualize --help\n</code></pre>"},{"location":"getting-started/cli/#the-split-command-precision-sentence-segmentation","title":"The <code>split</code> Command: Precision Sentence Segmentation \u2702\ufe0f","text":"<p>Need to break down text into individual sentences with surgical precision? The <code>split</code> command is your go-to! It leverages <code>chunklet</code>'s powerful <code>SentenceSplitter</code> to give you clean, segmented sentences.</p>"},{"location":"getting-started/cli/#quick-facts-for-split","title":"Quick Facts for <code>split</code>","text":"<ul> <li>Operates on a raw text string or a single file (<code>--source</code>).</li> <li>Outputs sentences separated by newline characters.</li> <li>Perfect for preprocessing text before more complex chunking.</li> </ul> Flag Description Default <code>&lt;TEXT&gt;</code> The input text to split. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path to a single file to read input from. Cannot be a directory. None <code>--destination, -d &lt;PATH&gt;</code> Path to a single file to write the segmented sentences. If not provided, output goes to STDOUT. STDOUT <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). Use 'auto' for automatic detection. auto <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#scenarios-splitting-like-a-pro","title":"Scenarios: Splitting Like a Pro!","text":""},{"location":"getting-started/cli/#scenario-1-splitting-text-directly-and-multilingually","title":"Scenario 1: Splitting Text Directly (and Multilingually!)","text":"<p>Segment a direct text input containing multiple languages into individual sentences, leveraging automatic language detection.</p> <pre><code>chunklet split \"This is the first sentence. Here is the second sentence, in French. C'est la vie! \u00bfC\u00f3mo est\u00e1s?\" --lang auto\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-splitting-a-file-and-saving-the-output","title":"Scenario 2: Splitting a File and Saving the Output","text":"<p>Process a document and save its segmented sentences to a new file. Easy peasy!</p> <pre><code>chunklet split --source my_novel_chapter.txt --destination sentences.txt --lang en\n</code></pre>"},{"location":"getting-started/cli/#the-chunk-command-your-intelligent-chunking-workhorse","title":"The <code>chunk</code> Command: Your Intelligent Chunking Workhorse!","text":"<p>The <code>chunk</code> command is where the real magic happens! It's your versatile tool for breaking down text, documents, and even code into RAG-ready chunks. The \"flavor\" of chunking (plain text, document, or code) is determined by the flags you provide.</p>"},{"location":"getting-started/cli/#key-flags-for-chunk-the-essentials","title":"Key Flags for <code>chunk</code> (The Essentials!)","text":"Flag Description Default <code>&lt;TEXT&gt;</code> The input text to chunk. If not provided, <code>--source</code> must be used. None <code>--source, -s &lt;PATH&gt;</code> Path(s) to one or more files or directories to read input from. Repeat for multiple sources (e.g., <code>-s file1.txt -s dir/</code>). None <code>--destination, -d &lt;PATH&gt;</code> Path to a file (writes JSON for <code>.json</code> extensions or existing files) or directory (writes separate files) to write the chunks. If a non-JSON file exists, a warning is shown and JSON is written. If not provided, output goes to STDOUT. STDOUT <code>--max-tokens</code> Maximum number of tokens per chunk. Applies to all chunking strategies. (Must be &gt;= 12) None <code>--max-sentences</code> Maximum number of sentences per chunk. Applies to DocumentChunker. (Must be &gt;= 1) None <code>--max-section-breaks</code> Maximum number of section breaks per chunk. Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and  tags. Applies to DocumentChunker. (Must be &gt;= 1) None <code>--overlap-percent</code> Percentage of overlap between chunks (0-85). Applies to DocumentChunker. 20.0 <code>--offset</code> Starting sentence offset for chunking. Applies to DocumentChunker. 0 <code>--lang</code> Language of the text (e.g., 'en', 'fr', 'auto'). auto <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for extra insights. False"},{"location":"getting-started/cli/#general-text-document-chunking-default-or-with-doc","title":"General Text &amp; Document Chunking (Default or with <code>--doc</code>) \ud83d\udcc4","text":"<p>This is your bread-and-butter chunking for everyday text and diverse document types.</p> <ul> <li>Default Behavior: If neither <code>--doc</code> nor <code>--code</code> is specified, <code>chunklet</code> uses the DocumentChunker for direct text input. The <code>DocumentChunker</code> is designed to transform unruly text into perfectly sized, context-aware chunks.</li> <li>Document Power-Up: Activate the DocumentChunker with the <code>--doc</code> flag to process <code>.pdf</code>, <code>.docx</code>, <code>.odt</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, <code>.csv</code>, '.xlsx' and <code>.rtf</code> files! It intelligently extracts text and then applies the same robust chunking logic.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-document-power-up","title":"Key Flags for Document Power-Up","text":"Flag Description Default <code>--doc</code> Activate the <code>DocumentChunker</code> for multi-format file processing. False <code>--n-jobs</code> Number of parallel jobs for batch processing. (None uses all available cores) None <code>--on-errors</code> How to handle errors during batch processing: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise"},{"location":"getting-started/cli/#scenarios-text-document-chunking-in-action","title":"Scenarios: Text &amp; Document Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-basic-text-chunking-with-token-limits-and-overlap","title":"Scenario 1: Basic Text Chunking with Token Limits and Overlap","text":"<p>Chunk a long text string into segments, ensuring no chunk exceeds 200 tokens, with a healthy 15% overlap for context.</p> <pre><code>chunklet chunk \"The quick brown fox jumps over the lazy dog. This is the first sentence. The second sentence is a bit longer. And this is the third one. Finally, the fourth sentence concludes our example. The last sentence is here to finish the text.\"\n  --max-tokens 200 \\\n  --overlap-percent 15\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-chunking-a-pdf-document-with-sentence-and-section-break-limits","title":"Scenario 2: Chunking a PDF Document with Sentence and Section Break Limits","text":"<p>Process a PDF document, ensuring chunks are no more than 10 sentences or 2 section breaks, and save the output to a file.</p> <pre><code>chunklet chunk --doc --source my_report.pdf \\\n  --max-sentences 10 \\\n  --max-section-breaks 2 \\\n  --destination processed_report_chunks.txt\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-batch-processing-a-directory-of-documents-with-error-handling","title":"Scenario 3: Batch Processing a Directory of Documents (with Error Handling!)","text":"<p>Process all supported documents within a directory, saving the chunks to a new folder. If any file causes an error, <code>chunklet</code> will gracefully skip it and continue!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/my/project_docs \\\n  --destination ./processed_chunks \\\n  --n-jobs 4 \\\n  --on-errors skip \\\n  --max-tokens 1024 \\\n  --metadata # Don't forget your metadata!\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-a-text-file-with-a-specific-language-and-metadata","title":"Scenario 4: Chunking a Text File with a Specific Language and Metadata","text":"<p>Chunk a French text file, limiting by tokens, and include all the juicy metadata for later analysis.</p> <pre><code>chunklet chunk --source french_article.txt \\\n  --lang fr \\\n  --max-tokens 300 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#code-chunking-with-code","title":"Code Chunking (with <code>--code</code>) \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>For the developers, by the developers! The CodeChunker is a language-agnostic wizard that breaks your source code into semantically meaningful blocks (functions, classes, etc.). Activate it with the <code>--code</code> flag.</p> <ul> <li>Heads Up! This mode is primarily token-based. <code>--max-sentences</code>, <code>--max-section-breaks</code>, and <code>--overlap-percent</code> are generally ignored here, as code structure takes precedence.</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-code-chunking","title":"Key Flags for Code Chunking","text":"Flag Description Default <code>--code</code> Activate the <code>CodeChunker</code> for structurally-aware code segmentation. False <code>--max-lines</code> Maximum number of lines per chunk. (Must be &gt;= 5) None <code>--max-functions</code> Maximum number of functions per chunk. (Must be &gt;= 1) None <code>--docstring-mode</code> Docstring processing strategy: <code>summary</code> (first line), <code>all</code>, or <code>excluded</code>. all <code>--strict</code> If <code>True</code>, raise an error when structural blocks exceed <code>--max-tokens</code>. If <code>False</code>, split oversized blocks. True <code>--include-comments</code> Include comments in output chunks. True"},{"location":"getting-started/cli/#scenarios-code-chunking-in-action","title":"Scenarios: Code Chunking in Action!","text":""},{"location":"getting-started/cli/#scenario-1-chunking-a-single-python-file-excluding-comments","title":"Scenario 1: Chunking a Single Python File, Excluding Comments","text":"<p>Get a clean, comment-free view of your code's structure. Perfect for quick reviews!</p> <pre><code>chunklet chunk --code --source my_script.py \\\n  --max-tokens 512 \\\n  --include-comments False\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-chunking-a-codebase-allowing-oversized-blocks","title":"Scenario 2: Batch Chunking a Codebase, Allowing Oversized Blocks","text":"<p>Process an entire code repository, letting <code>chunklet</code> split any functions or classes that are just too long, and save everything to a dedicated folder.</p> <pre><code>chunklet chunk --code \\\n  --source ./my_awesome_repo \\\n  --destination ./code_chunks \\\n  --max-tokens 1024 \\\n  --strict False \\\n  --n-jobs 8 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-extracting-function-summaries-docstring-mode-summary","title":"Scenario 3: Extracting Function Summaries (Docstring Mode: Summary)","text":"<p>Focus on the \"what\" of your functions by only including the first line of their docstrings.</p> <pre><code>chunklet chunk --code --source utils.py \\\n  --docstring-mode summary \\\n  --max-functions 1 \\\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-chunking-by-lines-and-functions-for-granular-control","title":"Scenario 4: Chunking by Lines and Functions for Granular Control","text":"<p>For super-fine-grained control, chunk a file by both maximum lines and maximum functions per chunk.</p> <pre><code>chunklet chunk --code --source main.go \\\n  --max-lines 100 \\\n  --max-functions 2 \\\n  --max-tokens 700\n</code></pre>"},{"location":"getting-started/cli/#advanced-system-hooks","title":"\ud83d\udee0\ufe0f Advanced System Hooks","text":"<p>These flags are your secret weapons for scaling up operations, integrating with external tools, and getting the most out of your chunked data. They apply to the <code>chunk</code> command.</p>"},{"location":"getting-started/cli/#system-hook-flags","title":"System Hook Flags","text":"Flag Description Default <code>--tokenizer-command</code> A shell command string for token counting. It must take text via STDIN and output the integer count via STDOUT. None <code>--tokenizer-timeout, -t</code> Timeout in seconds for the tokenizer command (default: no timeout). None <code>--n-jobs</code> Number of parallel processes to use during batch operations. (None uses all available CPU cores) None <code>--on-errors</code> Defines batch error handling: <code>raise</code> (stop), <code>skip</code> (ignore file, continue), or <code>break</code> (halt, return partial result). raise <code>--metadata</code> Include rich metadata (source, span, chunk num, etc.) in the output. If <code>--destination</code> is a directory, metadata is saved as separate <code>.json</code> files; otherwise, it's included inline in the output. False <code>--verbose, -v</code> Enable verbose logging for debugging or process detail. False"},{"location":"getting-started/cli/#scenarios-unleashing-advanced-power","title":"Scenarios: Unleashing Advanced Power!","text":""},{"location":"getting-started/cli/#scenario-1-verbose-debugging-for-a-single-file","title":"Scenario 1: Verbose Debugging for a Single File","text":"<p>When things get tricky, crank up the verbosity to see exactly what <code>chunklet</code> is doing under the hood while chunking a specific file.</p> <pre><code>chunklet chunk --source problematic_file.txt \\\n  --max-tokens 100 \\\n  --verbose\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-batch-processing-with-parallelism-and-error-skipping","title":"Scenario 2: Batch Processing with Parallelism and Error Skipping","text":"<p>Process a large collection of diverse documents, leveraging all your CPU cores, and gracefully skip any problematic files without halting the entire operation. Plus, get all the metadata!</p> <pre><code>chunklet chunk --doc \\\n  --source /path/to/massive_document_archive \\\n  --destination ./final_chunks \\\n  --n-jobs -1 # Use all available cores!\n  --on-errors skip \\\n  --max-tokens 512 \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#scenario-3-processing-multiple-specific-files-with-advanced-hooks","title":"Scenario 3: Processing Multiple Specific Files with Advanced Hooks","text":"<p>Process a selection of individual files, explicitly listing each one, and apply advanced chunking parameters. This demonstrates how to handle a non-directory batch of files, ensuring each is processed with metadata and error handling.</p> <pre><code>chunklet chunk --doc \\\n  --source my_document.pdf \\\n  --source another_report.docx \\\n  --source plain_text_notes.txt \\\n  --destination ./processed_specific_files \\\n  --max-tokens 700 \\\n  --metadata \\\n  --on-errors skip\n</code></pre>"},{"location":"getting-started/cli/#scenario-4-custom-token-counting-with-an-external-script","title":"Scenario 4: Custom Token Counting with an External Script","text":"<p>Align <code>chunklet</code>'s chunk sizes perfectly with your LLM's token limits using any external tokenizer you can imagine! Optionally set a timeout:</p> <pre><code>chunklet chunk --text \"Your text here\" \\\n  --max-tokens 50 \\\n  --tokenizer-command \"python ./my_tokenizer.py\" \\\n  --tokenizer-timeout 30\n</code></pre> <p>Optional flag</p> <p>Using <code>--text</code> is optional since when using <code>chunk</code> without <code>--doc</code> or <code>--code</code> flags, it defaults to text-based chunking.</p> <p>\ud83d\udc49 See the Custom Tokenizers guide for how to create a tokenizer script in any language.</p>"},{"location":"getting-started/cli/#scenario-5-saving-chunks-as-json-with-metadata","title":"Scenario 5: Saving Chunks as JSON with Metadata","text":"<p>Save processed chunks directly as a JSON file for easy parsing and integration:</p> <pre><code>chunklet chunk --doc \\\n  --source document.pdf \\\n  --destination chunks.json \\\n  --metadata\n</code></pre>"},{"location":"getting-started/cli/#diving-deeper-into-metadata","title":"Diving Deeper into Metadata","text":"<p>Want to know exactly what kind of rich context <code>chunklet</code> attaches to your chunks? From source paths and character spans to document-specific properties and code AST details.</p> <p>\ud83d\udc49 Head over to the Metadata in Chunklet-py guide to unlock all its secrets!</p>"},{"location":"getting-started/cli/#the-visualize-command-your-interactive-chunk-playground","title":"The <code>visualize</code> Command: Your Interactive Chunk Playground! \ud83c\udfae","text":"<p>Ready to see your chunking in action with a beautiful web interface? The <code>visualize</code> command launches Chunklet's interactive web visualizer - perfect for experimenting with parameters, seeing real-time results, and fine-tuning your chunking strategies!</p> <p>Want programmatic control?</p> <p>For code-based usage and detailed technical information, check out the Text Chunk Visualizer documentation.</p> <p>This command starts a local web server that gives you:</p> <ul> <li>Live parameter tuning - Adjust chunking settings and see results instantly</li> <li>Visual chunk exploration - See exactly how your text gets divided</li> <li>Multiple chunking modes - Try plain text, document, and code chunking all in one place</li> <li>Custom tokenizers - Plug in your own token counting for precise control</li> </ul>"},{"location":"getting-started/cli/#key-flags-for-visualize","title":"Key Flags for <code>visualize</code>","text":"Flag Description Default <code>--host, -h</code> Host IP to bind the server (use <code>0.0.0.0</code> for network access) 127.0.0.1 <code>--port, -p</code> Port number for the server 8000 <code>--tokenizer-command</code> Shell command for custom token counting None <code>--tokenizer-timeout, -t</code> Timeout in seconds for the tokenizer command (default: no timeout) None <code>--headless</code> Run without opening browser automatically False"},{"location":"getting-started/cli/#getting-started-with-visualization","title":"Getting Started with Visualization! \ud83d\udda5","text":""},{"location":"getting-started/cli/#scenario-1-basic-visualizer-launch","title":"Scenario 1: Basic Visualizer Launch","text":"<p>Fire up the visualizer on the default port and let it open your browser automatically:</p> <pre><code>chunklet visualize\n</code></pre>"},{"location":"getting-started/cli/#scenario-2-custom-port-and-host","title":"Scenario 2: Custom Port and Host","text":"<p>Run on a specific port and host (great for accessing from other devices):</p> <pre><code>chunklet visualize --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"getting-started/cli/#headless-cli","title":"Scenario 3: Headless Mode with Custom Tokenizer","text":"<p>Run in the background with your own token counting script:</p> <pre><code>chunklet visualize --headless \\\n  --tokenizer-command \"python my_tokenizer.py\" \\\n  --tokenizer-timeout 30\n</code></pre> <p>See the Custom Tokenizers guide for how to create a tokenizer script in any language.</p> <p>The visualizer will show you the URL to access it in your browser. Press <code>Ctrl+C</code> to stop the server when you're done!</p> <p>REST API for Headless Automation! \ud83e\udd16</p> <p>When running in headless mode, you can use the visualizer's REST API to programmatically upload files, chunk content, and retrieve results without any web interface! Perfect for automation scripts, CI/CD pipelines, or integrating chunking into your applications.</p> <p>See the Headless/REST API Usage section for complete examples of programmatic file processing.</p> <p>Pro Visualization Tips</p> <ul> <li>Use <code>--headless false</code> (or just omit it) to auto-open your browser</li> <li>Try different ports if 8000 is already in use</li> <li>Experiment with different chunking modes - text, document, and code all in one interface!</li> </ul> API Reference <p>For a deep dive into the <code>chunklet</code> CLI, its commands, and all the nitty-gritty details, check out the full API documentation</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Ready to get Chunklet-py up and running? Fantastic! This guide will walk you through the installation process, making it as smooth as possible.</p> <p>Requirements</p> <p>Chunklet-py requires Python 3.10 or newer. We recommend using Python 3.11+ for the best experience.</p> <p>chunklet-py (aka chunklet)</p> <p>The old <code>chunklet</code> package is no longer maintained. Use <code>chunklet-py</code> to get the latest version.</p>"},{"location":"getting-started/installation/#the-easy-way","title":"The Easy Way","text":"<p>The most straightforward method to install Chunklet-py is by using <code>pip</code>:</p> <pre><code># Install and verify version\npip install chunklet-py\nchunklet --version\n</code></pre> <p>And that's all there is to it! You're now ready to start using Chunklet-py.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Chunklet-py offers optional dependencies to unlock additional functionalities, such as document processing or code chunking. You can install these extras using the following syntax:</p> <ul> <li>Structured Documents: For handling <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, and other document formats:     <pre><code>pip install \"chunklet-py[structured-document]\"\n</code></pre></li> <li>Code Chunking: For Language-agnostic code chunking features:     <pre><code>pip install \"chunklet-py[code]\"\n</code></pre></li> <li>Visualization: For the interactive web-based chunk visualizer:     <pre><code>pip install \"chunklet-py[visualization]\"\n</code></pre></li> <li>All Extras: To install all optional dependencies:     <pre><code>pip install \"chunklet-py[all]\"\n</code></pre></li> </ul>"},{"location":"getting-started/installation/#the-alternative-way","title":"The Alternative Way","text":"<p>For those who prefer to build from source, you can clone the repository and install it manually. This method allows for direct modification of the source code and installation of all optional features:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\npip install .[all]\n</code></pre> <p>But why would you want to do that? The easy way is so much easier.</p>"},{"location":"getting-started/installation/#contributing-to-chunklet-py","title":"Contributing to Chunklet-py","text":"<p>Interested in helping make Chunklet-py even better? That's fantastic! Before you dive in, please take a moment to review our Contributing Guide. Here's how you can set up your development environment:</p> <pre><code>git clone https://github.com/speedyk-005/chunklet-py.git\ncd chunklet-py\n# For basic development (testing, linting)\npip install -e \".[dev]\"\n# For documentation development\npip install -e \".[docs]\"\n# For comprehensive development (including all optional features)\npip install -e \".[dev-all]\"\n</code></pre> <p>These commands install Chunklet-py in \"editable\" mode, ensuring that any changes you make to the source code are immediately reflected. The <code>[dev]</code>, <code>[docs]</code>, and <code>[dev-all]</code> options include the necessary dependencies for specific development tasks.</p> <p>Now, go forth and code! And remember, good developers always write tests. (Even in a Python project, we appreciate all forms of excellent code examples!)</p>"},{"location":"getting-started/metadata/","title":"Metadata in Chunklet-py: Your Chunk's Story \ud83d\udcd6","text":"<p>Ever wondered where your chunks come from and what makes them tick? \ud83e\udd14 Chunklet-py's metadata system tells the whole story! Each chunk comes with rich contextual information about its origin, location, and characteristics. Think of metadata as your chunk's detailed biography - the who, what, when, and where of your text.</p> <p>Every chunk is wrapped in a handy <code>Box</code> object with a <code>metadata</code> attribute. This metadata dictionary is your treasure trove of chunk insights. Access it easily with dot notation (<code>chunk.metadata</code>) or dictionary-style (<code>chunk[\"metadata\"]</code>) - your choice!</p>"},{"location":"getting-started/metadata/#common-metadata","title":"Common Metadata: The Essentials \ud83d\udccb","text":"<p>No matter which chunker you use, every chunk includes these fundamental metadata fields. Think of them as your chunk's basic information - the essentials you need to know.</p> <ul> <li><code>chunk_num</code> (int): Your chunk's sequential ID number within each source - perfect for keeping things organized</li> <li><code>span</code> (tuple[int, int]): Character position coordinates showing exactly where this chunk sits in the original text</li> <li><code>source</code> (str): Where did this chunk come from? (The origin story!)<ul> <li>File processing: Absolute path to the file (for DocumentChunker or CodeChunker)</li> <li>CLI text input: <code>\"stdin\"</code> (because it came from standard input)</li> <li>Document chunker Text input: Only included if you provide it via <code>base_metadata</code> parameter</li> <li>CodeChunker edge cases: Might be <code>\"N/A\"</code> if the source can't be determined</li> </ul> </li> </ul>"},{"location":"getting-started/metadata/#documentchunker-metadata","title":"DocumentChunker Metadata: Rich &amp; Detailed \ud83d\udcda","text":"<p>The <code>DocumentChunker</code> provides comprehensive metadata for both text and file inputs. The metadata varies based on your input type - think of it as your chunk's detailed biography! \ud83d\udc47</p>"},{"location":"getting-started/metadata/#text-input","title":"Text Input","text":"<p>Keeps things straightforward and clean. Your chunks include the essential Common Metadata fields (<code>chunk_num</code> and <code>span</code>). No frills, just the basics - perfect when you want clean, minimal metadata without any extra baggage. Additional metadata can be provided via the <code>base_metadata</code> parameter.</p>"},{"location":"getting-started/metadata/#file-input","title":"File Input","text":"<p>Need more context? File input's got you covered! Provides comprehensive metadata beyond the basics - revealing detailed insights about each file's properties and history:</p> <p>Universal Fields (for multi-section docs):</p> <ul> <li><code>section_count</code> (int): Total number of sections in the document (pages, chapters, etc.)</li> <li><code>curr_section</code> (int): Which section this chunk belongs to</li> </ul> <p>File-Type Specific Information:</p> <ul> <li>PDF Files: Includes <code>title</code>, <code>author</code>, <code>creator</code>, <code>producer</code>, <code>publisher</code>, <code>created</code>, <code>modified</code>, and <code>page_count</code> fields (powered by pdfminer.six)</li> <li>EPUB Files: Dublin Core metadata including <code>title</code>, <code>creator</code>, <code>contributor</code>, <code>publisher</code>, <code>date</code>, and `rights</li> <li>DOCX Files: Core properties like <code>title</code>, <code>author</code>, <code>publisher</code>, <code>last_modified_by</code>, <code>created</code>, <code>modified</code>, <code>rights</code>, and <code>version</code></li> <li>ODT Files: Dublin Core and OpenDocument metadata including <code>title</code>, <code>creator</code>, <code>initial_creator</code>, <code>created</code>, <code>chapter</code>, and <code>author</code> (powered by odfpy)</li> </ul> <p>Safety First with Optional Fields!</p> <p>These metadata fields are optional - not every document fills them out. Use <code>chunk.metadata.get(\"author\")</code> instead of <code>chunk.metadata[\"author\"]</code> to avoid <code>KeyError</code>s when a field is missing. Better safe than sorry! \ud83d\ude09</p>"},{"location":"getting-started/metadata/#codechunker-metadata","title":"CodeChunker Metadata: Code Intelligence \ud83d\udcbb","text":"<p>The <code>CodeChunker</code> provides code-specific insights beyond basic metadata. It helps you understand the structural context of each chunk - perfect for tracking where your code elements originated! \ud83d\udd0d</p> <p>Code-Specific Information:</p> <ul> <li><code>tree</code> (str): Abstract syntax tree representation showing structural relationships within the chunk</li> <li><code>start_line</code> (int): Line number where this chunk begins in the original file</li> <li><code>end_line</code> (int): Line number where this chunk ends in the original file</li> </ul> <p>Automatically included in every <code>Box</code> object when chunking code, helping you understand which functions, classes, or code blocks are in each chunk.</p>"},{"location":"getting-started/metadata/#cli-metadata-output-command-line-insights","title":"CLI Metadata Output: Command Line Insights \ud83d\udda5\ufe0f","text":"<p>The <code>chunklet</code> CLI adapts metadata output based on your input type and flags. Think of it as your CLI's helpful companion that provides just the right context!</p> <p>Metadata Control: The <code>--metadata</code> flag gives you control over what gets included.</p> <ul> <li>With <code>--metadata</code>: Your chunks come with their full context - metadata appears alongside content, either printed to stdout or saved in <code>.json</code> files with <code>--destination</code></li> <li>Without <code>--metadata</code>: Just the chunk content - clean and simple when you want to focus purely on the text</li> </ul> <p>Metadata by Input Type:</p> <ul> <li>Direct Text Input (<code>chunklet chunk \"Your text...\"</code>): Uses <code>DocumentChunker</code> with essential Common Metadata fields (<code>chunk_num</code>, <code>span</code>, ...) and <code>source</code> set to <code>\"stdin\"</code></li> <li>Document Processing (<code>chunklet chunk --doc --source document.pdf</code>): <code>DocumentChunker</code> provides rich document metadata including Common Metadata plus file-specific details (PDF titles, EPUB creators, DOCX authors, ODT creators) as detailed in DocumentChunker Metadata</li> <li>Code Processing (<code>chunklet chunk --code --source code.py</code>): <code>CodeChunker</code> includes structural information with Common Metadata and code-specific fields like <code>tree</code>, <code>start_line</code>, <code>end_line</code> as described in CodeChunker Metadata</li> </ul> <p>The CLI automatically provides the most relevant metadata for your use case - making chunk analysis both powerful and intuitive. Smart and simple! \ud83c\udfaf</p>"},{"location":"getting-started/programmatic/","title":"Overview","text":"<p>Welcome to the programmatic interface! This is where you integrate Chunklet-py's chunking capabilities directly into your Python apps. Building RAG pipelines, data processing workflows, or custom AI solutions? We've got you covered.</p> <ul> <li> <p> Sentence Splitter</p> <p>Precisely splits text into semantically meaningful sentences across 50+ languages with intelligent detection and complex structure handling.</p> <p>Essential for preparing clean text data for NLP tasks, LLMs, and any application that needs accurate sentence boundaries.</p> <p> Learn More</p> </li> <li> <p> Document Chunker</p> <p>Transforms plain text and diverse document formats (<code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, <code>.rtf</code>, <code>.odt</code>, <code>.csv</code>, and <code>.xlsx</code>) into perfectly sized, context-aware chunks with flexible composable constraints and intelligent overlap for optimal LLM and embedding performance.</p> <p>Perfect for RAG systems, document analysis, and any workflow that needs smart text segmentation with full control over chunk sizes.</p> <p> Learn More</p> </li> <li> <p> Code Chunker</p> <p>Intelligently chunks source code while preserving logical structure and context and maintaining code semantics across functions, classes, and modules.</p> <p>Language-agnostic and lightweight - ideal for code understanding and generation tasks, analysis, documentation, and AI model training.</p> <p> Learn More</p> </li> <li> <p> Text Chunk Visualizer</p> <p>Interactive web interface for real-time chunk visualization, parameter tuning, and exploring chunking results with live feedback.</p> <p>Perfect for experimenting with chunking strategies, comparing different settings, and understanding how your text gets processed.</p> <p> Learn More</p> </li> </ul> <p>Pick a card below to get started! \ud83d\udcc7</p>"},{"location":"getting-started/programmatic/code_chunker/","title":"Code Chunker","text":""},{"location":"getting-started/programmatic/code_chunker/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py[code]\n</code></pre> <p>This installs all the code processing dependencies needed for language-agnostic code chunking! \ud83d\udcbb</p>"},{"location":"getting-started/programmatic/code_chunker/#code-chunker-your-code-intelligence-sidekick","title":"Code Chunker: Your Code Intelligence Sidekick!","text":"<p>Got a massive codebase that's hard to navigate? The <code>CodeChunker</code> transforms tangled functions and classes into clean, understandable chunks that actually make sense.</p> <p>It uses pattern-based line-by-line processing to identify code structures \u2014 no heavy parsers needed. Lightweight yet surprisingly accurate across 30+ languages.</p>"},{"location":"getting-started/programmatic/code_chunker/#code-chunker-superpowers","title":"Code Chunker Superpowers! \u26a1","text":"<p>The <code>CodeChunker</code> comes packed with smart features for your coding adventures:</p> <ul> <li>Multi-Language Support: Works with 30+ languages out of the box \u2014 Python, JavaScript, Java, C++, Go, Rust, PHP, and more! One library to rule them all! \ud83c\udf0d</li> <li>Convention-Aware: Assumes your code plays by the rules \u2014 no full language parsers needed for surprisingly accurate results! \ud83c\udfaf</li> <li>Flexible Composable Constraints: Ultimate control over code segmentation! Mix and match limits based on tokens, lines, or functions for perfect chunks. \ud83c\udf9b\ufe0f</li> <li>Customizable Token Counting: Plug in your own token counter for perfect alignment with different LLMs. Because one size definitely doesn't fit all models! \ud83e\udd16</li> <li>Annotation-Aware: Keeps comments and docstrings intact \u2014 your code's story stays complete! \ud83d\udcdd</li> <li>Strict Mode Control: By default keeps functions and classes together even if large. Set <code>strict=False</code> for more flexibility. No more orphaned code! \ud83d\udee1\ufe0f</li> <li>Namespace Hierarchy Tracking: Builds a tree of your code's structure \u2014 functions, classes, namespaces \u2014 all tracked for accurate metadata \ud83c\udf33</li> <li>Memory-Conscious Operation: Handles massive codebases efficiently by yielding chunks one at a time. Your RAM will thank you later! \ud83d\udcbe</li> <li>Bulk Processing Powerhouse: Got a mountain of code files to conquer? No problem! This powerhouse efficiently processes multiple files in parallel. \ud83d\udcda\u26a1</li> </ul>"},{"location":"getting-started/programmatic/code_chunker/#code-constraints-your-chunking-control-panel","title":"Code Constraints: Your Chunking Control Panel! \ud83c\udf9b\ufe0f","text":"<p><code>CodeChunker</code> works primarily in structural mode, letting you set chunk boundaries based on code structure. Fine-tune your chunks with these constraint options:</p> Constraint Value Requirement Description <code>max_tokens</code> <code>int &gt;= 12</code> Token budget master! Code blocks exceeding this limit get split at smart structural boundaries. <code>max_lines</code> <code>int &gt;= 5</code> Line count commander! Perfect for managing chunks where line numbers often match logical code units. <code>max_functions</code> <code>int &gt;= 1</code> Function group guru! Keeps related functions together or splits them when you hit the limit. <p>Constraint Must-Have!</p> <p>You must specify at least one limit (<code>max_tokens</code>, <code>max_lines</code>, or <code>max_functions</code>) when using <code>chunk_text</code>, <code>chunk_file</code>, <code>chunk_texts</code>, or <code>chunk_files</code>. Skip this and you'll get an <code>InvalidInputError</code> - rules are rules!</p> <p>The <code>CodeChunker</code> has four main methods: <code>chunk_text</code>, <code>chunk_file</code>, <code>chunk_texts</code>, and <code>chunk_files</code>. <code>chunk_text</code> and <code>chunk_file</code> return a list of <code>Box</code> objects, while <code>chunk_texts</code> and <code>chunk_files</code> are memory-friendly generators that yield chunks one by one. Each <code>Box</code> has <code>content</code> (str) and <code>metadata</code> (dict). For metadata details, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#single-run","title":"Single Run:","text":"<p>Let's see <code>CodeChunker</code> in action with a single code input. It provides two methods:</p> <ul> <li><code>chunk_text()</code> - accepts raw code as a string</li> <li><code>chunk_file()</code> - accepts a file path as a string or <code>pathlib.Path</code> object</li> </ul>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-lines-line-count-control","title":"Chunking by Lines: Line Count Control! \ud83d\udccf","text":"<p>Ready to chunk code by line count? This gives you predictable, size-based chunks:</p> <pre><code>from chunklet.code_chunker import CodeChunker\n\nPYTHON_CODE = '''\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n'''\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk_text(\n    code=PYTHON_CODE,                \n    max_lines=10,               # (1)!\n    include_comments=True,      # (2)!\n    docstring_mode=\"all\",       # (3)!\n    strict=False,               # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> <ol> <li>Sets the maximum number of lines per chunk. If a code block exceeds this limit, it will be split.</li> <li>Set to True to include comments in the output chunks. Defaults to True.</li> <li><code>docstring_mode=\"all\"</code> ensures that complete docstrings, with all their multi-line details, are preserved in the code chunks. Other options are <code>\"summary\"</code> to include only the first line, or <code>\"excluded\"</code> to remove them entirely. Default is \"all\".</li> <li>When <code>strict=False</code>, structural blocks (like functions or classes) that exceed the limit set will be split into smaller chunks. If <code>strict=True</code> (default), a <code>TokenLimitError</code> would be raised instead.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\n\nMetadata:\n    chunk_num: 1\n    tree: global\n    start_line: 1\n    end_line: 7\n    span: (0, 38)\n    source: N/A\n\n--- Chunk 2 ---\nContent:\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n\nMetadata:\n    chunk_num: 2\n    tree: global\n    \u2514\u2500 class Calculator\n    start_line: 8\n    end_line: 14\n    span: (38, 192)\n    source: N/A\n\n--- Chunk 3 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n\nMetadata:\n    chunk_num: 3\n    tree: global\n    \u2514\u2500 class Calculator\n       \u2514\u2500 def add(\n    start_line: 15\n    end_line: 23\n    span: (192, 444)\n    source: N/A\n\n--- Chunk 4 ---\nContent:\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\n\nMetadata:\n    chunk_num: 4\n    tree: global\n    \u251c\u2500 class Calculator\n    \u2502  \u2514\u2500 def multiply(\n    \u2514\u2500 def standalone_function(\n    start_line: 24\n    end_line: 30\n    span: (444, 603)\n    source: N/A\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>CodeChunker</code>: <pre><code>chunker = CodeChunker(verbose=True)\n</code></pre></p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-tokens-token-budget-master","title":"Chunking by Tokens: Token Budget Master! \ud83e\ude99","text":"<p>Here's how you can use <code>CodeChunker</code> to chunk code by the number of tokens:</p> <pre><code># Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nchunks = chunker.chunk_text(\n    code=PYTHON_CODE,                \n    max_tokens=50,                        \n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\\\n{chunk.content}\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"{k}: {v}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n\nstart_line: 1\nend_line: 14\nspan: (0, 192)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\n    def multiply(self, x, y):\n        # Multiply two numbers\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u251c\u2500 def add(\n   \u2514\u2500 def multiply(\n\nstart_line: 15\nend_line: 27\nspan: (192, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to any chunking method (e.g., <code>chunker.chunk_text(..., token_counter=my_tokenizer_function)</code>). If a <code>token_counter</code> is provided in both the constructor and the chunking method, the one in the method call will be used.</p>"},{"location":"getting-started/programmatic/code_chunker/#chunking-by-functions-function-group-guru","title":"Chunking by Functions: Function Group Guru! \ud83d\udc65","text":"<p>This constraint is useful when you want to ensure that each chunk contains a specific number of functions, helping to maintain logical code units.</p> <pre><code>chunks = chunker.chunk_text(\n    code=PYTHON_CODE,\n    max_functions=1,\n    include_comments=False,\n)\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\n\n\"\"\"\nModule docstring\n\"\"\"\n\nimport os\n\nclass Calculator:\n    \"\"\"\n    A simple calculator class.\n\n    A calculator that Contains basic arithmetic operations for demonstration purposes.\n    \"\"\"\n\n    def add(self, x, y):\n        \"\"\"Add two numbers and return result.\n\n        This is a longer description that should be truncated\n        in summary mode. It has multiple lines and details.\n        \"\"\"\n        result = x + y\n        return result\n\nMetadata:\nchunk_num: 1\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def add(\n\nstart_line: 1\nend_line: 23\nspan: (0, 444)\nsource: N/A\n\n--- Chunk 2 ---\nContent:\n    def multiply(self, x, y):\n\n        return x * y\n\nMetadata:\nchunk_num: 2\ntree: global\n\u2514\u2500 class Calculator\n   \u2514\u2500 def multiply(\n\nstart_line: 24\nend_line: 27\nspan: (444, 527)\nsource: N/A\n\n--- Chunk 3 ---\nContent:\ndef standalone_function():\n    \"\"\"A standalone function.\"\"\"\n    return True\n\nMetadata:\nchunk_num: 3\ntree: global\n\u2514\u2500 def standalone_function(\n\nstart_line: 28\nend_line: 30\nspan: (527, 603)\nsource: N/A\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#combining-multiple-constraints-mix-and-match-magic","title":"Combining Multiple Constraints: Mix and Match Magic! \ud83c\udfad","text":"<p>The real power of <code>CodeChunker</code> comes from combining multiple constraints. This allows for highly specific and granular control over how your code is chunked. Here are a few examples of how you can combine different constraints.</p> <pre><code>chunks = chunker.chunk_text(\n    PYTHON_CODE,\n    max_lines=8,\n    max_tokens=150,\n    max_functions=1\n)\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#batch-run-processing-multiple-code-inputs-like-a-pro","title":"Batch Run: Processing Multiple Code Inputs Like a Pro! \ud83d\udcda","text":"<p>While <code>chunk_text</code>/<code>chunk_file</code> is perfect for single code inputs, <code>chunk_texts</code> and <code>chunk_files</code> are your power players for processing multiple code inputs in parallel. They use memory-friendly generators so you can handle massive codebases with ease.</p> <ul> <li><code>chunk_texts()</code> - process multiple raw code strings</li> <li><code>chunk_files()</code> - process multiple file paths</li> </ul> <p>Given we have the following code snippets saved as individual files in a <code>code_examples</code> directory:</p>"},{"location":"getting-started/programmatic/code_chunker/#cpp_calculatorcpp","title":"cpp_calculator.cpp","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n\n// Function 1: Simple greeting\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\n// Function 2: Logic block\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1; // Error code\n    }\n    int result = a + b;\n    return result;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#javadataprocessorjava","title":"JavaDataProcessor.java","text":"<pre><code>package com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n    // Constructor\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n    // Method 1: Getter\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n    // Method 2: Core processing logic\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n        // Assume processing logic here\n        return true;\n    }\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#js_utilsjs","title":"js_utils.js","text":"<pre><code>// Utility function\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n// Main function with control flow\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n    // Loop structure\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n</code></pre>"},{"location":"getting-started/programmatic/code_chunker/#go_configgo","title":"go_config.go","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\n// Struct definition\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n// Function 1: Factory function\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n// Function 2: Method on the struct\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\\\n\", c.Timeout, c.Retries)\n}\n</code></pre> <p>We can process them all at once by providing a list of paths to the <code>chunk_files</code> method. Assuming these files are saved in a <code>code_examples</code> directory:</p> <pre><code>from chunklet.code_chunker import CodeChunker\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\n# Initialize the chunker\nchunker = CodeChunker(token_counter=simple_token_counter)\n\nsources = [\n    \"code_examples/cpp_calculator.cpp\",\n    \"code_examples/JavaDataProcessor.java\",\n    \"code_examples/js_utils.js\",\n    \"code_examples/go_config.go\",\n]\n\nchunks = chunker.chunk_files(\n    paths=sources,\n    max_tokens=50,\n    include_comments=False,\n    n_jobs=2,               # (1)!\n    on_errors=\"raise\",      # (2)!\n    show_progress=True,     # (3)!\n)\n\n# Output the results\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Content:\\n{chunk.content.strip()}\\n\")\n    print(\"Metadata:\")\n    for k,v in chunk.metadata.items():\n        print(f\"  {k}: {v}\")\n    print()\n</code></pre> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. Determines how errors during chunking are handled. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will be halt and partial result will be returned.  If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to view output <pre><code>Chunking ...:   0%|          | 0/4 [00:00, ?it/s]\n--- Chunk 1 ---\nContent:\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nvoid say_hello(std::string name) {\n    std::cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; std::endl;\n}\n\nint calculate_sum(int a, int b) {\n    if (a &lt; 0 || b &lt; 0) {\n        return -1;\n    }\n    int result = a + b;\n    return result;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n  start_line: 1\n  end_line: 17\n  span: (0, 329)\n  source: code_examples/cpp_calculator.cpp\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 2 ---\nContent:\nconst sanitizeInput = (input) =&gt; {\n    return input.trim().substring(0, 100);\n};\n\n\nfunction processArray(data) {\n    if (!data || data.length === 0) {\n        return 0;\n    }\n\n    let total = 0;\n\n    for (let i = 0; i &lt; data.length; i++) {\n        total += data[i];\n    }\n    return total;\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u2514\u2500 function processArray(\n  start_line: 1\n  end_line: 19\n  span: (0, 372)\n  source: code_examples/js_utils.js\n\n--- Chunk 3 ---\nContent:\npackage com.chunker.data;\n\npublic class DataProcessor {\n    private String sourcePath;\n\n\n    public DataProcessor(String path) {\n        this.sourcePath = path;\n    }\n\n\n    public String getPath() {\n        return this.sourcePath;\n    }\n\n\n    public boolean process() {\n        if (this.sourcePath.isEmpty()) {\n            return false;\n        }\n\n        return true;\n    }\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package com\n\u2514\u2500 public class DataProcessor\n   \u251c\u2500 public DataProcessor(\n   \u251c\u2500 public String getPath(\n   \u2514\u2500 public boolean process(\n\n  start_line: 1\n  end_line: 25\n  span: (0, 500)\n  source: code_examples/JavaDataProcessor.java\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2/4 [00:00, 19.73it/s]\n--- Chunk 4 ---\nContent:\npackage main\n\nimport (\n    \"fmt\"\n)\n\n\ntype Config struct {\n    Timeout int\n    Retries int\n}\n\n\nfunc NewConfig() Config {\n    return Config{\n        Timeout: 5000,\n        Retries: 3,\n    }\n}\n\n\nfunc (c *Config) displayInfo() {\n    fmt.Printf(\"Timeout: %dms, Retries: %d\\n\", c.Timeout, c.Retries)\n}\n\nMetadata:\n  chunk_num: 1\n  tree: global\n\u251c\u2500 package main\n\u251c\u2500 type Config\n\u2514\u2500 func NewConfig(\n\n  start_line: 1\n  end_line: 26\n  span: (0, 382)\n  source: code_examples/go_config.go\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00, 19.71it/s]\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>chunk_files</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p>"},{"location":"getting-started/programmatic/code_chunker/#separator-keeping-your-code-batches-organized","title":"Separator: Keeping Your Code Batches Organized! \ud83d\udccb","text":"<p>The <code>separator</code> parameter lets you add a custom marker that gets yielded after all chunks from a single code file are processed. Super handy for batch processing when you want to clearly separate chunks from different source files.</p> <p>note</p> <p><code>None</code> cannot be used as a separator.</p> <pre><code>from chunklet.code_chunker import CodeChunker\nfrom more_itertools import split_at\n\n# Helper function\ndef simple_token_counter(text: str) -&gt; int:\n    \"\"\"Simple Token Counter For Testing.\"\"\"\n    return len(text.split())\n\nSIMPLE_SOURCES = [\n    # Python: Simple Function Definition Boundary\n    '''\ndef greet_user(name):\n    \"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n    ''',\n\n    # C#: Simple Method and Class Boundary\n    '''\npublic class Utility\n{\n    // C# Method\n    public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n    '''\n]\n\nchunker = CodeChunker(token_counter=simple_token_counter)\ncustom_separator = \"---END_OF_SOURCE---\"\n\nchunks_with_separators = chunker.chunk_texts(\n    codes=SIMPLE_SOURCES,\n    max_tokens=20,\n    separator=custom_separator,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, code_chunks in enumerate(chunk_groups):\n    if code_chunks: # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in code_chunks:\n            print(f\"Content:\\n {chunk.content}\\n\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>Chunking ...:   0%|          | 0/2 [00:00, ?it/s]\n--- Chunks for Document 1 ---\nContent:\ndef greet_user(name):\n\"\"\"Returns a simple greeting string.\"\"\"\n    message = \"Welcome back, \" + name\n    return message\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 def greet_user(\\n', 'start_line': 1, 'end_line': 5, 'span': (0, 124), 'source': 'N/A'}\n\nChunking ...:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 1/2 [00:00,  9.48it/s]\n--- Chunks for Document 2 ---\nContent:\npublic class Utility\n{\n    // C# Method\n\nMetadata: {'chunk_num': 1, 'tree': 'global\\n\u2514\u2500 public class Utility\\n', 'start_line': 1, 'end_line': 4, 'span': (0, 41), 'source': 'N/A'}\nContent:\n     public int Add(int x, int y)\n    {\n        int sum = x + y;\n        return sum;\n    }\n}\n\nMetadata: {'chunk_num': 2, 'tree': 'global\\n\u2514\u2500 public class Utility\\n   \u2514\u2500 public int Add(\\n', 'start_line': 5, 'end_line': 10, 'span': (41, 133), 'source': 'N/A'}\n\nChunking ...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00,  1.92it/s]\n</code></pre> <p>What are the limitations of CodeChunker?</p> <p>While powerful, <code>CodeChunker</code> isn't magic! It assumes your code is reasonably well-behaved (syntactically conventional). Highly obfuscated, minified, or macro-generated sources might give it a headache. Also, nested docstrings or comment blocks can be a bit tricky for it to handle perfectly.</p>"},{"location":"getting-started/programmatic/code_chunker/#inspiration-the-code-behind-the-magic","title":"Inspiration: The Code Behind the Magic! \u2728","text":"<p>The <code>CodeChunker</code> draws inspiration from various projects and concepts in the field of code analysis and segmentation. These influences have shaped its design principles and capabilities:</p> <ul> <li>code_chunker by Camel AI</li> <li>code_chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> API Reference <p>For complete technical details on the <code>CodeChunker</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/document_chunker/","title":"Document Chunker","text":""},{"location":"getting-started/programmatic/document_chunker/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py\n</code></pre> <p>No extra dependencies needed - <code>DocumentChunker</code> is ready to roll right out of the box for plain text! \ud83d\ude80</p> <p>For document processing (PDFs, DOCX, EPUB, ODT, Excel, etc.), install the structured-document extra:</p> <pre><code>pip install chunklet-py[structured-document]\n</code></pre> <p>This installs all the document processing dependencies needed to handle PDFs, DOCX, EPUB, ODT, Excel, and more! \ud83d\udcda</p>"},{"location":"getting-started/programmatic/document_chunker/#taming-your-text-and-documents-with-precision","title":"Taming Your Text and Documents with Precision","text":"<p>Got a wall of text that's overwhelming? The <code>DocumentChunker</code> transforms unruly paragraphs into perfectly sized, context-aware chunks. Perfect for RAG systems and document analysis.</p> <p>It preserves meaning and flow \u2014 no confusing puzzle pieces.</p>"},{"location":"getting-started/programmatic/document_chunker/#where-documentchunker-really-shines","title":"Where <code>DocumentChunker</code> Really Shines","text":"<p>The <code>DocumentChunker</code> comes packed with smart features that make it your go-to text wrangling sidekick:</p> <ul> <li>Flexible Composable Constraints: Ultimate control over your chunks! Mix and match limits based on sentences, tokens, or section breaks (headings, horizontal rules, <code>&lt;details&gt;</code> tags). Craft exactly the chunk size you need with precision control! \ud83c\udfaf</li> <li>Intelligent Overlap: Adds smart overlaps between chunks so your text flows smoothly. No more jarring transitions that leave readers scratching their heads!</li> <li>Extensive Multilingual Support: Speaks over 50 languages fluently, thanks to our trusty sentence splitter. Global domination through better text chunking! \ud83c\udf0d</li> <li>Customizable Token Counting: Plug in your own token counter for perfect alignment with different LLMs. Because one size definitely doesn't fit all models!</li> <li>Memory-Conscious Operation: Handles massive documents efficiently by yielding chunks one at a time. Your RAM will thank you later! \ud83d\udcbe</li> <li>Multi-Format Maestro: From corporate DOCX boardrooms to academic PDF libraries, this chunker speaks every file language fluently! Handles <code>.pdf</code>, <code>.docx</code>, <code>.epub</code>, <code>.txt</code>, <code>.tex</code>, <code>.html</code>, <code>.hml</code>, <code>.md</code>, <code>.rst</code>, <code>.rtf</code>, <code>.odt</code>, <code>.csv</code>, and <code>.xlsx</code> files like a pro. \ud83c\udf0d</li> <li>Metadata Magician: Not just text - it automatically enriches your chunks with valuable metadata. Your chunks come with bonus context! \ud83d\udcca</li> <li>Bulk Processing Powerhouse: Got a mountain of documents to conquer? No problem! This powerhouse efficiently processes multiple documents in parallel. \ud83d\udcda\u26a1</li> <li>Pluggable Processor Power: Have a mysterious file format that's one-of-a-kind? Plug in your own custom processors - <code>DocumentChunker</code> is ready for any challenge you throw at it! \ud83d\udd0c\ud83d\udee0\ufe0f</li> </ul> <p>No Scanned PDF Support</p> <p>Currently, <code>DocumentChunker</code> does not support scanned PDFs (images). It can only process PDFs with selectable/extractable text. For scanned documents, you'll need to OCR them first before chunking! \ud83d\udcf7</p>"},{"location":"getting-started/programmatic/document_chunker/#composable-constraints-your-text-your-rules","title":"Composable Constraints: Your Text, Your Rules!","text":"<p><code>DocumentChunker</code> lets you call the shots with composable constraints. Mix and match limits to craft the perfect chunk size for your needs. Here's the constraint menu:</p> Constraint Value Requirement Description <code>max_sentences</code> <code>int &gt;= 1</code> Sentence power mode! Tell us how many sentences per chunk, and we'll group them thoughtfully so your ideas flow like a well-written story. <code>max_tokens</code> <code>int &gt;= 12</code> Token budget watcher! We'll carefully pack sentences into chunks while respecting your token limits. If a sentence gets too chatty, we'll politely split it at clause boundaries. \ud83e\udd10 <code>max_section_breaks</code> <code>int &gt;= 1</code> Structure superhero! Limits section breaks per chunk \u2014 headings (<code>##</code>), horizontal rules (<code>---</code>, <code>***</code>, <code>___</code>), and <code>&lt;details&gt;</code> tags. Your document structure stays intact! <p>Quick Note: Constraints Required!</p> <p>You must specify at least one limit (<code>max_sentences</code>, <code>max_tokens</code>, or <code>max_section_breaks</code>) when using chunking methods. Forget to add one? You'll get an <code>InvalidInputError</code>!</p> <p>The <code>DocumentChunker</code> has four main methods: <code>chunk_text</code>, <code>chunk_file</code>, <code>chunk_texts</code>, and <code>chunk_files</code>. <code>chunk_text</code> and <code>chunk_file</code> return a list of <code>Box</code> objects, while <code>chunk_texts</code> and <code>chunk_files</code> are memory-friendly generators that yield chunks one by one. Each <code>Box</code> has <code>content</code> (the actual text) and <code>metadata</code> (all the juicy details). Check the Metadata guide for the full scoop!</p>"},{"location":"getting-started/programmatic/document_chunker/#single-chunk-one-text","title":"Single: Chunk One Text! \ud83d\udcdd","text":"<p>Chunk a single string of text into manageable pieces using various constraints: - <code>chunk_text()</code> - accepts raw text as a string - <code>chunk_file()</code> - accepts a file path as a string or <code>pathlib.Path</code> object</p>"},{"location":"getting-started/programmatic/document_chunker/#chunking-by-sentences-sentence-group-guru","title":"Chunking by Sentences: Sentence Group Guru! \ud83d\udcdd","text":"<p>Let's say you have this text:</p> <pre><code># Introduction to Chunking\n\nThis is the first paragraph of our document. It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization. We aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information. It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings. Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\n\nReady to go beyond the basics? Let's explore some pro-level techniques!\n\n### Overlap Considerations\n\nOverlap is your secret weapon! It includes a bit of the previous chunk at the start of the next one, ensuring your text doesn't feel choppy or disconnected.\n\n---\n\n# Conclusion\n\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\n</code></pre> <p>Now let's see how to chunk it:</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\ntext = \"...\"  # The text from above\n\nchunker = DocumentChunker()  # (1)!\n\nchunks = chunker.chunk_text(\n    text=text,\n    lang=\"auto\",             # (2)!\n    max_sentences=2,\n    overlap_percent=0,       # (3)!\n    offset=0                 # (4)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Initialize <code>DocumentChunker</code> - no extra dependencies needed for plain text!</li> <li><code>lang=\"auto\"</code> lets us detect the language automatically. Super convenient, but specifying a known language like <code>lang=\"en\"</code> can boost accuracy and speed.</li> <li><code>overlap_percent=0</code> means no overlap between chunks. By default, we add 20% overlap to keep your text flowing smoothly across chunks.</li> <li><code>offset=0</code> starts us from the very beginning of the text. (Zero-based indexing - because programmers love starting from zero!)</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (1, 73)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (74, 259)}\nContent: It discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (261, 370)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (371, 529)}\nContent: There are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (531, 748)}\nContent: ---\n\n## Advanced Chunking Techniques\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 6, 'span': (750, 787)}\nContent: Ready to go beyond the basics?\nLet's explore some pro-level techniques!\n\n--- Chunk 7 ---\nMetadata: {'chunk_num': 7, 'span': (788, 859)}\nContent: ### Overlap Considerations\nOverlap is your secret weapon!\n\n--- Chunk 8 ---\nMetadata: {'chunk_num': 8, 'span': (861, 919)}\nContent: ### Overlap Considerations\nOverlap is your secret weapon!\n\n--- Chunk 9 ---\nMetadata: {'chunk_num': 9, 'span': (920, 1050)}\nContent: It includes a bit of the previous chunk at the start of the next one, ensuring your text doesn't feel choppy or disconnected.\n\n---\n\n--- Chunk 10 ---\nMetadata: {'chunk_num': 10, 'span': (1052, 1157)}\nContent: # Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#chunking-by-tokens-token-budget-master","title":"Chunking by Tokens: Token Budget Master! \ud83e\ude99","text":"<p>Token Counter Requirement</p> <p>When using the <code>max_tokens</code> constraint, a <code>token_counter</code> function is essential. This function, which you provide, should accept a string and return an integer representing its token count. Failing to provide a <code>token_counter</code> will result in a <code>MissingTokenCounterError</code>.</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nchunker = DocumentChunker(token_counter=word_counter)\n\nchunks = chunker.chunk_text(\n    text=text,\n    max_tokens=50,\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (1, 290)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (261, 573)}\nContent: ... \n## Why is Chunking Important?\n\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking,\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (531, 859)}\nContent: There are several strategies for chunking,\nincluding splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nReady to go beyond the basics?\nLet's explore some pro-level techniques!\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (819, 1080)}\nContent: Let's explore some pro-level techniques!\n\n\n\n### Overlap Considerations\nOverlap is your secret weapon!\nIt includes a bit of the previous chunk at the start of the next one, ensuring your text doesn't feel choppy or disconnected.\n\n---\n\n# Conclusion\nIn conclusion,\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1052, 1157)}\nContent: ... \n# Conclusion\nIn conclusion,\nmastering chunking is key to unlocking the full potential of your text data.\n</code></pre> <p>Overrides token_counter</p> <p>You can also provide the <code>token_counter</code> directly to any chunking method. If provided in both the constructor and the method, the one in the method will be used.</p>"},{"location":"getting-started/programmatic/document_chunker/#chunking-by-section-breaks-structure-superhero","title":"Chunking by Section Breaks: Structure Superhero! \ud83e\uddb8\u200d\u2640\ufe0f","text":"<p>This constraint is useful for documents structured with Markdown headings or thematic breaks.</p> <pre><code>chunks = chunker.chunk_text(\n    text=text,\n    max_section_breaks=2\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (1, 503)}\nContent: # Introduction to Chunking\nThis is the first paragraph of our document.\nIt discusses the importance of text segmentation for various NLP tasks, such as RAG systems and summarization.\nWe aim to break down large documents into manageable, context-rich pieces.\n\n## Why is Chunking Important?\nEffective chunking helps in maintaining the semantic coherence of information.\nIt ensures that each piece of text retains enough context to be meaningful on its own, which is crucial for downstream applications.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (371, 753)}\nContent: It ensures that each piece of text retains enough context to be meaningful on its own,\nwhich is crucial for downstream applications.\n\n### Different Strategies\nThere are several strategies for chunking, including splitting by sentences, by a fixed number of tokens, or by structural elements like headings.\nEach method has its own advantages depending on the specific use case.\n\n---\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (678, 859)}\nContent: Each method has its own advantages depending on the specific use case.\n\n---\n\n## Advanced Chunking Techniques\nReady to go beyond the basics?\nLet's explore some pro-level techniques!\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 4, 'span': (819, 1050)}\nContent: Let's explore some pro-level techniques!\n\n### Overlap Considerations\nOverlap is your secret weapon!\nIt includes a bit of the previous chunk at the start of the next one, ensuring your text doesn't feel choppy or disconnected.\n\n---\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 5, 'span': (1047, 1157)}\nContent: ... \n\n---\n\n# Conclusion\nIn conclusion, mastering chunking is key to unlocking the full potential of your text data.\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#combining-multiple-constraints-mix-and-match-magic","title":"Combining Multiple Constraints: Mix and Match Magic! \ud83c\udfad","text":"<p>The real power of <code>DocumentChunker</code> comes from combining multiple constraints. The chunking will stop as soon as any of the limits is reached.</p> <pre><code>chunks = chunker.chunk_text(\n    text,\n    max_sentences=5,\n    max_tokens=100,\n    max_section_breaks=2\n)\n</code></pre> <p>Customizing the Continuation Marker</p> <p>You can customize the continuation marker, which is prepended to clauses that don't fit in the previous chunk. To do this, pass the <code>continuation_marker</code> parameter to the chunker's constructor.</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"[...]\")\n</code></pre> <p>If you don't want any continuation marker, you can set it to an empty string:</p> <pre><code>chunker = DocumentChunker(continuation_marker=\"\")\n</code></pre> <p>Enable Verbose Logging</p> <p>To see detailed logging during the chunking process, you can set the <code>verbose</code> parameter to <code>True</code> when initializing the <code>DocumentChunker</code>: <pre><code>chunker = DocumentChunker(verbose=True)\n</code></pre></p> <p>Custom Sentence Splitter</p> <p>You can provide a custom <code>SentenceSplitter</code> instance to <code>DocumentChunker</code> for specialized sentence splitting behavior. For more details, see the Sentence Splitter documentation.</p> <p>Adding Base Metadata</p> <p>You can pass a <code>base_metadata</code> dictionary to <code>chunk_text</code> and <code>chunk_texts</code>. This metadata will be included in each chunk. For example: <code>chunker.chunk_text(..., base_metadata={\"source\": \"my_document.txt\"})</code>. For more details, see the Metadata guide.</p>"},{"location":"getting-started/programmatic/document_chunker/#single-file-process-one-document","title":"Single File: Process One Document! \ud83d\udcc4","text":"<p>While <code>chunk_text</code> is perfect for plain text, <code>chunk_file</code> handles document files. It supports the same constraints (max_sentences, max_tokens, max_section_breaks, etc.).</p> <pre><code>from chunklet.document_chunker import DocumentChunker\n\nfile_path = \"sample_text.txt\"\n\nchunker = DocumentChunker()\n\nchunks = chunker.chunk_file(\n    path=file_path,\n    lang=\"auto\",\n    max_sentences=4,\n    overlap_percent=20,\n    offset=0\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <p>Special Handling for Streaming Processors</p> <p>Some processors work differently due to their streaming nature - they yield content page by page or in blocks rather than all at once. This means they require special care:</p> <p>Streaming processors (PDF, EPUB, DOCX, ODT): These beauties process content as they go, so they're designed for <code>chunk_files</code> method. Using them with <code>chunk_file</code> will throw a <code>FileProcessingError</code> since <code>chunk_file</code> expects all content upfront.</p> <p>Regular processors work fine with both <code>chunk_file</code> and <code>chunk_files</code> methods.</p>"},{"location":"getting-started/programmatic/document_chunker/#batch-chunk-multiple-items","title":"Batch: Chunk Multiple Items! \ud83d\udcda","text":"<p>While <code>chunk_text</code> is perfect for single texts and <code>chunk_file</code> for single files, <code>chunk_texts</code> and <code>chunk_files</code> are your power players for processing multiple texts or files in parallel. They use memory-friendly generators so you can handle massive collections with ease.</p> <ul> <li><code>chunk_texts()</code> - process multiple raw text strings</li> <li><code>chunk_files()</code> - process multiple file paths</li> </ul>"},{"location":"getting-started/programmatic/document_chunker/#for-texts","title":"For Texts","text":"<pre><code>from chunklet.document_chunker import DocumentChunker\n\ndef word_counter(text: str) -&gt; int:\n    return len(text.split())\n\nEN_TEXT = \"This is the first document. It has multiple sentences for chunking. Here is the second document.\"\nES_TEXT = \"Este es el primer documento. Contiene varias frases para la segmentaci\u00f3n de texto.\"\n\nchunker = DocumentChunker(token_counter=word_counter)\n\nchunks = chunker.chunk_texts(\n    texts=[EN_TEXT, ES_TEXT],\n    max_sentences=5,\n    max_tokens=20,\n    overlap_percent=30,\n    n_jobs=2,                    # (1)!\n    on_errors=\"raise\",           # (2)!\n    show_progress=True,          # (3)!\n)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"--- Chunk {i+1} ---\")\n    print(f\"Metadata: {chunk.metadata}\")\n    print(f\"Content: {chunk.content}\")\n    print()\n</code></pre> <ol> <li>Specifies the number of parallel processes to use for chunking. The default value is <code>None</code> (use all available CPU cores).</li> <li>Define how to handle errors during processing. If set to <code>\"raise\"</code> (default), an exception will be raised immediately. If set to <code>\"break\"</code>, the process will halt and partial result will be returned. If set to <code>\"ignore\"</code>, errors will be silently ignored.</li> <li>Display a progress bar during batch processing. The default value is <code>False</code>.</li> </ol> Click to show output <pre><code>--- Chunk 1 ---\nMetadata: {'chunk_num': 1, 'span': (0, 82)}\nContent: Este es el primer documento.\nContiene varias frases para la segmentaci\u00f3n de texto.\n\n--- Chunk 2 ---\nMetadata: {'chunk_num': 2, 'span': (83, 196)}\nContent: El segundo ejemplo es m\u00e1s extenso para probar el procesamiento por lotes.\nLa tercera oraci\u00f3n a\u00f1ade m\u00e1s contenido.\n\n--- Chunk 3 ---\nMetadata: {'chunk_num': 3, 'span': (197, 236)}\nContent: Y la cuarta oraci\u00f3n para mayor medida.\n\n--- Chunk 4 ---\nMetadata: {'chunk_num': 1, 'span': (0, 96)}\nContent: This is the first document.\nIt has multiple sentences for chunking.\nHere is the second document.\n\n--- Chunk 5 ---\nMetadata: {'chunk_num': 2, 'span': (97, 201)}\nContent: It is a bit longer to test batch processing effectively.\nThis is the third sentence to add more content.\n\n--- Chunk 6 ---\nMetadata: {'chunk_num': 3, 'span': (202, 294)}\nContent: And the fourth sentence for good measure.\nThe fifth sentence makes it even more interesting.\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#for-files","title":"For Files","text":"<pre><code>PATHS = [\n    \"samples/document.pdf\",\n    \"samples/document.docx\",\n]\n\nchunks = chunker.chunk_files(PATHS, ...)\n</code></pre> <p>Generator Cleanup</p> <p>When using <code>chunk_texts</code>, it's crucial to ensure the generator is properly closed, especially if you don't iterate through all the chunks. This is necessary to release the underlying multiprocessing resources. The recommended way is to use a <code>try...finally</code> block to call <code>close()</code> on the generator. For more details, see the Troubleshooting guide.</p>"},{"location":"getting-started/programmatic/document_chunker/#separator-keeping-your-batches-organized","title":"Separator: Keeping Your Batches Organized! \ud83d\udccb","text":"<p>The <code>separator</code> parameter works for both <code>chunk_texts</code> and <code>chunk_files</code>. It lets you add a custom marker that gets yielded after all chunks from a single input are processed. Super handy for batch processing when you want to clearly separate chunks from different source texts.</p> <p>Quick Note</p> <p><code>None</code> won't work as a separator - you'll need something more substantial!</p> <pre><code>from chunklet.document_chunker import DocumentChunker\nfrom more_itertools import split_at\n\nchunker = DocumentChunker()\ntexts = [\n    \"This is the first document. It has two sentences.\",\n    \"This is the second document. It also has two sentences.\"\n]\ncustom_separator = \"---END_OF_DOCUMENT---\"\n\nchunks_with_separators = chunker.chunk_texts(\n    texts,\n    max_sentences=1,\n    separator=custom_separator,\n    show_progress=False,\n)\n\nchunk_groups = split_at(chunks_with_separators, lambda x: x == custom_separator)\n# Process the results using split_at\nfor i, doc_chunks in enumerate(chunk_groups):\n    if doc_chunks:        # (1)!\n        print(f\"--- Chunks for Document {i+1} ---\")\n        for chunk in doc_chunks:\n            print(f\"Content: {chunk.content}\")\n            print(f\"Metadata: {chunk.metadata}\")\n        print()\n</code></pre> <ol> <li>Avoid processing the empty list at the end if stream ends with separator</li> </ol> Click to show output <pre><code>--- Chunks for Document 1 ---\nContent: This is the first document.\nMetadata: {'chunk_num': 1, 'span': (0, 27)}\nContent: It has two sentences.\nMetadata: {'chunk_num': 2, 'span': (28, 49)}\n\n--- Chunks for Document 2 ---\nContent: This is the second document.\nMetadata: {'chunk_num': 1, 'span': (0, 28)}\nContent: It also has two sentences.\nMetadata: {'chunk_num': 2, 'span': (29, 55)}\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#custom-processors-build-your-own-document-wizards","title":"Custom processors: Build Your Own Document Wizards! \ud83d\udee0\ufe0f\ud83d\udd2e","text":"<p>Want to handle exotic file formats that <code>DocumentChunker</code> doesn't know about? Create your own custom processors! This lets you add specialized processing for any file type and prioritize your custom processors over the built-in ones.</p> <p>Global Registry Alert!</p> <p>Custom processors get registered globally - once you add one, it's available everywhere in your app. Watch out for side effects if you're registering processors across different parts of your codebase, especially in multi-threaded or long-running applications!</p> <p>To use a custom processor, you leverage the <code>@custom_processor_registry.register</code> decorator. This decorator allows you to register your function for one or more file extensions directly. Your custom processor function must accept a single <code>file_path</code> parameter (str) and return a <code>tuple[str | list[str], dict]</code> containing extracted text (or list of texts for multi-section documents) and a metadata dictionary.</p> <p>Custom Processor Rules</p> <ul> <li>Your function must accept exactly one required parameter (the file path)</li> <li>Optional parameters with defaults are totally fine</li> <li>File extensions must start with a dot (like <code>.json</code>, <code>.custom</code>)</li> <li>Lambda functions are not supported unless you provide a <code>name</code> parameter</li> <li>The metadata dictionary will be merged with common metadata (chunk_num, span, source)</li> <li>For multi-section documents, return a list of strings - each will be processed as a separate section</li> <li>If an error occurs during the document processing (e.g., an issue with the custom processor function), a <code>CallbackError</code> will be raised</li> </ul> <pre><code>import os\nimport re\nimport json\nimport tempfile\nfrom chunklet.document_chunker import DocumentChunker, custom_processor_registry\n\n\n# Define a simple custom processor for .json files\n@custom_processor_registry.register(\".json\", name=\"MyJSONProcessor\")\ndef my_json_processor(file_path: str) -&gt; tuple[str, dict]:\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Assuming the json has a \"text\" field with paragraphs\n    text_content = \"\\n\".join(data.get(\"text\", []))\n    metadata = data.get(\"metadata\", {})\n    metadata[\"source\"] = file_path\n    return text_content, metadata\n\n# A longer and more complex JSON sample\njson_data = {\n    \"metadata\": {\n        \"document_id\": \"doc-12345\",\n        \"created_at\": \"2025-11-05\"\n    },\n    \"text\": [\n        \"This is the first paragraph of our longer JSON sample. It contains multiple sentences to test the chunking process.\",\n        \"The second paragraph introduces a new topic. We are exploring the capabilities of custom processors in the chunklet library.\",\n        \"Finally, the third paragraph concludes our sample. We hope this demonstrates the flexibility of the system in handling various data formats.\"\n    ]\n}\n\nchunker = DocumentChunker()\n\n# Use a temporary file\nwith tempfile.NamedTemporaryFile(mode='w+', suffix=\".json\") as tmp:\n    json.dump(json_data, tmp)\n    tmp.seek(0)\n    tmp_path = tmp.name\n\n    chunks = chunker.chunk_file(\n        path=tmp_path,\n        max_sentences=5,\n    )\n\n    for i, chunk in enumerate(chunks):\n        print(f\"--- Chunk {i+1} ---\")\n        print(f\"Content:\\n{chunk.content}\\n\")\n        print(f\"Metadata:\\n{chunk.metadata}\")\n        print()\n\n# Optionally unregister\ncustom_processor_registry.unregister(\".json\")\n</code></pre> Click to show output <pre><code>--- Chunk 1 ---\nContent:\nThis is the first paragraph of our longer JSON sample.\nIt contains multiple sentences to test the chunking process.\nThe second paragraph introduces a new topic.\nWe are exploring the capabilities of custom processors in the chunklet library.\nFinally, the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 1, 'span': (0, 292)}\n\n--- Chunk 2 ---\nContent:\n... the third paragraph concludes our sample.\n\nMetadata:\n{'document_id': 'doc-12345', 'created_at': '2025-11-05', 'source': '/tmp/tmpdt6xa5rh.json', 'chunk_num': 2, 'span': (250, 292)}\n</code></pre> <p>Registering Without the Decorator</p> <p>If you prefer not to use decorators, you can directly use the <code>custom_processor_registry.register()</code> method. This is particularly useful when registering processors dynamically.</p> <pre><code>from chunklet.document_chunker import custom_processor_registry\n\ndef my_other_processor(file_path: str) -&gt; tuple[str, dict]:\n    # ... your logic ...\n    return \"some text\", {\"source\": file_path}\n\ncustom_processor_registry.register(my_other_processor, \".custom\", name=\"MyOtherProcessor\")\n</code></pre>"},{"location":"getting-started/programmatic/document_chunker/#custom_processor_registry-methods-summary","title":"<code>custom_processor_registry</code> Methods Summary","text":"<ul> <li><code>processors</code>: Returns a shallow copy of the dictionary of registered processors.</li> <li><code>is_registered(ext: str)</code>: Checks if a processor is registered for the given file extension, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], ReturnType] | None = None, *exts: str, name: str | None = None)</code>: Registers a processor callback for one or more file extensions.</li> <li><code>unregister(*exts: str)</code>: Removes processor(s) from the registry.</li> <li><code>clear()</code>: Clears all registered processors from the registry.</li> <li><code>extract_data(file_path: str, ext: str)</code>: Processes a file using a registered processor, returning the extracted data and the name of the processor used.</li> </ul> API Reference <p>For complete technical details on the <code>DocumentChunker</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/sentence_splitter/","title":"Sentence Splitter","text":""},{"location":"getting-started/programmatic/sentence_splitter/#the-art-of-precise-sentence-splitting","title":"The Art of Precise Sentence Splitting \u2702\ufe0f","text":"<p>Splitting text by periods is like trying to perform surgery with a butter knife \u2014 it barely works and makes a mess. Abbreviations get misinterpreted, sentences get cut mid-thought, and your NLP models end up confused.</p> <p>This problem has a name: Sentence Boundary Disambiguation. That's where <code>SentenceSplitter</code> comes in.</p> <p>Think of it as a skilled linguist who knows where sentences actually end. It handles grammar, context, and those tricky abbreviations (like \"Dr.\" or \"U.S.A.\") without breaking a sweat. Supports 50+ languages out of the box.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#whats-under-the-hood","title":"What's Under the Hood? \u2699\ufe0f","text":"<p>The <code>SentenceSplitter</code> is a sophisticated system:</p> <ul> <li>Multilingual Support \ud83c\udf0d: Handles over 50 languages with intelligent detection. See the full list.</li> <li>Custom Splitters \ud83d\udd27: Plug in your own splitting logic for specialized languages or domains.</li> <li>Reliable Fallback \ud83d\udee1\ufe0f: For unsupported languages, a rule-based fallback kicks in.</li> <li>Error Monitoring \ud83d\udd0d: Reports issues with custom splitters clearly.</li> <li>Output Refinement \u2728: Removes empty sentences and fixes punctuation.</li> </ul>"},{"location":"getting-started/programmatic/sentence_splitter/#example-usage","title":"Example Usage","text":""},{"location":"getting-started/programmatic/sentence_splitter/#split-text-into-sentences","title":"Split Text into Sentences","text":"<p>Here's a quick example of how you can use the <code>SentenceSplitter</code> to split a block of text into sentences:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nTEXT = \"\"\"\nShe loves cooking. He studies AI. \"You are a Dr.\", she said. The weather is great. We play chess. Books are fun, aren't they?\n\nThe Playlist contains:\n  - two videos\n  - one image\n  - one music\n\nRobots are learning. It's raining. Let's code. Mars is red. Sr. sleep is rare. Consider item 1. This is a test. The year is 2025. This is a good year since N.A.S.A. reached 123.4 light year more.\n\"\"\"\n\nsplitter = SentenceSplitter(verbose=True)\nsentences = splitter.split_text(TEXT, lang=\"auto\") #(1)!\n\nfor sentence in sentences:\n    print(sentence)\n</code></pre> <ol> <li>Auto language detection: Let the splitter automatically detect the language of your text. For best results, specify a language code like <code>\"en\"</code> or <code>\"fr\"</code> directly.</li> </ol> Click to show output <pre><code>2025-11-02 16:27:29.277 | WARNING  | chunklet.sentence_splitter.sentence_splitter:split_text:192 - The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\n2025-11-02 16:27:29.316 | INFO     | chunklet.sentence_splitter.sentence_splitter:detected_top_language:146 - Language detection: 'en' with confidence 10/10.\n2025-11-02 16:27:29.447 | INFO     | chunklet.sentence_splitter.sentence_splitter:split_text:166 - Text splitted into sentences. Total sentences detected: 19\nShe loves cooking.\nHe studies AI.\n\"You are a Dr.\", she said.\nThe weather is great.\nWe play chess.\nBooks are fun, aren't they?\nThe Playlist contains:\n- two videos\n- one image\n- one music\nRobots are learning.\nIt's raining.\nLet's code.\nMars is red.\nSr. sleep is rare.\nConsider item 1.\nThis is a test.\nThe year is 2025.\nThis is a good year since N.A.S.A. reached 123.4 light year more.\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#splitting-files-from-document-to-sentences","title":"Splitting Files: From Document to Sentences \ud83d\udcc4","text":"<p>Need to split a file directly into sentences? Use <code>split_file</code>:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nsplitter = SentenceSplitter()\nsentences = splitter.split_file(\"sample.txt\", lang=\"en\")\n\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n</code></pre> Click to show output <pre><code>Sentence 1: This is the first sentence.\nSentence 2: This is the second sentence.\nSentence 3: And the third.\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#detecting-top-languages","title":"Detecting Top Languages \ud83c\udfaf","text":"<p>Here's how you can detect the top language of a given text using the <code>SentenceSplitter</code>:</p> <pre><code>from chunklet.sentence_splitter import SentenceSplitter\n\nlang_texts = {\n    \"en\": \"This is a sentence. This is another sentence. Mr. Smith went to Washington. He said 'Hello World!'. The quick brown fox jumps over the lazy dog.\",\n    \"fr\": \"Ceci est une phrase. Voici une autre phrase. M. Smith est all\u00e9 \u00e0 Washington. Il a dit 'Bonjour le monde!'. Le renard brun et rapide saute par-dessus le chien paresseux.\",\n    \"es\": \"Esta es una oraci\u00f3n. Aqu\u00ed hay otra oraci\u00f3n. El Sr. Smith fue a Washington. Dijo '\u00a1Hola Mundo!'. El r\u00e1pido zorro marr\u00f3n salta sobre el perro perezoso.\",\n    \"de\": \"Dies ist ein Satz. Hier ist ein weiterer Satz. Herr Smith ging nach Washington. Er sagte 'Hallo Welt!'. Der schnelle braune Fuchs springt \u00fcber den faulen Hund.\",\n    \"hi\": \"\u092f\u0939 \u090f\u0915 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u092f\u0939 \u090f\u0915 \u0914\u0930 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964 \u0936\u094d\u0930\u0940 \u0938\u094d\u092e\u093f\u0925 \u0935\u093e\u0936\u093f\u0902\u0917\u091f\u0928 \u0917\u090f\u0964 \u0909\u0938\u0928\u0947 \u0915\u0939\u093e '\u0928\u092e\u0938\u094d\u0924\u0947 \u0926\u0941\u0928\u093f\u092f\u093e!'\u0964 \u0924\u0947\u091c \u092d\u0942\u0930\u093e \u0932\u094b\u092e\u0921\u093c\u0940 \u0906\u0932\u0938\u0940 \u0915\u0941\u0924\u094d\u0924\u0947 \u092a\u0930 \u0915\u0942\u0926\u0924\u093e \u0939\u0948\u0964\"\n}\n\nsplitter = SentenceSplitter()\n\nfor lang, text in lang_texts.items():\n    detected_lang, confidence = splitter.detected_top_language(text)\n    print(f\"Original language: {lang}\")\n    print(f\"Detected language: {detected_lang} with confidence {confidence:.2f}\")\n    print(\"-\" * 20)\n</code></pre> Click to show output <pre><code>Original language: en\nDetected language: en with confidence 1.00\n--------------------\nOriginal language: fr\nDetected language: fr with confidence 1.00\n--------------------\nOriginal language: es\nDetected language: es with confidence 1.00\n--------------------\nOriginal language: de\nDetected language: de with confidence 1.00\n--------------------\nOriginal language: hi\nDetected language: hi with confidence 1.00\n--------------------\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#custom-sentence-splitter","title":"Custom Sentence Splitter: Your Playground \ud83c\udfa8","text":"<p>Want to bring your own splitting logic? You can plug in custom splitter functions to Chunklet! Perfect for specialized languages or domains.</p> <p>Global Registry Alert!</p> <p>Custom splitters get registered globally - once you add one, it's available everywhere in your app. Watch out for side effects if you're registering splitters across different parts of your codebase, especially in multi-threaded or long-running applications!</p> <p>To use a custom splitter, you leverage the <code>@registry.register</code> decorator. This decorator allows you to register your function for one or more languages directly. Your custom splitter function must accept a single <code>text</code> parameter (str) and return a <code>list[str]</code> of sentences.</p> <p>Custom Splitter Rules</p> <ul> <li>Your function must accept exactly one required parameter (the text)</li> <li>Optional parameters with defaults are totally fine</li> <li>Must return a list of strings</li> <li>Empty strings get filtered out automatically</li> <li>Lambda functions work if you provide a <code>name</code> parameter</li> <li>Errors during splitting will raise a <code>CallbackError</code></li> </ul>"},{"location":"getting-started/programmatic/sentence_splitter/#basic-custom-splitter","title":"Basic Custom Splitter","text":"<p>Create a custom sentence splitter for a single language using the registry decorator:</p> <pre><code>import re\nfrom chunklet.sentence_splitter import SentenceSplitter, custom_splitter_registry\n\nsplitter = SentenceSplitter(verbose=False)\n\n@custom_splitter_registry.register(\"en\", name=\"MyCustomEnglishSplitter\")\ndef english_sent_splitter(text: str) -&gt; list[str]:\n    \"\"\"A simple custom sentence splitter\"\"\"\n    return [s.strip() for s in re.split(r'(?&lt;=\\\\.)\\s+', text) if s.strip()]\n\ntext = \"This is the first sentence. This is the second sentence. And the third.\"\nsentences = splitter.split_text(text=text, lang=\"en\")\n\nprint(\"--- Sentences using Custom Splitter ---\")\nfor i, sentence in enumerate(sentences):\n    print(f\"Sentence {i+1}: {sentence}\")\n</code></pre> Click to show output <pre><code>--- Sentences using Custom Splitter ---\nSentence 1: This is the first sentence.\nSentence 2: This is the second sentence.\nSentence 3: And the third.\n</code></pre>"},{"location":"getting-started/programmatic/sentence_splitter/#multi-language-custom-splitter","title":"Multi-Language Custom Splitter","text":"<p>Register the same splitter function for multiple languages at once:</p> <pre><code>@custom_splitter_registry.register(\"fr\", \"es\", name=\"MultiLangExclamationSplitter\")  #(1)!\ndef multi_lang_splitter(text: str) -&gt; list[str]:\n    return [s.strip() for s in re.split(r'(?&lt;=!)\\s+', text) if s.strip()]\n</code></pre> <ol> <li>This registers the same custom splitter for both French (\"fr\") and Spanish (\"es\") languages.</li> </ol>"},{"location":"getting-started/programmatic/sentence_splitter/#unregistering-custom-splitters","title":"Unregistering Custom Splitters","text":"<p>Remove a registered custom splitter when you no longer need it:</p> <pre><code>custom_splitter_registry.unregister(\"en\")  # (1)!\n</code></pre> <ol> <li>This will remove the custom splitter associated with the \"en\" language code. Note that you can unregister multiple languages if you had registered them with the same function: <code>registry.unregister(\"fr\", \"es\")</code></li> </ol> <p>Skip the Decorator?</p> <p>Not a fan of decorators? No worries - you can directly use the <code>registry.register()</code> method. Super handy for dynamic registration or when your callback function isn't in the global scope.</p> <pre><code>from chunklet.sentence_splitter import custom_splitter_registry\n\ndef my_other_splitter(text: str) -&gt; list[str]:\n    return text.split(' ')\n\ncustom_splitter_registry.register(my_other_splitter, \"jp\", name=\"MyOtherSplitter\")\n</code></pre> <p>Want to Build from Scratch?</p> <p>Going full custom? Inherit from the <code>BaseSplitter</code> abstract class! It gives you a clear interface (<code>def split(self, text: str, lang: str) -&gt; list[str]</code>) to implement. Your custom splitter will then work seamlessly with <code>DocumentChunker</code>.</p>"},{"location":"getting-started/programmatic/sentence_splitter/#customsplitterregistry-methods-summary","title":"<code>CustomSplitterRegistry</code> Methods Summary","text":"<ul> <li><code>splitters</code>: Returns a shallow copy of the dictionary of registered splitters.</li> <li><code>is_registered(lang: str)</code>: Checks if a splitter is registered for the given language, returning <code>True</code> or <code>False</code>.</li> <li><code>register(callback: Callable[[str], list[str]] | None = None, *langs: str, name: str | None = None)</code>: Registers a splitter callback for one or more languages.</li> <li><code>unregister(*langs: str)</code>: Removes splitter(s) from the registry.</li> <li><code>clear()</code>: Clears all registered splitters from the registry.</li> <li><code>split(text: str, lang: str)</code>: Processes a text using a splitter registered for the given language, returning a list of sentences and the name of the splitter used.</li> </ul> API Reference <p>For complete technical details on the <code>SentenceSplitter</code> class, check out the API documentation.</p>"},{"location":"getting-started/programmatic/visualizer/","title":"Text Chunk Visualizer","text":"<p> Interactive chunking in action - upload, process, and explore! </p>"},{"location":"getting-started/programmatic/visualizer/#quick-install","title":"Quick Install","text":"<pre><code>pip install chunklet-py[visualization]\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#text-chunk-visualizer-your-window-into-the-chunking-abyss","title":"Text Chunk Visualizer: Your Window into the Chunking Abyss","text":"<p>This installs the web interface dependencies (FastAPI + Uvicorn) for interactive chunk visualization! \ud83c\udf10</p> <p>Ever wondered what your text or code looks like after being chopped up by a chunking algorithm? The Text Chunk Visualizer demystifies text segmentation with a clean web interface - WYSIWYG (what you see is what you get).</p> <p>No more guessing games - see your chunking results in real-time!</p>"},{"location":"getting-started/programmatic/visualizer/#so-how-do-i-get-this-running","title":"So How Do I Get This Running?","text":"<p>First, make sure you have the visualization dependencies:</p> <pre><code>pip install \"chunklet-py[visualization]\"\n</code></pre> <p>Here's the basic code to get it running:</p> <pre><code>from chunklet.visualizer import Visualizer\n\n# Optional: Define a custom token counter\n# def my_token_counter(text: str) -&gt; int:\n#     return len(text.split())  # Simple word-based counting\n\nvisualizer = Visualizer(\n    host=\"127.0.0.1\",    #(1)!\n    port=8000,          #(2)!\n    # token_counter=my_token_counter  # Uncomment if using custom counter\n)\n\nvisualizer.serve()  # Blocks until Ctrl+C\n</code></pre> <ol> <li>Host: The IP address where the server will listen. Use <code>\"127.0.0.1\"</code> for localhost or <code>\"0.0.0.0\"</code> to allow access from other devices on your network.</li> <li>Port: The port number for the web server. The visualizer will be accessible at <code>http://host:port</code>.  </li> </ol> Click to show output <pre><code>Starting Chunklet Visualizer...\nURL: http://127.0.0.1:8000\nPress Ctrl+C to stop the server\nOpened in default browser\n = = = = = = = = = = = = = = = = = = = = = = = = = =\n\nTEXT CHUNK VISUALIZER\n= = = = = = = = = = = = = = = = = = = = = = = = = =\nURL: http://127.0.0.1:8000\nINFO:     Started server process [30999]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     127.0.0.1:45482 - \"GET / HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45490 - \"GET /static/js/app.js HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45482 - \"GET /static/css/style.css HTTP/1.1\" 304 Not Modified\nINFO:     127.0.0.1:45490 - \"GET /api/token_counter_status HTTP/1.1\" 200 OK\n</code></pre> <p>Run this and you'll see the server start up with the URL where your visualizer is ready!</p> <p>(But honestly, the CLI <code>chunklet visualize</code> command is way easier for most use cases!)</p> <p>Prefer command line?</p> <p>For quick access without writing code, check out the CLI visualize command.</p>"},{"location":"getting-started/programmatic/visualizer/#whats-the-web-interface-like","title":"What's the Web Interface Like?","text":"<p>Open your browser to the URL shown in the terminal output. You'll find a clean interface designed for quick chunking experiments.</p> How do I upload files? <p>Simple: drag and drop your text files (<code>.txt</code>, <code>.md</code>, <code>.py</code>, etc.) onto the upload area, or click \"Browse Files\" to select them manually. The visualizer accepts any text-based file.</p> What's the difference between Document and Code mode? <p>Choose your chunking strategy after upload:</p> <ul> <li>Document Mode: For general text, articles, and documents - focuses on sentences and sections</li> <li>Code Mode: For source code - understands functions, classes, and code structure</li> </ul> <p>Each mode has its own parameter controls because text and code need different chunking approaches.</p> How Do I Process My Content? <p>Select your mode and parameters, then click \"Process Document\" or \"Process Code\". The visualizer applies your settings and shows exactly how your content gets chunked.</p> What About the Interactive Features? <p>The interface gives you great visibility:</p> <ul> <li>Click to Highlight: Click text to see which chunk(s) contain it</li> <li>Double-Click for Details: Get full metadata popups with span, chunk number, and source info</li> <li>Overlap Toggle: Use \"Reveal Overlaps\" to see where chunks share content</li> </ul> Can I Export My Results? <p>Absolutely! Click \"Download Chunks\" to get a JSON file with all chunks, their content, and complete metadata - perfect for further processing or analysis.</p> <p>The exported JSON follows this structure:</p> <pre><code>{\n    \"chunks\": [\n     {\n        \"content\": \"The actual text content of this chunk...\",\n        \"metadata\": {\n            \"source\": \"path/to/source/file.txt\",\n            \"chunk_num\": 1,\n            \"span\": [0, 150],\n            // ... other metadata fields\n        }\n    },\n    // ... more chunks\n    ],\n     \"stats\": {\n    \"chunk_count\": 3,\n    \"overlap_count\": 2,\n    \"text_length\": 696,\n    \"mode\": \"document\",\n    \"generated\": \"2025-12-18T15:16:11.379Z\"\n  }\n}\n</code></pre> <p>Quick Tips for Better Results</p> <ul> <li>Start with small files to get familiar with the interface</li> <li>Experiment with different parameter combinations to see their effects</li> <li>Use the metadata views to understand chunk boundaries</li> <li>The visualizer is perfect for comparing chunking strategies side-by-side</li> </ul> <p>Go experiment! The visualizer makes it easy to see exactly what your settings produce, so you can fine-tune for optimal chunking.</p>"},{"location":"getting-started/programmatic/visualizer/#headlessrest-api-usage","title":"Headless/REST API Usage","text":"<p>The <code>Visualizer</code> isn't just a web interface - it also provides a complete REST API for headless chunking operations. This means you can use Chunklet's interactive features programmatically without the web UI!</p>"},{"location":"getting-started/programmatic/visualizer/#available-endpoints","title":"Available Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/health</code> Health check endpoint <code>GET</code> <code>/api/token_counter_status</code> Check if token counter is configured <code>POST</code> <code>/api/chunk</code> Upload and chunk a file"},{"location":"getting-started/programmatic/visualizer/#chunking-files-programmatically","title":"Chunking Files Programmatically","text":""},{"location":"getting-started/programmatic/visualizer/#option-1-cli-headless-server-recommended","title":"Option 1: CLI Headless Server (Recommended)","text":"<p>The easiest way to start a headless visualizer server is with the CLI:</p> <pre><code>chunklet visualize --headless --port 8000\n</code></pre> <p>CLI Headless Mode</p> <p>See the Scenario 3: Headless Mode in the CLI documentation for more details on headless CLI usage with custom tokenizers.</p>"},{"location":"getting-started/programmatic/visualizer/#option-2-python-server-script","title":"Option 2: Python Server Script","text":"<p>For more programmatic control, create a custom server script (<code>server.py</code>):</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Headless visualizer server for programmatic chunking.\"\"\"\n\nfrom chunklet.visualizer import Visualizer\n\n# Configure your visualizer\nvisualizer = Visualizer(\n    host=\"127.0.0.1\",\n    port=8000,\n    # token_counter=lambda text: len(text.split())  # Optional custom tokenizer\n)\n\nprint(\"Starting headless visualizer server...\")\nprint(\"Press Ctrl+C to stop\")\nvisualizer.serve()\n</code></pre> <p>Run the server:</p> <pre><code>python server.py\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#using-the-rest-api-client","title":"Using the REST API Client","text":"<p>Use this Python client to chunk files programmatically:</p> <pre><code>import requests\n\n# Connect to your running server\nbase_url = \"http://127.0.0.1:8000\"\n\n# Check if token counter is available\nresponse = requests.get(f\"{base_url}/api/token_counter_status\")\nprint(response.json())  # {\"token_counter_available\": false}\n\n# Chunk a file\nwith open(\"my_document.txt\", \"rb\") as f:\n    files = {\"file\": (\"my_document.txt\", f, \"text/plain\")}\n    data = {\n        \"mode\": \"document\",  # or \"code\"\n        \"params\": '{\"max_sentences\": 3, \"overlap_percent\": 20}'\n    }\n\n    response = requests.post(f\"{base_url}/api/chunk\", files=files, data=data)\n\nif response.status_code == 200:\n    result = response.json()\n    print(f\"Created {result['stats']['chunk_count']} chunks\")\n\n    # Access chunks\n    for chunk in result[\"chunks\"]:\n        print(f\"Chunk content: {chunk['content']}\")\n        print(f\"Metadata: {chunk['metadata']}\")\nelse:\n    print(f\"Error: {response.status_code} - {response.text}\")\n</code></pre>"},{"location":"getting-started/programmatic/visualizer/#response-format","title":"Response Format","text":"<p>The <code>/api/chunk</code> endpoint returns:</p> <pre><code>{\n  \"text\": \"Original file content...\",\n  \"chunks\": [\n    {\n      \"content\": \"Chunk text content...\",\n      \"metadata\": {\n        \"source\": \"filename.txt\",\n        \"chunk_num\": 1,\n        \"span\": [0, 150],\n        // ... additional metadata\n      }\n    }\n  ],\n  \"stats\": {\n    \"text_length\": 696,\n    \"chunk_count\": 3,\n    \"mode\": \"document\"\n  }\n}\n</code></pre> <p>Perfect for Integration</p> <p>Use the REST API to integrate Chunklet's visualizer capabilities into your own applications, automation scripts, or testing pipelines!</p> API Reference <p>For complete technical details on the <code>Visualizer</code> class, check out the API documentation.</p>"},{"location":"how-to/custom-tokenizer/","title":"Custom Tokenizers","text":""},{"location":"how-to/custom-tokenizer/#why-custom-tokenizers","title":"Why Custom Tokenizers?","text":"<p>Got a specific LLM in mind? Or maybe a unique tokenization method for your use case? Chunklet's got you covered! Our custom tokenizer support lets you plug in any tokenization logic you can imagine. Because one size definitely doesn't fit all models!</p> <p>Whether you're working with GPT-4, Claude, a local model, or something totally custom - Chunklet plays nice with your tokenizer of choice! \ud83c\udfaf</p>"},{"location":"how-to/custom-tokenizer/#how-it-works","title":"How It Works","text":"<p>Chunklet passes your text to the tokenizer via STDIN and expects an integer token count on STDOUT. Simple as that!</p> Component Details Input Read text from <code>stdin</code> Output Print only the integer count to <code>stdout</code> Language Any programming language works! <p>Any Language, Any Platform</p> <p>Your tokenizer can be Python, JavaScript, Go, Rust, Bash, or whatever floats your boat! As long as it reads from stdin and outputs a number, you're golden. \ud83c\udf1f</p>"},{"location":"how-to/custom-tokenizer/#examples","title":"Examples","text":""},{"location":"how-to/custom-tokenizer/#python-the-classic-choice","title":"Python - The Classic Choice","text":"<pre><code>#!/usr/bin/env python3\n# my_tokenizer.py\nimport sys\nimport tiktoken\n\ntext = sys.stdin.read()\nencoding = tiktoken.encoding_for_model(\"gpt-4\")\nprint(len(encoding.encode(text)))\n</code></pre>"},{"location":"how-to/custom-tokenizer/#javascriptnodejs-for-the-js-fans","title":"JavaScript/Node.js - For the JS Fans","text":"<pre><code>#!/usr/bin/env node\n// my_tokenizer.js\nconst readline = require('readline');\n\nconst rl = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n  terminal: false\n});\n\nlet text = '';\nrl.on('line', (line) =&gt; { text += line + '\\n'; });\nrl.on('close', () =&gt; {\n  const tokens = text.split(/\\s+/).filter(w =&gt; w.length &gt; 0).length;\n  console.log(tokens);\n});\n</code></pre>"},{"location":"how-to/custom-tokenizer/#shellbash-keep-it-simple","title":"Shell/Bash - Keep It Simple!","text":"<pre><code>#!/bin/bash\n# my_tokenizer.sh\n# Simple word count - works everywhere!\ntext=$(cat)\necho \"$text\" | wc -w\n</code></pre>"},{"location":"how-to/custom-tokenizer/#go-for-the-performance-nerds","title":"Go - For the Performance Nerds","text":"<pre><code>// my_tokenizer.go\n// Note: Go doesn't support shebangs - use interpreter prefix below\npackage main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"os\"\n    \"strings\"\n)\n\nfunc main() {\n    reader := bufio.NewReader(os.Stdin)\n    text, _ := reader.ReadString('\\0')\n\n    tokens := strings.Fields(text)\n    fmt.Println(len(tokens))\n}\n</code></pre> <p>No Extra Fluff!</p> <p>Chunklet expects only the integer. No units, no explanations, no emoji - just the raw number. Otherwise, things might get a little... confused. \ud83e\udd2f</p> <pre><code># \u274c Bad - extra output confuses Chunklet\nprint(f\"Token count: {count}\")\n\n# \u2705 Good - just the number\nprint(count)\n</code></pre>"},{"location":"how-to/custom-tokenizer/#usage","title":"Usage","text":""},{"location":"how-to/custom-tokenizer/#cli-command-line-power","title":"CLI - Command Line Power!","text":""},{"location":"how-to/custom-tokenizer/#with-chunk-command","title":"With <code>chunk</code> command:","text":"<pre><code>chunklet chunk --text \"Your text here\" \\\n  --max-tokens 50 \\\n  --tokenizer-command \"python ./my_tokenizer.py\"\n</code></pre>"},{"location":"how-to/custom-tokenizer/#with-visualize-command","title":"With <code>visualize</code> command:","text":"<pre><code>chunklet visualize \\\n  --tokenizer-command \"python ./my_tokenizer.py\" \\\n  --tokenizer-timeout 30\n</code></pre> <p>Make It Executable (with shebang)</p> <p>If you're on Unix/Linux/Mac and your script has a shebang (e.g., <code>#!/usr/bin/env python3</code>), you can make it executable with <code>chmod +x my_tokenizer.py</code> and then use <code>--tokenizer-command \"./my_tokenizer.py\"</code> - no interpreter prefix needed! \ud83d\ude80</p>"},{"location":"how-to/custom-tokenizer/#programmatic-python-power","title":"Programmatic - Python Power!","text":"<pre><code>from chunklet import DocumentChunker\n\n# Your custom tokenizer function\ndef my_tokenizer(text: str) -&gt; int:\n    return len(text.split())  # Simple word count!\n\nchunker = DocumentChunker(token_counter=my_tokenizer)\nchunks = chunker.chunk_text(text, max_tokens=50)\n\nfor chunk in chunks:\n    print(chunk.content)\n</code></pre> Learn More <ul> <li>Beginner's Intro to Reading from Standard Input - for understanding stdin basics in python</li> <li>CLI Documentation for command-line usage</li> <li>Document Chunker for multi-format document processing</li> <li>DocumentChunker API for programmatic usage</li> </ul>"},{"location":"reference/chunklet/","title":"chunklet","text":""},{"location":"reference/chunklet/#chunklet","title":"chunklet","text":"<p>Chunklet: Advanced Text, Code, and Document Chunking for LLM Applications</p> <p>A comprehensive library for semantic text segmentation, interactive chunk visualization, and multi-format document processing. Split content intelligently across 50+ languages, visualize chunks in real-time, and handle various file types with flexible, context-aware chunking strategies.</p> <p>Key Features:</p> <ul> <li>Sentence splitting: Multilingual text segmentation across 50+ languages</li> <li>Semantic chunking: PlainTextChunker, DocumentChunker, and CodeChunker</li> <li>Interactive visualization: Web-based chunk exploration and parameter tuning</li> <li>Multi-format support: Text, code, PDF, DOCX, EPUB, and more</li> <li>Batch processing: Memory-optimized generators with flexible error handling</li> </ul> Note <p>PlainTextChunker has been merged into DocumentChunker since v2.2.0. Use DocumentChunker.chunk_text() or DocumentChunker.chunk_texts() instead.</p> <p>Modules:</p> <ul> <li> <code>base_chunker</code>           \u2013            <p>Base Chunker Abstract Class</p> </li> <li> <code>cli</code>           \u2013            </li> <li> <code>code_chunker</code>           \u2013            </li> <li> <code>common</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>exceptions</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> <li> <code>visualizer</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/#chunklet.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/#chunklet.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/#chunklet.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/#chunklet.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/#chunklet.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the chunking method, like `chunker.chunk_text(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/#chunklet.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/#chunklet.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p> <p>This can happen if: - The file extension is not in the supported list - The file has no extension - The processor returns an iterable (requires batch processing)</p>"},{"location":"reference/chunklet/base_chunker/","title":"base_chunker","text":""},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker","title":"chunklet.base_chunker","text":"<p>Base Chunker Abstract Class</p> <p>Defines the interface for chunkers.</p> <p>Classes:</p> <ul> <li> <code>BaseChunker</code>           \u2013            <p>Abstract base class for chunkers.</p> </li> </ul>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker","title":"BaseChunker","text":"<pre><code>BaseChunker(verbose: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for chunkers.</p> <p>Defines the standard interface for chunking content into units.</p> <p>Methods:</p> <ul> <li> <code>chunk_file</code>             \u2013              <p>Read and chunk a file.</p> </li> <li> <code>chunk_files</code>             \u2013              <p>Process multiple files.</p> </li> <li> <code>chunk_text</code>             \u2013              <p>Extract chunks from text.</p> </li> <li> <code>chunk_texts</code>             \u2013              <p>Process multiple texts.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>def __init__(self, verbose: bool = False):\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.chunk_file","title":"chunk_file  <code>abstractmethod</code>","text":"<pre><code>chunk_file(*args, **kwargs) -&gt; list[Box]\n</code></pre> <p>Read and chunk a file.</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of chunks with content and metadata.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk_file(self, *args, **kwargs) -&gt; list[Box]:\n    \"\"\"\n    Read and chunk a file.\n\n    Returns:\n        list[Box]: List of chunks with content and metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.chunk_files","title":"chunk_files  <code>abstractmethod</code>","text":"<pre><code>chunk_files(*args, **kwargs) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple files.</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk_files(self, *args, **kwargs) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple files.\n\n    Yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.chunk_text","title":"chunk_text  <code>abstractmethod</code>","text":"<pre><code>chunk_text(*args, **kwargs) -&gt; list[Box]\n</code></pre> <p>Extract chunks from text.</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of chunks with content and metadata.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk_text(self, *args, **kwargs) -&gt; list[Box]:\n    \"\"\"\n    Extract chunks from text.\n\n    Returns:\n        list[Box]: List of chunks with content and metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/base_chunker/#chunklet.base_chunker.BaseChunker.chunk_texts","title":"chunk_texts  <code>abstractmethod</code>","text":"<pre><code>chunk_texts(*args, **kwargs) -&gt; list[list[Box]]\n</code></pre> <p>Process multiple texts.</p> <p>Returns:</p> <ul> <li> <code>list[list[Box]]</code>           \u2013            <p>list[list[Box]]: List of chunks for each input text.</p> </li> </ul> Source code in <code>src/chunklet/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk_texts(self, *args, **kwargs) -&gt; list[list[Box]]:\n    \"\"\"\n    Process multiple texts.\n\n    Returns:\n        list[list[Box]]: List of chunks for each input text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/cli/","title":"cli","text":""},{"location":"reference/chunklet/cli/#chunklet.cli","title":"chunklet.cli","text":"<p>Functions:</p> <ul> <li> <code>chunk_command</code>             \u2013              <p>Chunk text or files based on specified parameters.</p> </li> <li> <code>split_command</code>             \u2013              <p>Split text or a single file into sentences</p> </li> <li> <code>visualize_command</code>             \u2013              <p>Start the web-based chunk visualizer interface for interactive text and code chunking.</p> </li> </ul>"},{"location":"reference/chunklet/cli/#chunklet.cli.chunk_command","title":"chunk_command","text":"<pre><code>chunk_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to chunk. If not provided, --source must be used.\",\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    code: bool = typer.Option(\n        False,\n        \"--code\",\n        help=\"Use CodeChunker for code files.\",\n    ),\n    doc: bool = typer.Option(\n        False,\n        \"--doc\",\n        help=\"Use DocumentChunker for document files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        \"-l\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85).\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=\"A shell command to use for token counting. The command should take text as stdin and output the token count as a number.\",\n    ),\n    tokenizer_timeout: int | None = typer.Option(\n        None,\n        \"--tokenizer-timeout\",\n        \"-t\",\n        help=\"Timeout in seconds for the tokenizer command.\",\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        \"-m\",\n        help=\"Include metadata in the output. If --destination is a directory, metadata is saved as separate .json files; otherwise, it's included inline in the output.\",\n    ),\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking.\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n)\n</code></pre> <p>Chunk text or files based on specified parameters.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"chunk\", help=\"Chunk text or files based on specified parameters.\")\ndef chunk_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to chunk. If not provided, --source must be used.\"\n    ),\n    source: Optional[List[Path]] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path(s) to one or more files or directories to read input from. Overrides the 'text' argument.\",\n    ),\n    # flags for chunker type\n    code: bool = typer.Option(False, \"--code\", help=\"Use CodeChunker for code files.\"),\n    doc: bool = typer.Option(\n        False, \"--doc\", help=\"Use DocumentChunker for document files.\"\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a file (for single output) or a directory (for batch output) to write the chunks.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        \"-l\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        help=\"Maximum number of tokens per chunk. Applies to all chunking strategies. (must be &gt;= 12)\",\n    ),\n    max_sentences: int = typer.Option(\n        None,\n        \"--max-sentences\",\n        help=\"Maximum number of sentences per chunk. (must be &gt;= 1)\",\n    ),\n    max_section_breaks: Optional[int] = typer.Option(\n        None,\n        \"--max-section-breaks\",\n        help=\"Maximum number of section breaks per chunk. (must be &gt;= 1)\",\n    ),\n    overlap_percent: float = typer.Option(\n        20.0,\n        \"--overlap-percent\",\n        help=\"Percentage of overlap between chunks (0-85).\",\n    ),\n    offset: int = typer.Option(\n        0,\n        \"--offset\",\n        help=\"Starting sentence offset for chunking.\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=(\n            \"A shell command to use for token counting. \"\n            \"The command should take text as stdin and output the token count as a number.\"\n        ),\n    ),\n    tokenizer_timeout: int | None = typer.Option(\n        None,\n        \"--tokenizer-timeout\",\n        \"-t\",\n        help=\"Timeout in seconds for the tokenizer command.\",\n    ),\n    metadata: bool = typer.Option(\n        False,\n        \"--metadata\",\n        \"-m\",\n        help=(\n            \"Include metadata in the output. If --destination is a directory, \"\n            \"metadata is saved as separate .json files; otherwise, it's \"\n            \"included inline in the output.\"\n        ),\n    ),\n    # for Batching\n    n_jobs: Optional[int] = typer.Option(\n        None,\n        \"--n-jobs\",\n        help=\"Number of parallel jobs for batch chunking.\",\n    ),\n    on_errors: OnError = typer.Option(\n        OnError.raise_,\n        \"--on-errors\",\n        help=\"How to handle errors during processing: 'raise', 'skip' or 'break'\",\n    ),\n    # CodeChunker specific arguments\n    max_lines: int = typer.Option(\n        None,\n        \"--max-lines\",\n        help=\"Maximum number of lines per chunk. Applies to CodeChunker only. (must be &gt;= 5)\",\n    ),\n    max_functions: int = typer.Option(\n        None,\n        \"--max-functions\",\n        help=\"Maximum number of functions per chunk. Applies to CodeChunker only. (must be &gt;= 1)\",\n    ),\n    docstring_mode: DocstringMode = typer.Option(\n        DocstringMode.all_,\n        \"--docstring-mode\",\n        help=\"Docstring processing strategy for CodeChunker: 'summary', 'all', or 'excluded'. Applies to CodeChunker only.\",\n    ),\n    strict: bool = typer.Option(\n        True,\n        \"--strict\",\n        help=\"If True, raise error when structural blocks exceed max_tokens in CodeChunker. If False, split oversized blocks. Applies to CodeChunker only.\",\n    ),\n    include_comments: bool = typer.Option(\n        True,\n        \"--include-comments\",\n        help=\"Include comments in output chunks for CodeChunker. Applies to CodeChunker only.\",\n    ),\n):\n    \"\"\"Chunk text or files based on specified parameters.\"\"\"\n    # --- Input validation logic ---\n    provided_inputs = [arg for arg in [text, source] if arg]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please provide a text, or use the --source option.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Please provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if doc and code:\n        typer.echo(\n            \"Error: Please specify either '--doc' or '--code', but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # --- Tokenizer setup ---\n    token_counter = None\n    if tokenizer_command:\n        token_counter = _create_external_tokenizer(tokenizer_command, tokenizer_timeout)\n\n    # Construct chunk_kwargs dynamically\n    chunk_kwargs = {\n        \"max_tokens\": max_tokens,\n        \"token_counter\": token_counter,\n    }\n\n    if code:\n        if CodeChunker is None:\n            typer.echo(\n                \"Error: CodeChunker dependencies not available.\\n\"\n                \"Please install with: pip install chunklet-py[code]\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        chunker_instance = CodeChunker(\n            verbose=verbose,\n            token_counter=token_counter,\n        )\n        chunk_kwargs.update(\n            {\n                \"max_lines\": max_lines,\n                \"max_functions\": max_functions,\n                \"docstring_mode\": docstring_mode,\n                \"strict\": strict,\n                \"include_comments\": include_comments,\n            }\n        )\n    else:\n        if DocumentChunker is None:\n            typer.echo(\n                \"Error: DocumentChunker dependencies not available.\\n\"\n                \"Please install with: pip install chunklet-py[structured-document]\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        chunker_instance = DocumentChunker(\n            verbose=verbose,\n            token_counter=token_counter,\n        )\n        chunk_kwargs.update(\n            {\n                \"lang\": lang,\n                \"max_sentences\": max_sentences,\n                \"max_section_breaks\": max_section_breaks,\n                \"overlap_percent\": overlap_percent,\n                \"offset\": offset,\n            }\n        )\n\n    # --- Chunking logic ---\n    if text:\n        chunks = chunker_instance.chunk_text(\n            text,\n            **chunk_kwargs,\n        )\n    else:\n        file_paths = _extract_files(source)\n\n        if len(file_paths) == 1 and file_paths[0].suffix not in {\n            \".docx\",\n            \".epub\",\n            \".pdf\",\n            \".odt\",\n        }:\n            single_file = file_paths[0]\n            chunks = chunker_instance.chunk_file(\n                single_file,\n                **chunk_kwargs,\n            )\n        else:\n            # Batch input logic\n            chunks = chunker_instance.chunk_files(\n                file_paths,\n                n_jobs=n_jobs,\n                show_progress=True,\n                on_errors=on_errors,\n                **chunk_kwargs,\n            )\n\n    if not chunks:\n        typer.echo(\n            \"Warning: No chunks were generated. \"\n            \"This might be because the input was empty or did not contain any processable content.\",\n            err=True,\n        )\n        raise typer.Exit(code=0)\n\n    if destination:\n        _write_chunks(chunks, destination, metadata)\n    else:\n        _print_chunks(chunks, destination, metadata)\n</code></pre>"},{"location":"reference/chunklet/cli/#chunklet.cli.split_command","title":"split_command","text":"<pre><code>split_command(\n    text: Optional[str] = typer.Argument(\n        None,\n        help=\"The input text to split. If not provided, --source must be used.\",\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        \"-l\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\",\n        \"-v\",\n        help=\"Enable verbose logging.\",\n    ),\n)\n</code></pre> <p>Split text or a single file into sentences</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(name=\"split\", help=\"Splits text or a single file into sentences.\")\ndef split_command(\n    text: Optional[str] = typer.Argument(\n        None, help=\"The input text to split. If not provided, --source must be used.\"\n    ),\n    source: Optional[Path] = typer.Option(\n        None,\n        \"--source\",\n        \"-s\",\n        help=\"Path to a single file to read input from. Cannot be a directory or multiple files.\",\n    ),\n    destination: Optional[Path] = typer.Option(\n        None,\n        \"--destination\",\n        \"-d\",\n        help=\"Path to a single file to write the segmented sentences (separated by \\\\n). Cannot be a directory.\",\n    ),\n    lang: str = typer.Option(\n        \"auto\",\n        \"--lang\",\n        \"-l\",\n        help=\"Language of the text (e.g., 'en', 'fr', 'auto').\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging.\"\n    ),\n):\n    \"\"\"Split text or a single file into sentences\"\"\"\n    # Validation and Input Acquisition\n    provided_inputs = [arg for arg in [text, source] if arg is not None]\n\n    if len(provided_inputs) == 0:\n        typer.echo(\n            \"Error: No input provided. Please use a text argument or the --source option.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if len(provided_inputs) &gt; 1:\n        typer.echo(\n            \"Error: Provide either a text string, or use the --source option, but not both.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    if source:\n        # --- Source Constraints ---\n        if source.is_dir():\n            typer.echo(\n                f\"Error: Source path '{source}' cannot be a directory for the 'split' command.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n        if not source.is_file():\n            typer.echo(\n                f\"Error: Source path '{source}' not found or is not a file.\",\n                err=True,\n            )\n            raise typer.Exit(code=1)\n\n        try:\n            input_text = source.read_text(encoding=\"utf-8\")\n        except Exception as e:\n            typer.echo(f\"Error reading source file: {e}\", err=True)\n            raise typer.Exit(code=1) from None\n    else:\n        input_text = text\n\n    # --- Destination Constraint ---\n    if destination and destination.is_dir():\n        typer.echo(\n            f\"Error: Destination path '{destination}' cannot be a directory for the 'split' command.\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Split Logic\n    splitter = SentenceSplitter(verbose=verbose)\n\n    if source:\n        sentences = splitter.split_file(source, lang=lang or \"auto\")\n        lang_detected, confidence = splitter.detected_top_language(\n            source.read_text(encoding=\"utf-8\")\n        )\n    else:\n        sentences = splitter.split_text(input_text, lang=lang or \"auto\")\n        lang_detected, confidence = splitter.detected_top_language(input_text)\n\n    # Output Handling\n    if destination:\n        output_str = \"\\n\".join(sentences)\n        source_display = f\"from {source.name}\" if source else \"(from stdin)\"\n\n        try:\n            destination.write_text(output_str, encoding=\"utf-8\")\n            typer.echo(\n                f\"Successfully split and wrote {len(sentences)} sentences \"\n                f\"{source_display} to {destination} (Confidence: {confidence})\",\n                err=True,\n            )\n        except Exception as e:\n            typer.echo(f\"Error writing to destination file: {e}\", err=True)\n            raise typer.Exit(code=1) from None\n    else:\n        source_display = f\"Source: {source.name}\" if source else \"Source: stdin\"\n\n        typer.echo(\n            f\"--- Sentences ({len(sentences)}): \"\n            f\" [{source_display} | Lang: {lang.upper()} | Confidence: {confidence}] ---\"\n        )\n\n        for sentence in sentences:\n            typer.echo(sentence)\n</code></pre>"},{"location":"reference/chunklet/cli/#chunklet.cli.visualize_command","title":"visualize_command","text":"<pre><code>visualize_command(\n    host: str = typer.Option(\n        \"127.0.0.1\",\n        \"--host\",\n        \"-h\",\n        help=\"Host IP to bind the visualizer server.\",\n    ),\n    port: int = typer.Option(\n        8000,\n        \"--port\",\n        \"-p\",\n        help=\"Port number to run the visualizer server.\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=\"A shell command to use for token counting in the visualizer. The command should take text as stdin and output the token count as a number.\",\n    ),\n    tokenizer_timeout: int | None = typer.Option(\n        None,\n        \"--tokenizer-timeout\",\n        \"-t\",\n        help=\"Timeout in seconds for the tokenizer command.\",\n    ),\n    headless: bool = typer.Option(\n        False,\n        \"--headless\",\n        help=\"Run visualizer in headless mode (don't open browser automatically).\",\n    ),\n)\n</code></pre> <p>Start the web-based chunk visualizer interface for interactive text and code chunking.</p> Source code in <code>src/chunklet/cli.py</code> <pre><code>@app.command(\n    name=\"visualize\",\n    help=\"Start the web-based chunk visualizer interface for interactive text and code chunking.\",\n)\ndef visualize_command(\n    host: str = typer.Option(\n        \"127.0.0.1\",\n        \"--host\",\n        \"-h\",\n        help=\"Host IP to bind the visualizer server.\",\n    ),\n    port: int = typer.Option(\n        8000,\n        \"--port\",\n        \"-p\",\n        help=\"Port number to run the visualizer server.\",\n    ),\n    tokenizer_command: Optional[str] = typer.Option(\n        None,\n        \"--tokenizer-command\",\n        help=(\n            \"A shell command to use for token counting in the visualizer. \"\n            \"The command should take text as stdin and output the token count as a number.\"\n        ),\n    ),\n    tokenizer_timeout: int | None = typer.Option(\n        None,\n        \"--tokenizer-timeout\",\n        \"-t\",\n        help=\"Timeout in seconds for the tokenizer command.\",\n    ),\n    headless: bool = typer.Option(\n        False,\n        \"--headless\",\n        help=\"Run visualizer in headless mode (don't open browser automatically).\",\n    ),\n):\n    \"\"\"\n    Start the web-based chunk visualizer interface for interactive text and code chunking.\n    \"\"\"\n    if Visualizer is None:\n        typer.echo(\n            \"Error: Visualization dependencies not available.\\n\"\n            \"Please install with: pip install chunklet-py[visualization]\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Check if port is available\n    url = f\"http://{host}:{port}\"\n    if not _check_port_available(host, port):\n        typer.echo(f\"Error: Port {port} is already in use on {host}\", err=True)\n        typer.echo(\"Options:\", err=True)\n        typer.echo(f\"  1. Stop the process currently occupying {url}\", err=True)\n        typer.echo(\n            \"  2. Use a different port: chunklet visualize --port &lt;different_port&gt;\",\n            err=True,\n        )\n        typer.echo(\n            \"  3. Find the PID:\\n\"\n            f\"     - Linux: 'ss -tunlp | grep :{port}' or 'fuser {port}/tcp'\\n\"\n            f\"     - Windows: 'netstat -ano | findstr :{port}'\\n\"\n            f\"     - Mac: 'lsof -i :{port}'\",\n            err=True,\n        )\n        raise typer.Exit(code=1)\n\n    # Create token counter if tokenizer command provided\n    token_counter = None\n    if tokenizer_command:\n        token_counter = _create_external_tokenizer(tokenizer_command, tokenizer_timeout)\n\n    # Start the visualizer\n    visualizer = Visualizer(host=host, port=port, token_counter=token_counter)\n\n    typer.echo(\"Starting Chunklet Visualizer...\")\n    typer.echo(f\"URL: {url}\")\n    typer.echo(\"Press Ctrl+C to stop the server\")\n\n    if not headless:\n        import webbrowser\n\n        try:\n            webbrowser.open(url)\n            typer.echo(\"Opened in default browser\")\n        except Exception as e:\n            typer.echo(f\"Could not open browser: {e}\", err=True)\n\n    try:\n        visualizer.serve()\n    except KeyboardInterrupt:\n        typer.echo(\"\\nVisualizer stopped.\")\n    except Exception as e:\n        typer.echo(f\"Error starting visualizer: {e}\", err=True)\n        raise typer.Exit(code=1) from None\n</code></pre>"},{"location":"reference/chunklet/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/#chunklet.code_chunker","title":"chunklet.code_chunker","text":"<p>Modules:</p> <ul> <li> <code>code_chunker</code>           \u2013            <p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> </li> <li> <code>patterns</code>           \u2013            <p>regex_patterns.py</p> </li> <li> <code>utils</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/","title":"_code_structure_extractor","text":""},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor","title":"chunklet.code_chunker._code_structure_extractor","text":"<p>Internal module for extracting code structures from source code files.</p> <p>Provides functionality to parse and analyze code syntax trees, identifying functions, classes, namespaces, and other structural elements. This module is used by CodeChunker to understand code structure before splitting into chunks.</p> <p>Classes:</p> <ul> <li> <code>CodeStructureExtractor</code>           \u2013            <p>Extracts structural units from source code.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor","title":"CodeStructureExtractor","text":"<pre><code>CodeStructureExtractor(verbose: bool = False)\n</code></pre> <p>Extracts structural units from source code.</p> <p>This class provides functionality to parse source code files and identify functions, classes, namespaces, and other structural elements using a language-agnostic approach.</p> <p>Methods:</p> <ul> <li> <code>extract_code_structure</code>             \u2013              <p>Preprocess and parse code into individual snippet boxes.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/_code_structure_extractor.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure","title":"extract_code_structure","text":"<pre><code>extract_code_structure(\n    code: str,\n    include_comments: bool,\n    docstring_mode: str,\n    is_python_code: bool = False,\n) -&gt; tuple[list[dict], tuple[int, ...]]\n</code></pre> <p>Preprocess and parse code into individual snippet boxes.</p> <p>This function-first extraction identifies functions as primary units while implicitly handling other structures within the function context.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[dict], tuple[int, ...]]</code>           \u2013            <p>tuple[list[dict], tuple[int, ...]]: A tuple containing the list of extracted code structure boxes and the line lengths.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/_code_structure_extractor.py</code> <pre><code>def extract_code_structure(\n    self,\n    code: str,\n    include_comments: bool,\n    docstring_mode: str,\n    is_python_code: bool = False,\n) -&gt; tuple[list[dict], tuple[int, ...]]:\n    \"\"\"\n    Preprocess and parse code into individual snippet boxes.\n\n    This function-first extraction identifies functions as primary units\n    while implicitly handling other structures within the function context.\n\n    Args:\n        code (str): Raw code string.\n        include_comments (bool): Whether to include comments in output.\n        docstring_mode (Literal[\"summary\", \"all\", \"excluded\"]): How to handle docstrings.\n        is_python_code (bool): Whether the code is Python.\n\n    Returns:\n        tuple[list[dict], tuple[int, ...]]: A tuple containing the list of extracted code structure boxes and the line lengths.\n    \"\"\"\n    if not code:\n        return [], ()\n\n    code, cumulative_lengths = self._preprocess(\n        code, include_comments, docstring_mode\n    )\n\n    state = {\n        \"curr_struct\": [],\n        \"block_indent_level\": 0,\n        \"snippet_dicts\": [],\n    }\n    buffer = defaultdict(list)\n\n    for line_no, line in enumerate(code.splitlines(), start=1):\n        indent_level = len(line) - len(line.lstrip())\n\n        # Detect annotated lines\n        matched = re.search(r\"\\(-- ([A-Z]+) --&gt;\\) \", line)\n        if matched:\n            self._handle_annotated_line(\n                line=line,\n                line_no=line_no,\n                matched=matched,\n                buffer=buffer,\n                state=state,\n            )\n            continue\n\n        if buffer[\"STR\"]:\n            self._flush_snippet([], state[\"snippet_dicts\"], buffer)\n\n        # -- Manage block accumulation logic--\n\n        func_start = FUNCTION_DECLARATION.match(line)\n        func_start = func_start.group(0) if func_start else None\n\n        if not state[\"curr_struct\"]:  # Fresh block\n            state[\"curr_struct\"] = [\n                CodeLine(line_no, line, indent_level, func_start)\n            ]\n            state[\"block_indent_level\"] = indent_level\n            continue\n\n        # Block start triggered by functions or namespaces indentification\n        # You might think it is in the wrong place, but it isnt\n        self._handle_block_start(\n            line=line,\n            indent_level=indent_level,\n            buffer=buffer,\n            state=state,\n            code=code,\n            func_start=func_start,\n            is_python_code=is_python_code,\n        )\n\n        if (\n            line.strip()\n            and indent_level &lt;= state[\"block_indent_level\"]\n            and not (OPENER.match(line) or CLOSER.match(line))\n        ):  # Block end\n            state[\"block_indent_level\"] = indent_level\n            self._flush_snippet(\n                state[\"curr_struct\"], state[\"snippet_dicts\"], buffer\n            )\n\n        state[\"curr_struct\"].append(\n            CodeLine(line_no, line, indent_level, func_start)\n        )\n\n    # Append last snippet\n    if state[\"curr_struct\"]:\n        self._flush_snippet(state[\"curr_struct\"], state[\"snippet_dicts\"], buffer)\n\n    snippet_dicts = self._post_processing(state[\"snippet_dicts\"])\n    log_info(\n        self.verbose, \"Extracted {} structural blocks from code\", len(snippet_dicts)\n    )\n\n    return snippet_dicts, cumulative_lengths\n</code></pre>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(code)","title":"<code>code</code>","text":"(<code>str</code>)           \u2013            <p>Raw code string.</p>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to include comments in output.</p>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal[summary, all, excluded]</code>)           \u2013            <p>How to handle docstrings.</p>"},{"location":"reference/chunklet/code_chunker/_code_structure_extractor/#chunklet.code_chunker._code_structure_extractor.CodeStructureExtractor.extract_code_structure(is_python_code)","title":"<code>is_python_code</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the code is Python.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/","title":"code_chunker","text":""},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker","title":"chunklet.code_chunker.code_chunker","text":"<p>Author: Speedyk-005 | Copyright (c) 2025 | License: MIT</p> <p>Language-Agnostic Code Chunking Utility</p> <p>This module provides a robust, convention-aware engine for segmenting source code into semantic units (\"chunks\") such as functions, classes, namespaces, and logical blocks.</p> <p>The chunking process uses pattern-based line-by-line processing to identify code structures and track context through indentation levels, enabling accurate detection of nested structures while respecting language-specific syntax.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker--limitations","title":"Limitations","text":"<p><code>CodeChunker</code> assumes syntactically conventional code. Highly obfuscated, minified, or macro-generated sources may not fully respect its boundary patterns, though such cases fall outside its intended domain.</p> Inspired by <ul> <li>Camel.utils.chunker.CodeChunker (@ CAMEL-AI.org)</li> <li>code-chunker by JimAiMoment</li> <li>whats_that_code by matthewdeanmartin</li> <li>CintraAI Code Chunker</li> </ul> <p>Classes:</p> <ul> <li> <code>CodeChunker</code>           \u2013            <p>Language-agnostic code chunking utility for semantic code segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker","title":"CodeChunker","text":"<pre><code>CodeChunker(\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseChunker</code></p> <p>Language-agnostic code chunking utility for semantic code segmentation.</p> <p>Extracts structural units (functions, classes, namespaces) from source code across multiple programming languages using pattern-based detection and token-aware segmentation.</p> Key Features <ul> <li>Cross-language support (Python, C/C++, Java, C#, JavaScript, Go, etc.)</li> <li>Structural analysis with namespace hierarchy tracking</li> <li>Configurable token limits with strict/lenient overflow handling</li> <li>Flexible docstring and comment processing modes</li> <li>Accurate line number preservation and source tracking</li> <li>Parallel batch processing for multiple files</li> <li>Comprehensive logging and progress tracking</li> </ul> <p>Initialize the CodeChunker with optional token counter and verbosity control.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Batch chunk multiple code sources.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunk code into semantic pieces.</p> </li> <li> <code>chunk_file</code>             \u2013              <p>Extract semantic code chunks from source using multi-dimensional analysis.</p> </li> <li> <code>chunk_files</code>             \u2013              <p>Process multiple source files or code strings in parallel.</p> </li> <li> <code>chunk_text</code>             \u2013              <p>Extract semantic code chunks from source using multi-dimensional analysis.</p> </li> <li> <code>chunk_texts</code>             \u2013              <p>Process multiple source files or code strings in parallel.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbose setting.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    verbose: bool = False,\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize the CodeChunker with optional token counter and verbosity control.\n\n    Args:\n        verbose (bool): Enable verbose logging.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n    \"\"\"\n    self.token_counter = token_counter\n    self._verbose = verbose\n    self.extractor = CodeStructureExtractor(verbose=self._verbose)\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbose setting.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    sources: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Batch chunk multiple code sources.</p> Note <p>Deprecated since v2.2.0. Will be removed in v3.0.0. Use <code>chunk_files</code> instead.</p> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"chunk_texts or chunk_files\",\n    deprecated_in=\"2.2.0\",\n    removed_in=\"3.0.0\",\n)\ndef batch_chunk(  # pragma: no cover\n    self,\n    sources: \"restricted_iterable(str | Path)\",  # pyright: ignore\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Batch chunk multiple code sources.\n\n    Note:\n        Deprecated since v2.2.0. Will be removed in v3.0.0. Use `chunk_files` instead.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=sources,\n        iterable_name=\"sources\",\n        separator=separator,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True\n) -&gt; list[Box]\n</code></pre> <p>Chunk code into semantic pieces.</p> Note <p>Deprecated since v2.2.0. Will be removed in v3.0.0. Use <code>chunk_file</code> or <code>chunk_text</code> instead.</p> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"chunk_text or chunk_file\",\n    deprecated_in=\"2.2.0\",\n    removed_in=\"3.0.0\",\n)\ndef chunk(  # pragma: no cover\n    self,\n    source: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunk code into semantic pieces.\n\n    Note:\n        Deprecated since v2.2.0. Will be removed in v3.0.0. Use `chunk_file` or `chunk_text` instead.\n    \"\"\"\n    if isinstance(source, Path) or (\n        isinstance(source, str) and is_path_like(source)\n    ):\n        return self.chunk_file(\n            path=source,\n            max_tokens=max_tokens,\n            max_lines=max_lines,\n            max_functions=max_functions,\n            token_counter=token_counter,\n            include_comments=include_comments,\n            docstring_mode=docstring_mode,\n            strict=strict,\n        )\n    return self.chunk_text(\n        code=source,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file","title":"chunk_file","text":"<pre><code>chunk_file(\n    path: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True\n) -&gt; list[Box]\n</code></pre> <p>Extract semantic code chunks from source using multi-dimensional analysis. Processes source code by identifying structural boundaries (functions, classes, namespaces) and grouping content based on multiple constraints including tokens, lines, and logical units while preserving semantic coherence.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of code chunks with metadata. Each Box contains:</p> <ul> <li>content (str): Code content</li> <li>tree (str): Namespace hierarchy</li> <li>start_line (int): Starting line in original source</li> <li>end_line (int): Ending line in original source</li> <li>span (tuple[int, int]): Character-level span (start and end offsets) in the original source.</li> <li>source_path (str): Source file path</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid configuration parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk_file(\n    self,\n    path: str | Path,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n) -&gt; list[Box]:\n    \"\"\"\n    Extract semantic code chunks from source using multi-dimensional analysis.\n    Processes source code by identifying structural boundaries (functions, classes,\n    namespaces) and grouping content based on multiple constraints including\n    tokens, lines, and logical units while preserving semantic coherence.\n\n    Args:\n        path (str | Path): File path to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable, optional): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode (Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\".\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n\n    Returns:\n        list[Box]: List of code chunks with metadata. Each Box contains:\n\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path\n\n    Raises:\n        InvalidInputError: Invalid configuration parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    path = Path(path)\n    code = read_text_file(path)\n\n    if not code.strip():\n        log_info(self.verbose, \"Input code is empty. Returning empty list.\")\n        return []\n\n    log_info(self.verbose, \"Starting chunk processing for file: {}\", path)\n\n    return self.chunk_text(\n        code=code,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>File path to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy:</p> <ul> <li>\"summary\": Include only first line of docstrings</li> <li>\"all\": Include complete docstrings</li> <li>\"excluded\": Remove all docstrings Defaults to \"all\".</li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_file(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files","title":"chunk_files","text":"<pre><code>chunk_files(\n    paths: restricted_iterable(str | Path),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple source files or code strings in parallel. Leverages multiprocessing to efficiently chunk multiple code sources, applying consistent chunking rules across all inputs.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata. Includes:</p> <ul> <li>content (str): Code content</li> <li>tree (str): Namespace hierarchy</li> <li>start_line (int): Starting line in original source</li> <li>end_line (int): Ending line in original source</li> <li>span (tuple[int, int]): Character-level span (start and end offsets) in the original source.</li> <li>source_path (str): Source file path</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid input parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>FileProcessingError</code>             \u2013            <p>Source file cannot be read.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk_files(\n    self,\n    paths: \"restricted_iterable(str | Path)\",  # pyright: ignore\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple source files or code strings in parallel.\n    Leverages multiprocessing to efficiently chunk multiple code sources,\n    applying consistent chunking rules across all inputs.\n\n    Args:\n        paths (restricted_iterable[str | Path]): A restricted iterable of file paths to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable | None): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.\n        n_jobs (int | None): Number of parallel workers. Uses all available CPUs if None.\n        show_progress (bool): Display progress bar during processing. Defaults to True.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to 'raise'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n            Includes:\n\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): Source file path\n\n    Raises:\n        InvalidInputError: Invalid input parameters.\n        MissingTokenCounterError: No token counter available.\n        FileProcessingError: Source file cannot be read.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk_file,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=paths,\n        iterable_name=\"paths\",\n        separator=separator,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(paths)","title":"<code>paths</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of file paths to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy:</p> <ul> <li>\"summary\": Include only first line of docstrings</li> <li>\"all\": Include complete docstrings</li> <li>\"excluded\": Remove all docstrings Defaults to \"all\"</li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers. Uses all available CPUs if None.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Display progress bar during processing. Defaults to True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_files(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text","title":"chunk_text","text":"<pre><code>chunk_text(\n    code: str,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True\n) -&gt; list[Box]\n</code></pre> <p>Extract semantic code chunks from source using multi-dimensional analysis. Processes source code by identifying structural boundaries (functions, classes, namespaces) and grouping content based on multiple constraints including tokens, lines, and logical units while preserving semantic coherence.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: List of code chunks with metadata. Each Box contains:</p> <ul> <li>content (str): Code content</li> <li>tree (str): Namespace hierarchy</li> <li>start_line (int): Starting line in original source</li> <li>end_line (int): Ending line in original source</li> <li>span (tuple[int, int]): Character-level span (start and end offsets) in the original source.</li> <li>source_path (str): \"N/A\"</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid configuration parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk_text(\n    self,\n    code: str,\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n) -&gt; list[Box]:\n    \"\"\"\n    Extract semantic code chunks from source using multi-dimensional analysis.\n    Processes source code by identifying structural boundaries (functions, classes,\n    namespaces) and grouping content based on multiple constraints including\n    tokens, lines, and logical units while preserving semantic coherence.\n\n    Args:\n        code (str | Path): Raw code string or file path to process.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable, optional): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode (Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\".\n        strict (bool): If True, raise error when structural blocks exceed\n            max_tokens. If False, split oversized blocks. Default: True.\n\n    Returns:\n        list[Box]: List of code chunks with metadata. Each Box contains:\n\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid configuration parameters.\n        MissingTokenCounterError: No token counter available.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    self._validate_constraints(max_tokens, max_lines, max_functions, token_counter)\n\n    if max_tokens is None:\n        max_tokens = sys.maxsize\n    if max_lines is None:\n        max_lines = sys.maxsize\n    if max_functions is None:\n        max_functions = sys.maxsize\n\n    token_counter = token_counter or self.token_counter\n\n    if not code.strip():\n        log_info(self.verbose, \"Input code is empty. Returning empty list.\")\n        return []\n\n    log_info(\n        self.verbose,\n        \"Starting chunk processing for code starting with:\\n```\\n{}...\\n```\",\n        code[:100],\n    )\n\n    snippet_dicts, cumulative_lengths = self.extractor.extract_code_structure(\n        code, include_comments, docstring_mode, is_python_code(code)\n    )\n\n    result_chunks = self._group_by_chunk(\n        snippet_dicts=snippet_dicts,\n        cumulative_lengths=cumulative_lengths,\n        token_counter=token_counter,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        strict=strict,\n        source=code,\n    )\n\n    log_info(self.verbose, \"Generated {} chunk(s) for the code\", len(result_chunks))\n\n    return result_chunks\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(code)","title":"<code>code</code>","text":"(<code>str | Path</code>)           \u2013            <p>Raw code string or file path to process.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy:</p> <ul> <li>\"summary\": Include only first line of docstrings</li> <li>\"all\": Include complete docstrings</li> <li>\"excluded\": Remove all docstrings Defaults to \"all\".</li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_text(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts","title":"chunk_texts","text":"<pre><code>chunk_texts(\n    codes: restricted_iterable(str),\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\n        \"summary\", \"all\", \"excluded\"\n    ] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Process multiple source files or code strings in parallel. Leverages multiprocessing to efficiently chunk multiple code sources, applying consistent chunking rules across all inputs.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata. Includes:</p> <ul> <li>content (str): Code content</li> <li>tree (str): Namespace hierarchy</li> <li>start_line (int): Starting line in original source</li> <li>end_line (int): Ending line in original source</li> <li>span (tuple[int, int]): Character-level span (start and end offsets) in the original source.</li> <li>source_path (str): \"N/A\"</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>Invalid input parameters.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>No token counter available.</p> </li> <li> <code>TokenLimitError</code>             \u2013            <p>Structural block exceeds max_tokens in strict mode.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/code_chunker.py</code> <pre><code>@validate_input\ndef chunk_texts(\n    self,\n    codes: \"restricted_iterable(str)\",  # pyright: ignore\n    *,\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_lines: Annotated[int | None, Field(ge=5)] = None,\n    max_functions: Annotated[int | None, Field(ge=1)] = None,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    include_comments: bool = True,\n    docstring_mode: Literal[\"summary\", \"all\", \"excluded\"] = \"all\",\n    strict: bool = True,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Process multiple source files or code strings in parallel.\n    Leverages multiprocessing to efficiently chunk multiple code sources,\n    applying consistent chunking rules across all inputs.\n\n    Args:\n        codes (restricted_iterable[str]): A restricted iterable of raw code strings.\n        max_tokens (int, optional): Maximum tokens per chunk. Must be &gt;= 12.\n        max_lines (int, optional): Maximum number of lines per chunk. Must be &gt;= 5.\n        max_functions (int, optional): Maximum number of functions per chunk. Must be &gt;= 1.\n        token_counter (Callable | None): Token counting function. Uses instance\n            counter if None. Required for token-based chunking.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        include_comments (bool): Include comments in output chunks. Default: True.\n        docstring_mode(Literal[\"summary\", \"all\", \"excluded\"]): Docstring processing strategy:\n\n            - \"summary\": Include only first line of docstrings\n            - \"all\": Include complete docstrings\n            - \"excluded\": Remove all docstrings\n            Defaults to \"all\"\n        strict (bool): If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.\n        n_jobs (int | None): Number of parallel workers. Uses all available CPUs if None.\n        show_progress (bool): Display progress bar during processing. Defaults to True.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to 'raise'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n            Includes:\n\n            - content (str): Code content\n            - tree (str): Namespace hierarchy\n            - start_line (int): Starting line in original source\n            - end_line (int): Ending line in original source\n            - span (tuple[int, int]): Character-level span (start and end offsets) in the original source.\n            - source_path (str): \"N/A\"\n\n    Raises:\n        InvalidInputError: Invalid input parameters.\n        MissingTokenCounterError: No token counter available.\n        TokenLimitError: Structural block exceeds max_tokens in strict mode.\n        CallbackError: If the token counter fails or returns an invalid type.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk_text,\n        max_tokens=max_tokens,\n        max_lines=max_lines,\n        max_functions=max_functions,\n        token_counter=token_counter or self.token_counter,\n        include_comments=include_comments,\n        docstring_mode=docstring_mode,\n        strict=strict,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=codes,\n        iterable_name=\"codes\",\n        separator=separator,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(codes)","title":"<code>codes</code>","text":"(<code>restricted_iterable[str]</code>)           \u2013            <p>A restricted iterable of raw code strings.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(max_lines)","title":"<code>max_lines</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of lines per chunk. Must be &gt;= 5.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(max_functions)","title":"<code>max_functions</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of functions per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Token counting function. Uses instance counter if None. Required for token-based chunking.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(include_comments)","title":"<code>include_comments</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Include comments in output chunks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(docstring_mode)","title":"<code>docstring_mode</code>","text":"(<code>Literal['summary', 'all', 'excluded']</code>, default:                   <code>'all'</code> )           \u2013            <p>Docstring processing strategy:</p> <ul> <li>\"summary\": Include only first line of docstrings</li> <li>\"all\": Include complete docstrings</li> <li>\"excluded\": Remove all docstrings Defaults to \"all\"</li> </ul>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(strict)","title":"<code>strict</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise error when structural blocks exceed max_tokens. If False, split oversized blocks. Default: True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers. Uses all available CPUs if None.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Display progress bar during processing. Defaults to True.</p>"},{"location":"reference/chunklet/code_chunker/code_chunker/#chunklet.code_chunker.code_chunker.CodeChunker.chunk_texts(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/code_chunker/patterns/","title":"patterns","text":""},{"location":"reference/chunklet/code_chunker/patterns/#chunklet.code_chunker.patterns","title":"chunklet.code_chunker.patterns","text":"<p>regex_patterns.py</p> <p>Written by: Speedyk-005 Copyright 2025 License: MIT</p> <p>This module contains regular expressions for chunking and parsing source code across multiple programming languages. The patterns are designed to match:</p> <ul> <li>General multiline string</li> <li>Single-line comments (Python, C/C++, Java, JavaScript, Lisp, etc.)</li> <li>Multi-line comments / docstrings (Python, C-style, Ruby, Lisp, etc.)</li> <li>Function or method definitions across various languages</li> <li>Namespaces, classes, modules, and interfaces</li> <li>Annotations / decorators (Python, C#, Java, Php, etc.)</li> <li>Block-ending indicators (closing symbol or keyword)</li> </ul> <p>These regexes can be imported into a chunker or parser to identify logical sections of code for semantic analysis, tokenization, or processing.</p> Note <ul> <li>re.M = multiline (^,$ match each line)</li> <li>re.S = DOTALL (. matches newline)</li> </ul>"},{"location":"reference/chunklet/code_chunker/utils/","title":"utils","text":""},{"location":"reference/chunklet/code_chunker/utils/#chunklet.code_chunker.utils","title":"chunklet.code_chunker.utils","text":"<p>Functions:</p> <ul> <li> <code>is_python_code</code>             \u2013              <p>Check if a source is written in Python.</p> </li> </ul>"},{"location":"reference/chunklet/code_chunker/utils/#chunklet.code_chunker.utils.is_python_code","title":"is_python_code","text":"<pre><code>is_python_code(source: str | Path) -&gt; bool\n</code></pre> <p>Check if a source is written in Python.</p> <p>This function uses multiple indicators, prioritizing syntactic validity via the Abstract Syntax Tree (AST) parser for maximum confidence.</p> Indicators used <ul> <li>File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).</li> <li>Shebang line detection (e.g., \"#!/usr/bin/python\").</li> <li>Definitive syntax check using Python's <code>ast.parse()</code>.</li> <li>Fallback heuristic via Pygments lexer guessing.</li> </ul> Note <p>The function is definitive for complete, syntactically correct code blocks. It falls back to a Pygments heuristic only for short, incomplete, or ambiguous code snippets that fail AST parsing.</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the source is written in Python.</p> </li> </ul> Source code in <code>src/chunklet/code_chunker/utils.py</code> <pre><code>@validate_input\ndef is_python_code(source: str | Path) -&gt; bool:\n    \"\"\"\n    Check if a source is written in Python.\n\n    This function uses multiple indicators, prioritizing syntactic validity\n    via the Abstract Syntax Tree (AST) parser for maximum confidence.\n\n    Indicators used:\n      - File extension check for path inputs (e.g., .py, .pyi, .pyx, .pyw).\n      - Shebang line detection (e.g., \"#!/usr/bin/python\").\n      - Definitive syntax check using Python's `ast.parse()`.\n      - Fallback heuristic via Pygments lexer guessing.\n\n    Note:\n        The function is definitive for complete, syntactically correct code blocks.\n        It falls back to a Pygments heuristic only for short, incomplete, or\n        ambiguous code snippets that fail AST parsing.\n\n    Args:\n        source (str | Path): raw code string or Path to source file to check.\n\n    Returns:\n        bool: True if the source is written in Python.\n    \"\"\"\n    # Path-based check\n    if isinstance(source, Path) or (isinstance(source, str) and is_path_like(source)):\n        path = Path(source)\n        return path.suffix.lower() in {\".py\", \".pyi\", \".pyx\", \".pyw\"}\n\n    if isinstance(source, str):\n        # Shebang line check\n        if re.match(r\"#!/usr/bin/(env\\s+)?python\", source.strip()):\n            return True\n\n        # Definitive syntactic check (Highest confidence)\n        try:\n            ast.parse(source)\n            # If parsing succeeds, it's definitely Python code\n            return True\n        except Exception:  # noqa: S110\n            # If fails, it might still be Python code (e.g., incomplete snippet), so continue with heuristics\n            pass\n\n    # Pygments heuristic (Lowest confidence, last resort)\n    try:\n        lexer = guess_lexer(source)\n        return lexer.name.lower() == \"python\"\n    except ClassNotFound:\n        return False\n</code></pre>"},{"location":"reference/chunklet/code_chunker/utils/#chunklet.code_chunker.utils.is_python_code(source)","title":"<code>source</code>","text":"(<code>str | Path</code>)           \u2013            <p>raw code string or Path to source file to check.</p>"},{"location":"reference/chunklet/common/","title":"common","text":""},{"location":"reference/chunklet/common/#chunklet.common","title":"chunklet.common","text":"<p>Modules:</p> <ul> <li> <code>batch_runner</code>           \u2013            </li> <li> <code>deprecation</code>           \u2013            </li> <li> <code>logging_utils</code>           \u2013            </li> <li> <code>path_utils</code>           \u2013            </li> <li> <code>token_utils</code>           \u2013            </li> <li> <code>validation</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/","title":"batch_runner","text":""},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner","title":"chunklet.common.batch_runner","text":"<p>Functions:</p> <ul> <li> <code>capture_result_and_exception</code>             \u2013              <p>Decorator to capture result and exception from a function call.</p> </li> <li> <code>run_in_batch</code>             \u2013              <p>Processes a batch of items in parallel using multiprocessing.</p> </li> </ul>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.capture_result_and_exception","title":"capture_result_and_exception","text":"<pre><code>capture_result_and_exception(func)\n</code></pre> <p>Decorator to capture result and exception from a function call.</p> Source code in <code>src/chunklet/common/batch_runner.py</code> <pre><code>def capture_result_and_exception(func):\n    \"\"\"Decorator to capture result and exception from a function call.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        try:\n            res = func(*args, **kwargs)\n            return res, None\n        except Exception as e:\n            return None, e\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch","title":"run_in_batch","text":"<pre><code>run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of items in parallel using multiprocessing. Splits the iterable into chunks and executes the function on each.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> Source code in <code>src/chunklet/common/batch_runner.py</code> <pre><code>def run_in_batch(\n    func: Callable,\n    iterable_of_args: Iterable,\n    iterable_name: str,\n    n_jobs: int | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n    separator: Any = None,\n    verbose: bool = True,\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of items in parallel using multiprocessing.\n    Splits the iterable into chunks and executes the function on each.\n\n    Args:\n        func (Callable): The function to call for each argument.\n        iterable_of_args (Iterable): An iterable of inputs to process.\n        iterable_name: Name of the iterable. needed for logging and exception message.\n        n_jobs (int | None): Number of parallel workers to use.\n            If None, uses all available CPUs. Must be &gt;= 1 if specified.\n        show_progress (bool): Whether to display a progress bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]):\n            How to handle errors during processing. Defaults to \"raise\".\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        verbose (bool): Whether to enable verbose logging.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n    \"\"\"\n    from mpire import WorkerPool\n\n    total, iterable_of_args = safely_count_iterable(iterable_name, iterable_of_args)\n\n    log_info(verbose, \"Starting batch chunking for {} items.\", total)\n\n    if total == 0:\n        log_info(verbose, \"Input {} is empty. Returning empty iterator.\", iterable_name)\n        return iter([])\n\n    failed_count = 0\n    try:\n        with WorkerPool(n_jobs=n_jobs) as pool:\n            imap_func = pool.imap if separator is not None else pool.imap_unordered\n\n            progress_bar_options = {\n                \"bar_format\": \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]\",\n                \"desc\": \"Chunking ...\",\n            }\n\n            task_iter = imap_func(\n                capture_result_and_exception(func),\n                iterable_of_args,\n                iterable_len=total,\n                progress_bar=show_progress,\n                progress_bar_options=progress_bar_options,\n            )\n\n            for res, error in task_iter:\n                if error:\n                    failed_count += 1\n                    if on_errors == \"raise\":\n                        raise error\n                    elif on_errors == \"break\":\n                        logger.error(\n                            \"A task for {} failed. Returning partial results.\\nReason: {}\",\n                            iterable_name,\n                            error,\n                        )\n                        break\n\n                    #  Else: skip\n                    logger.warning(\"Skipping a failed task.\\nReason: {}\", error)\n                    continue\n\n                yield from res\n\n                if separator is not None:\n                    yield separator\n\n    finally:\n        log_info(\n            verbose,\n            \"Batch processing completed. {}/{} items processed successfully.\",\n            total - failed_count,\n            total,\n        )\n</code></pre>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(func)","title":"<code>func</code>","text":"(<code>Callable</code>)           \u2013            <p>The function to call for each argument.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_of_args)","title":"<code>iterable_of_args</code>","text":"(<code>Iterable</code>)           \u2013            <p>An iterable of inputs to process.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(iterable_name)","title":"<code>iterable_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the iterable. needed for logging and exception message.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display a progress bar.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to \"raise\".</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/common/batch_runner/#chunklet.common.batch_runner.run_in_batch(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enable verbose logging.</p>"},{"location":"reference/chunklet/common/deprecation/","title":"deprecation","text":""},{"location":"reference/chunklet/common/deprecation/#chunklet.common.deprecation","title":"chunklet.common.deprecation","text":"<p>Functions:</p> <ul> <li> <code>deprecated_callable</code>             \u2013              <p>Decorate a function or class with warning message.</p> </li> </ul>"},{"location":"reference/chunklet/common/deprecation/#chunklet.common.deprecation.deprecated_callable","title":"deprecated_callable","text":"<pre><code>deprecated_callable(\n    use_instead: str, deprecated_in: str, removed_in: str\n) -&gt; Callable\n</code></pre> <p>Decorate a function or class with warning message.</p> <p>This decorator marks a function or class as deprecated.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Callable</code> (              <code>Callable</code> )          \u2013            <p>Decorator function that wraps the source function/class.</p> </li> </ul> Source code in <code>src/chunklet/common/deprecation.py</code> <pre><code>def deprecated_callable(\n    use_instead: str,\n    deprecated_in: str,\n    removed_in: str,\n) -&gt; Callable:\n    \"\"\"Decorate a function or class with warning message.\n\n    This decorator marks a function or class as deprecated.\n\n    Args:\n        use_instead (str): Replacement name (e.g., \"split_text\", \"DocumentChunker\", or \"chunk_text or chunk_file\").\n        deprecated_in (str): Version when the function was deprecated (e.g., \"2.2.0\").\n        removed_in (str): Version when the function will be removed (e.g., \"3.0.0\").\n\n    Returns:\n        Callable: Decorator function that wraps the source function/class.\n    \"\"\"\n\n    def decorator(func_or_cls: Callable) -&gt; Callable:\n        warn_message = (\n            f\"`{func_or_cls.__qualname__}` was deprecated since v{deprecated_in} \"\n            f\"in favor of `{use_instead}`. It will be removed in v{removed_in}.\"\n        )\n        remove_message = (\n            f\"`{func_or_cls.__qualname__}` was removed in v{removed_in}. \"\n            f\"Use `{use_instead}` instead.\"\n        )\n\n        @functools.wraps(func_or_cls)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            if Version(CURRENT_VERSION) &gt;= Version(removed_in):\n                raise AttributeError(remove_message)\n            warnings.warn(warn_message, FutureWarning, stacklevel=2)\n            return func_or_cls(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/chunklet/common/deprecation/#chunklet.common.deprecation.deprecated_callable(use_instead)","title":"<code>use_instead</code>","text":"(<code>str</code>)           \u2013            <p>Replacement name (e.g., \"split_text\", \"DocumentChunker\", or \"chunk_text or chunk_file\").</p>"},{"location":"reference/chunklet/common/deprecation/#chunklet.common.deprecation.deprecated_callable(deprecated_in)","title":"<code>deprecated_in</code>","text":"(<code>str</code>)           \u2013            <p>Version when the function was deprecated (e.g., \"2.2.0\").</p>"},{"location":"reference/chunklet/common/deprecation/#chunklet.common.deprecation.deprecated_callable(removed_in)","title":"<code>removed_in</code>","text":"(<code>str</code>)           \u2013            <p>Version when the function will be removed (e.g., \"3.0.0\").</p>"},{"location":"reference/chunklet/common/logging_utils/","title":"logging_utils","text":""},{"location":"reference/chunklet/common/logging_utils/#chunklet.common.logging_utils","title":"chunklet.common.logging_utils","text":"<p>Functions:</p> <ul> <li> <code>log_info</code>             \u2013              <p>Log an info message if verbose is enabled.</p> </li> </ul>"},{"location":"reference/chunklet/common/logging_utils/#chunklet.common.logging_utils.log_info","title":"log_info","text":"<pre><code>log_info(verbose: bool, *args, **kwargs) -&gt; None\n</code></pre> <p>Log an info message if verbose is enabled.</p> <p>This is a convenience function that only logs when verbose mode is enabled, avoiding unnecessary log output in production.</p> <p>Parameters:</p> Example <p>log_info(True, \"Processing file: {}\", filepath) Processing file: /path/to/file log_info(False, \"This will not be logged\") (no output)</p> Source code in <code>src/chunklet/common/logging_utils.py</code> <pre><code>def log_info(verbose: bool, *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message if verbose is enabled.\n\n    This is a convenience function that only logs when verbose mode is enabled,\n    avoiding unnecessary log output in production.\n\n    Args:\n        verbose: If True, logs the message; if False, does nothing.\n        *args: Positional arguments passed to logger.info().\n        **kwargs: Keyword arguments passed to logger.info().\n\n    Example:\n        &gt;&gt;&gt; log_info(True, \"Processing file: {}\", filepath)\n        Processing file: /path/to/file\n        &gt;&gt;&gt; log_info(False, \"This will not be logged\")\n        (no output)\n    \"\"\"\n    if verbose:\n        logger.info(*args, **kwargs)\n</code></pre>"},{"location":"reference/chunklet/common/logging_utils/#chunklet.common.logging_utils.log_info(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>)           \u2013            <p>If True, logs the message; if False, does nothing.</p>"},{"location":"reference/chunklet/common/logging_utils/#chunklet.common.logging_utils.log_info(*args)","title":"<code>*args</code>","text":"\u2013            <p>Positional arguments passed to logger.info().</p>"},{"location":"reference/chunklet/common/logging_utils/#chunklet.common.logging_utils.log_info(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Keyword arguments passed to logger.info().</p>"},{"location":"reference/chunklet/common/path_utils/","title":"path_utils","text":""},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils","title":"chunklet.common.path_utils","text":"<p>Functions:</p> <ul> <li> <code>is_path_like</code>             \u2013              <p>Check if a string looks like a filesystem path (file or folder),</p> </li> <li> <code>read_text_file</code>             \u2013              <p>Read text file with automatic encoding detection.</p> </li> </ul>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like","title":"is_path_like","text":"<pre><code>is_path_like(text: str) -&gt; bool\n</code></pre> <p>Check if a string looks like a filesystem path (file or folder), including Unix/Windows paths, hidden files, and scripts without extensions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if string appears to be a filesystem path.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\nTrue\n&gt;&gt;&gt; is_path_like(\"C:\\Users\\User\\file.pdf\")\nTrue\n&gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\nTrue\n&gt;&gt;&gt; is_path_like(\".hidden_file\")\nTrue\n&gt;&gt;&gt; is_path_like(\"no_extension_script\")\nTrue\n&gt;&gt;&gt; is_path_like(\"path/with/newline\\nchar\")\nFalse\n&gt;&gt;&gt; is_path_like(\"string_with_null_byte\\x00\")\nFalse\n</code></pre> Source code in <code>src/chunklet/common/path_utils.py</code> <pre><code>@validate_input\ndef is_path_like(text: str) -&gt; bool:\n    \"\"\"\n    Check if a string looks like a filesystem path (file or folder),\n    including Unix/Windows paths, hidden files, and scripts without extensions.\n\n    Args:\n        text (str): text to check.\n\n    Returns:\n        bool: True if string appears to be a filesystem path.\n\n    Examples:\n        &gt;&gt;&gt; is_path_like(\"/home/user/document.txt\")\n        True\n        &gt;&gt;&gt; is_path_like(\"C:\\\\Users\\\\User\\\\file.pdf\")\n        True\n        &gt;&gt;&gt; is_path_like(\"folder/subfolder/script.sh\")\n        True\n        &gt;&gt;&gt; is_path_like(\".hidden_file\")\n        True\n        &gt;&gt;&gt; is_path_like(\"no_extension_script\")\n        True\n        &gt;&gt;&gt; is_path_like(\"path/with/newline\\\\nchar\")\n        False\n        &gt;&gt;&gt; is_path_like(\"string_with_null_byte\\\\x00\")\n        False\n    \"\"\"\n    if not text or \"\\n\" in text or \"\\0\" in text:\n        return False\n    if sys.platform == \"win32\" and any(c in text for c in '&lt;&gt;:\"|?*'):\n        return False\n\n    try:\n        # Attempt to call is_file() to trigger OS-level path validation,\n        # especially for path length.\n        Path(text).is_file()\n    except OSError as e:\n        # If an OSError occurs, check if it's specifically due to the name being too long.\n        if e.errno == errno.ENAMETOOLONG:\n            return False\n        else:\n            # For other OSErrors (e.g., permission denied, invalid characters not caught by initial checks),\n            # we let the regex check proceed, as the focus is on structural validity, not existence or access.\n            pass\n\n    return bool(PATH_PATTERN.match(text))\n</code></pre>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.is_path_like(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>text to check.</p>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.read_text_file","title":"read_text_file","text":"<pre><code>read_text_file(path: str | Path) -&gt; str\n</code></pre> <p>Read text file with automatic encoding detection.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>File content.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileProcessingError</code>             \u2013            <p>If file cannot be read.</p> </li> </ul> Source code in <code>src/chunklet/common/path_utils.py</code> <pre><code>@validate_input\ndef read_text_file(path: str | Path) -&gt; str:\n    \"\"\"Read text file with automatic encoding detection.\n\n    Args:\n        path: File path to read.\n\n    Returns:\n        str: File content.\n\n    Raises:\n        FileProcessingError: If file cannot be read.\n    \"\"\"\n    from charset_normalizer import from_path\n\n    path = Path(path)\n\n    if not path.exists():\n        raise FileProcessingError(f\"File does not exist: {path}\")\n\n    if _is_binary_file(path):\n        raise FileProcessingError(f\"Binary file not supported: {path}\")\n\n    match = from_path(str(path)).best()\n    return str(match) if match else \"\"\n</code></pre>"},{"location":"reference/chunklet/common/path_utils/#chunklet.common.path_utils.read_text_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>File path to read.</p>"},{"location":"reference/chunklet/common/token_utils/","title":"token_utils","text":""},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils","title":"chunklet.common.token_utils","text":"<p>Functions:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count tokens in a string using a provided token counting function.</p> </li> </ul>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens","title":"count_tokens  <code>cached</code>","text":"<pre><code>count_tokens(\n    text: str, token_counter: Callable[[str], int]\n) -&gt; int\n</code></pre> <p>Count tokens in a string using a provided token counting function.</p> <p>Wraps the token counting function with error handling. Ensures the returned value is numeric and converts it to an integer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Number of tokens.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the token counter fails or returns an invalid type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n...     return len(text.split())\n&gt;&gt;&gt; text = \"This is a sample sentence.\"\n&gt;&gt;&gt; count_tokens(text, simple_word_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n...     return len(text)\n&gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n5\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with a failing token counter\n&gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n...     raise ValueError(\"Something went wrong!\")\n&gt;&gt;&gt; try:\n...     count_tokens(\"test\", failing_counter)\n... except CallbackError as e:\n...     print(e)\nToken counter failed while processing text starting with: 'test...'.\n\ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\nDetails: Something went wrong!\n</code></pre> Source code in <code>src/chunklet/common/token_utils.py</code> <pre><code>@lru_cache(maxsize=1024)\ndef count_tokens(text: str, token_counter: Callable[[str], int]) -&gt; int:\n    \"\"\"\n    Count tokens in a string using a provided token counting function.\n\n    Wraps the token counting function with error handling. Ensures the returned\n    value is numeric and converts it to an integer.\n\n    Args:\n        text (str): Text to count tokens in.\n        token_counter (Callable[[str], int]): Function that returns the number of tokens.\n\n    Returns:\n        int: Number of tokens.\n\n    Raises:\n        CallbackError: If the token counter fails or returns an invalid type.\n\n    Examples:\n        &gt;&gt;&gt; def simple_word_counter(text: str) -&gt; int:\n        ...     return len(text.split())\n        &gt;&gt;&gt; text = \"This is a sample sentence.\"\n        &gt;&gt;&gt; count_tokens(text, simple_word_counter)\n        5\n\n        &gt;&gt;&gt; def char_counter(text: str) -&gt; int:\n        ...     return len(text)\n        &gt;&gt;&gt; count_tokens(\"hello\", char_counter)\n        5\n\n        &gt;&gt;&gt; # Example with a failing token counter\n        &gt;&gt;&gt; def failing_counter(text: str) -&gt; int:\n        ...     raise ValueError(\"Something went wrong!\")\n        &gt;&gt;&gt; try:\n        ...     count_tokens(\"test\", failing_counter)\n        ... except CallbackError as e:\n        ...     print(e)\n        Token counter failed while processing text starting with: 'test...'.\n        \ud83d\udca1 Hint: Please ensure the token counter function handles all edge cases and returns an integer.\n        Details: Something went wrong!\n    \"\"\"\n    try:\n        token_count = token_counter(text)\n        if isinstance(token_count, (int, float)):\n            return int(token_count)\n        raise CallbackError(\n            f\"Token counter returned invalid type ({type(token_count).__name__}) \"\n            f\"for text starting with: '{text[:100]}'\"\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"Token counter failed while processing text starting with: '{text[:100]}...'.\\n\"\n            \"\ud83d\udca1 Hint: Please ensure the token counter function handles \"\n            f\"all edge cases and returns an integer. \\nDetails: {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>Text to count tokens in.</p>"},{"location":"reference/chunklet/common/token_utils/#chunklet.common.token_utils.count_tokens(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>)           \u2013            <p>Function that returns the number of tokens.</p>"},{"location":"reference/chunklet/common/validation/","title":"validation","text":""},{"location":"reference/chunklet/common/validation/#chunklet.common.validation","title":"chunklet.common.validation","text":"<p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>restricted_iterable</code>             \u2013              <p>Creates a Pydantic Annotated type that represents a RestrictedIterable</p> </li> <li> <code>safely_count_iterable</code>             \u2013              <p>Counts elements in an iterable while preserving its state and forcing validation.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            (\n                f\"{ind}) {formatted_loc} {msg}.\\n\"\n                f\"  Found: (input={input_value!r}, type={input_type})\"\n            )\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.restricted_iterable","title":"restricted_iterable","text":"<pre><code>restricted_iterable(*hints: Any) -&gt; Any\n</code></pre> <p>Creates a Pydantic Annotated type that represents a RestrictedIterable containing the specified hints (*hints), and applies a PlainValidator to reject str input.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def restricted_iterable(*hints: Any) -&gt; Any:\n    \"\"\"\n    Creates a Pydantic Annotated type that represents a RestrictedIterable\n    containing the specified hints (*hints), and applies a PlainValidator\n    to reject str input.\n    \"\"\"\n\n    def enforce_non_string(v: Any) -&gt; Any:\n        if isinstance(v, str):\n            # Pydantic-Core is sometimes pickier; using ValueError often works better\n            # with external validators than a raw TypeError\n            # Sliced to avoid overflowing screen\n            input_val = v if len(v) &lt; 500 else v[:500] + \"...\"\n            raise ValueError(\n                f\"Input cannot be a string.\\n  Found: (input={input_val!r}, type=str)\"\n            )\n        return v\n\n    item_union = Union[hints] if len(hints) == 1 else hints[0]\n\n    target_type = (\n        list[item_union]\n        | tuple[item_union, ...]\n        | set[item_union]\n        | frozenset[item_union]\n        | Generator[item_union, None, None]\n    )\n\n    return Annotated[target_type, PlainValidator(enforce_non_string)]\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable","title":"safely_count_iterable","text":"<pre><code>safely_count_iterable(\n    name: str, iterable: Iterable\n) -&gt; tuple[int, Iterable]\n</code></pre> <p>Counts elements in an iterable while preserving its state and forcing validation.</p> <p>If the input is an Iterator, it is duplicated using <code>itertools.tee</code> to prevent consumption during counting. The iteration simultaneously triggers any underlying Pydantic item validation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[int, Iterable]</code>           \u2013            <p>tuple[int, Iterable]: The element count and the original (or preserved)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any element fails validation during the counting process.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With a list\n&gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 5\n&gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\nOriginal list is preserved: [1, 2, 3, 4, 5]\n</code></pre> <pre><code>&gt;&gt;&gt; # With an iterator (generator)\n&gt;&gt;&gt; my_iterator = (x for x in range(10))\n&gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n&gt;&gt;&gt; print(f\"Count: {count}\")\nCount: 10\n&gt;&gt;&gt; # The iterator is preserved and can still be consumed\n&gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\nSum of preserved iterator: 45\n</code></pre> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>@validate_input\ndef safely_count_iterable(name: str, iterable: Iterable) -&gt; tuple[int, Iterable]:\n    \"\"\"\n    Counts elements in an iterable while preserving its state and forcing validation.\n\n    If the input is an Iterator, it is duplicated using `itertools.tee` to prevent\n    consumption during counting. The iteration simultaneously triggers any\n    underlying Pydantic item validation.\n\n    Args:\n        name (str): Descriptive name for the iterable (used in error context).\n        iterable (Iterable): The iterable or iterator to count and validate.\n\n    Returns:\n        tuple[int, Iterable]: The element count and the original (or preserved)\n\n    Raises:\n        InvalidInputError: If any element fails validation during the counting process.\n\n    Examples:\n        &gt;&gt;&gt; # With a list\n        &gt;&gt;&gt; my_list = [1, 2, 3, 4, 5]\n        &gt;&gt;&gt; count, preserved_list = safely_count_iterable(\"my_list\", my_list)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 5\n        &gt;&gt;&gt; print(f\"Original list is preserved: {list(preserved_list)}\")\n        Original list is preserved: [1, 2, 3, 4, 5]\n\n        &gt;&gt;&gt; # With an iterator (generator)\n        &gt;&gt;&gt; my_iterator = (x for x in range(10))\n        &gt;&gt;&gt; count, preserved_iterator = safely_count_iterable(\"my_iterator\", my_iterator)\n        &gt;&gt;&gt; print(f\"Count: {count}\")\n        Count: 10\n        &gt;&gt;&gt; # The iterator is preserved and can still be consumed\n        &gt;&gt;&gt; print(f\"Sum of preserved iterator: {sum(preserved_iterator)}\")\n        Sum of preserved iterator: 45\n    \"\"\"\n    try:  # If pydantic wrap it as ValidatorIterator object\n        # Tee if it's an iterator\n        if isinstance(iterable, Iterator):\n            iterable, copy_iterable = tee(iterable)\n            count = ilen(copy_iterable)\n        else:\n            count = len(iterable)\n    except ValidationError as e:\n        e.subtitle = name  # to be less generic\n        e.hint = \"\ud83d\udca1 Hint: Ensure all elements in the iterable are valid.\"\n        raise InvalidInputError(pretty_errors(e)) from None\n\n    return count, iterable\n</code></pre>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(name)","title":"<code>name</code>","text":"(<code>str</code>)           \u2013            <p>Descriptive name for the iterable (used in error context).</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.safely_count_iterable(iterable)","title":"<code>iterable</code>","text":"(<code>Iterable</code>)           \u2013            <p>The iterable or iterator to count and validate.</p>"},{"location":"reference/chunklet/common/validation/#chunklet.common.validation.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker","title":"chunklet.document_chunker","text":"<p>Modules:</p> <ul> <li> <code>converters</code>           \u2013            </li> <li> <code>document_chunker</code>           \u2013            </li> <li> <code>processors</code>           \u2013            </li> <li> <code>registry</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways:</p> <ol> <li> <p>As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> </li> <li> <p>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</p> </li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            (\n                f\"{ind}) {formatted_loc} {msg}.\\n\"\n                f\"  Found: (input={input_value!r}, type={input_type})\"\n            )\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/#chunklet.document_chunker.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/","title":"_plain_text_chunker","text":""},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker","title":"chunklet.document_chunker._plain_text_chunker","text":"<p>Classes:</p> <ul> <li> <code>PlainTextChunker</code>           \u2013            <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker","title":"PlainTextChunker","text":"<pre><code>PlainTextChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>A powerful text chunking utility offering flexible strategies for optimal text segmentation.</p> Key Features <ul> <li>Flexible Constraint-Based Chunking: Segment text by specifying limits on sentence count, token count and section breaks or combination of them.</li> <li>Clause-Level Overlap: Ensures semantic continuity between chunks by overlapping</li> </ul> <p>at natural clause boundaries with Customizable continuation marker.     - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage.     - Pluggable Token Counters: Integrate custom token counting functions (e.g., for specific LLM tokenizers).     - Parallel Processing: Efficiently handles batch chunking of multiple texts using multiprocessing.     - Memory friendly batching: Yields chunks one at a time, reducing memory usage, especially for very large documents.</p> <p>Initialize The PlainTextChunker.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Processes a batch of texts in parallel, splitting each into chunks.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunks a single text into smaller pieces based on specified parameters.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/_plain_text_chunker.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initialize The PlainTextChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter, optional): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int], optional): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    # Initialize SentenceSplitter\n    self.sentence_splitter = sentence_splitter or SentenceSplitter()\n    self.sentence_splitter.verbose = self._verbose\n</code></pre>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int]</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Any, None, None]\n</code></pre> <p>Processes a batch of texts in parallel, splitting each into chunks. Leverages multiprocessing for efficient batch chunking.</p> <p>If a task fails, <code>chunklet</code> will now stop processing and return the results of the tasks that completed successfully, preventing wasted work.</p> <p>Parameters:</p> <p>Yields:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>A <code>Box</code> object containing the chunk content and metadata, or any separator object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If <code>texts</code> is not an iterable of strings, or if <code>n_jobs</code> is less than 1.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/_plain_text_chunker.py</code> <pre><code>@validate_input\ndef batch_chunk(\n    self,\n    texts: \"restricted_iterable(str)\",  # pyright: ignore\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    base_metadata: dict[str, Any] | None = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Any, None, None]:\n    \"\"\"\n    Processes a batch of texts in parallel, splitting each into chunks.\n    Leverages multiprocessing for efficient batch chunking.\n\n    If a task fails, `chunklet` will now stop processing and return the results\n    of the tasks that completed successfully, preventing wasted work.\n\n    Args:\n        texts (restricted_iterable[str]): A restricted iterable of input texts to be chunked.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): The token counting function.\n            Required if `max_tokens` is set.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n            Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors (Literal[\"raise\", \"skip\", \"break\"]): How to handle errors during processing.\n            Defaults to 'raise'.\n\n    Yields:\n        Any: A `Box` object containing the chunk content and metadata, or any separator object.\n\n    Raises:\n        InvalidInputError: If `texts` is not an iterable of strings, or if `n_jobs` is less than 1.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting\n            or token counting within a chunking task.\n    \"\"\"\n    chunk_func = partial(\n        self.chunk,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        overlap_percent=overlap_percent,\n        max_section_breaks=max_section_breaks,\n        offset=offset,\n        base_metadata=base_metadata,\n        token_counter=token_counter or self.token_counter,\n    )\n\n    yield from run_in_batch(\n        func=chunk_func,\n        iterable_of_args=texts,\n        iterable_name=\"texts\",\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n        separator=separator,\n        verbose=self.verbose,\n    )\n</code></pre>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(texts)","title":"<code>texts</code>","text":"(<code>restricted_iterable[str]</code>)           \u2013            <p>A restricted iterable of input texts to be chunked.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>The token counting function. Required if <code>max_tokens</code> is set.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs. Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.batch_chunk(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Defaults to 'raise'.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single text into smaller pieces based on specified parameters. Supports flexible constraint-based chunking, clause-level overlap, and custom token counters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each containing the chunk content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any chunking configuration parameter is invalid.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If an error occurs during sentence splitting or token counting within a chunking task.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/_plain_text_chunker.py</code> <pre><code>@validate_input\ndef chunk(\n    self,\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single text into smaller pieces based on specified parameters.\n    Supports flexible constraint-based chunking, clause-level overlap,\n    and custom token counters.\n\n    Args:\n        text (str): The input text to chunk.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk. Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-75). Defaults to 20\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable, optional): Optional token counting function.\n            Required for token-based modes only.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each containing the chunk content and metadata.\n\n    Raises:\n        InvalidInputError: If any chunking configuration parameter is invalid.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If an error occurs during sentence splitting or token counting within a chunking task.\n    \"\"\"\n    self._validate_constraints(\n        max_tokens, max_sentences, max_section_breaks, token_counter\n    )\n\n    log_info(\n        self.verbose,\n        \"Starting chunk processing for text starting with: {}.\",\n        f\"{text[:100]}...\",\n    )\n\n    # Adjust limits for _group_by_chunk's internal use\n    if max_tokens is None:\n        max_tokens = sys.maxsize\n    if max_sentences is None:\n        max_sentences = sys.maxsize\n    if max_section_breaks is None:\n        max_section_breaks = sys.maxsize\n\n    if not text.strip():\n        log_info(self.verbose, \"Input text is empty. Returning empty list.\")\n        return []\n\n    try:\n        sentences = self.sentence_splitter.split_text(\n            text,\n            lang,\n        )\n    except Exception as e:\n        raise CallbackError(\n            f\"An error occurred during the sentence splitting process.\\nDetails: {e}\\n\"\n            \"\ud83d\udca1 Hint: This may be due to an issue with the underlying sentence splitting library.\"\n        ) from e\n\n    if not sentences:\n        return []\n\n    offset = round(offset)\n    if offset &gt;= len(sentences):\n        logger.warning(\n            \"Offset {} &gt;= total sentences {}. Returning empty list.\",\n            offset,\n            len(sentences),\n        )\n        return []\n\n    chunks = self._group_by_chunk(\n        sentences[offset:],\n        token_counter=token_counter or self.token_counter,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n    )\n\n    # Leave the user's original dict untouched\n    base_metadata = (base_metadata or {}).copy()\n\n    return self._create_chunk_boxes(chunks, base_metadata, text)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to chunk.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-75). Defaults to 20</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required for token-based modes only.</p>"},{"location":"reference/chunklet/document_chunker/_plain_text_chunker/#chunklet.document_chunker._plain_text_chunker.PlainTextChunker.chunk(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/document_chunker/converters/","title":"converters","text":""},{"location":"reference/chunklet/document_chunker/converters/#chunklet.document_chunker.converters","title":"chunklet.document_chunker.converters","text":"<p>Modules:</p> <ul> <li> <code>html_2_md</code>           \u2013            </li> <li> <code>latex_2_md</code>           \u2013            </li> <li> <code>rst_2_md</code>           \u2013            </li> <li> <code>table_2_md</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/","title":"html_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md","title":"chunklet.document_chunker.converters.html_2_md","text":"<p>Functions:</p> <ul> <li> <code>html_to_md</code>             \u2013              <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md","title":"html_to_md","text":"<pre><code>html_to_md(\n    file_path: str | Path = None,\n    raw_text: str | None = None,\n    max_url_length: int = 150,\n) -&gt; str\n</code></pre> <p>Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/html_2_md.py</code> <pre><code>def html_to_md(\n    file_path: str | Path = None, raw_text: str | None = None, max_url_length: int = 150\n) -&gt; str:\n    \"\"\"\n    Convert HTML content to Markdown, remove hrefs from links, and truncate long URLs.\n\n    Args:\n        file_path (str | Path): Path to the html file.\n        raw_text (str, optional): Raw HTML text. If both file_path and raw_text is provided,\n            then raw_text will be used instead.\n        max_url_length (int): The maximum length of a URL. Defaults to 150.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if md is None:\n        raise ImportError(\n            \"The 'markdownify' library is not installed. \"\n            \"Please install it with 'pip install markdownify' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        )\n\n    if raw_text:\n        markdown_content = md(raw_text)\n    elif file_path:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            markdown_content = md(f.read())\n    else:\n        raise ValueError(\"Either file_path or raw_text must be provided.\")\n\n    # Normalize consecutive newlines that are more than 2\n    markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n    # Truncate long URLs in Markdown links or images\n    def truncate_url(match: re.Match) -&gt; str:\n        prefix, url = match.group(1), match.group(2)\n        if len(url) &gt; max_url_length:\n            url = url[: max_url_length - 3] + \"...\"\n        return f\"{prefix}({url})\"\n\n    return re.sub(r\"(!?\\[[^\\]]*\\])\\((.*?)\\)\", truncate_url, markdown_content)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>, default:                   <code>None</code> )           \u2013            <p>Path to the html file.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(raw_text)","title":"<code>raw_text</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Raw HTML text. If both file_path and raw_text is provided, then raw_text will be used instead.</p>"},{"location":"reference/chunklet/document_chunker/converters/html_2_md/#chunklet.document_chunker.converters.html_2_md.html_to_md(max_url_length)","title":"<code>max_url_length</code>","text":"(<code>int</code>, default:                   <code>150</code> )           \u2013            <p>The maximum length of a URL. Defaults to 150.</p>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/","title":"latex_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md","title":"chunklet.document_chunker.converters.latex_2_md","text":"<p>Functions:</p> <ul> <li> <code>latex_to_md</code>             \u2013              <p>Convert LaTeX code to Markdown-style plain text.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md","title":"latex_to_md","text":"<pre><code>latex_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Convert LaTeX code to Markdown-style plain text.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in markdown</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/latex_2_md.py</code> <pre><code>def latex_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Convert LaTeX code to Markdown-style plain text.\n\n    Args:\n        file_path (str | Path): Path to the latex file.\n\n    Returns:\n        str: The full text content in markdown\n    \"\"\"\n    if LatexNodes2Text is None:\n        raise ImportError(\n            \"The 'pylatexenc' library is not installed. \"\n            \"Please install it with 'pip install 'pylatexenc&gt;=2.10'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        )\n\n    with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n        latex_code = f.read()\n\n    # Convert to text\n    latex_node = LatexNodes2Text()\n    text = latex_node.latex_to_text(latex_code)\n\n    # Replace \u00a7 by #\n    markdown_content = re.sub(r\"\u00a7\\.?\", \"#\", text)\n\n    # Normalize consecutive newlines more than two\n    return re.sub(r\"\\n{2,}\", \"\\n\\n\", markdown_content.strip())\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/latex_2_md/#chunklet.document_chunker.converters.latex_2_md.latex_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the latex file.</p>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/","title":"rst_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md","title":"chunklet.document_chunker.converters.rst_2_md","text":"<p>Functions:</p> <ul> <li> <code>rst_to_md</code>             \u2013              <p>Converts reStructuredText (RST) content into Markdown.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md","title":"rst_to_md","text":"<pre><code>rst_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Converts reStructuredText (RST) content into Markdown.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The full text content in Markdown.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/rst_2_md.py</code> <pre><code>def rst_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Converts reStructuredText (RST) content into Markdown.\n\n    Args:\n        file_path (str | Path): Path to the rst file.\n\n    Returns:\n        str: The full text content in Markdown.\n    \"\"\"\n    if publish_string is None:\n        raise ImportError(\n            \"The 'docutils' library is not installed. \"\n            \"Please install it with 'pip install 'docutils&gt;=0.21.2'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        )\n\n    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        rst_content = f.read()\n\n    # Convert the rst content to HTML first\n    html_content = publish_string(source=rst_content, writer_name=\"html\").decode(\n        \"utf-8\"\n    )\n\n    # Now we can convert it to markdown\n    markdown_content = html_to_md(raw_text=html_content)\n    return markdown_content\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/rst_2_md/#chunklet.document_chunker.converters.rst_2_md.rst_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the rst file.</p>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/","title":"table_2_md","text":""},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md","title":"chunklet.document_chunker.converters.table_2_md","text":"<p>Functions:</p> <ul> <li> <code>table_to_md</code>             \u2013              <p>Convert a CSV or XLSX file into a Markdown-formatted table string.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md.table_to_md","title":"table_to_md","text":"<pre><code>table_to_md(file_path: str | Path) -&gt; str\n</code></pre> <p>Convert a CSV or XLSX file into a Markdown-formatted table string.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown table representation of the file contents.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/converters/table_2_md.py</code> <pre><code>def table_to_md(file_path: str | Path) -&gt; str:\n    \"\"\"\n    Convert a CSV or XLSX file into a Markdown-formatted table string.\n\n    Args:\n        file_path (str | Path): Path to the input file (.csv or .xlsx).\n\n    Returns:\n        str: Markdown table representation of the file contents.\n    \"\"\"\n    file_path = Path(file_path)\n    ext = file_path.suffix.lower()\n\n    # Read CSV\n    if ext == \".csv\":\n        with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n            data = list(csv.reader(f))\n\n    # Read Excel (.xlsx)\n    elif ext == \".xlsx\":\n        try:\n            from openpyxl import load_workbook\n        except ImportError as e:  # pragma: no cover\n            raise ImportError(\n                \"The 'openpyxl' library is not installed. \"\n                \"Please install it with 'pip install openpyxl&gt;=3.1.2' \"\n                \"or install the document processing extras with \"\n                \"'pip install chunklet-py[structured-document]'\"\n            ) from e\n        wb = load_workbook(file_path, read_only=True)\n        sheet = wb.active\n        data = list(sheet.iter_rows(values_only=True))\n        wb.close()\n\n    else:\n        raise ValueError(f\"Unsupported file type: {ext}\")\n\n    headers = data[0]\n    rows = data[1:]\n\n    if not tabulate:\n        raise ImportError(\n            \"The 'tabulate2' library is not installed. \"\n            \"Please install it with 'pip install tabulate2&gt;=1.10.0' \"\n            \"or install the document processing extras with \"\n            \"'pip install chunklet-py[structured-document]'\"\n        )\n\n    return tabulate(rows, headers=headers, tablefmt=\"pipe\")\n</code></pre>"},{"location":"reference/chunklet/document_chunker/converters/table_2_md/#chunklet.document_chunker.converters.table_2_md.table_to_md(file_path)","title":"<code>file_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the input file (.csv or .xlsx).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/","title":"document_chunker","text":""},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker","title":"chunklet.document_chunker.document_chunker","text":"<p>Classes:</p> <ul> <li> <code>DocumentChunker</code>           \u2013            <p>A comprehensive document chunker that handles various file formats.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker","title":"DocumentChunker","text":"<pre><code>DocumentChunker(\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseChunker</code></p> <p>A comprehensive document chunker that handles various file formats.</p> <p>This class provides a high-level interface to chunk text from different document types. It automatically detects the file format and uses the appropriate method to extract content before passing it to an underlying <code>PlainTextChunker</code> instance.</p> Key Features <ul> <li>Multi-Format Support: Chunks text from PDF, TXT, MD, and RST files.</li> <li>Metadata Enrichment: Automatically adds source file path and other</li> </ul> <p>document-level metadata (e.g., PDF page numbers) to each chunk.     - Bulk Processing: Efficiently chunks multiple documents in a single call.     - Pluggable Document processors: Integrate custom processors allowing definition of specific logic for extracting text from various file types.</p> <p>Initializes the DocumentChunker.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If any of the input arguments are invalid or if the provided <code>sentence_splitter</code> is not an instance of <code>BaseSplitter</code>.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>batch_chunk</code>             \u2013              <p>Batch chunk multiple document files.</p> </li> <li> <code>chunk</code>             \u2013              <p>Chunk a document file into semantic pieces.</p> </li> <li> <code>chunk_file</code>             \u2013              <p>Chunks a single document from a given path.</p> </li> <li> <code>chunk_files</code>             \u2013              <p>Chunks multiple documents from a list of file paths.</p> </li> <li> <code>chunk_text</code>             \u2013              <p>Chunks raw text content.</p> </li> <li> <code>chunk_texts</code>             \u2013              <p>Chunks multiple text contents.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>supported_extensions</code>           \u2013            <p>Get the supported extensions, including the custom ones.</p> </li> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Get the verbosity status.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>def __init__(\n    self,\n    sentence_splitter: Any | None = None,\n    verbose: bool = False,\n    continuation_marker: str = \"...\",\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"\n    Initializes the DocumentChunker.\n\n    Args:\n        sentence_splitter (BaseSplitter | None): An optional BaseSplitter instance.\n            If None, a default SentenceSplitter will be initialized.\n        verbose (bool): Enable verbose logging.\n        continuation_marker (str): The marker to prepend to unfitted clauses. Defaults to '...'.\n        token_counter (Callable[[str], int] | None): Function that counts tokens in text.\n            If None, must be provided when calling chunk() methods.\n\n    Raises:\n        InvalidInputError: If any of the input arguments are invalid or if the provided `sentence_splitter` is not an instance of `BaseSplitter`.\n    \"\"\"\n    self._verbose = verbose\n    self.token_counter = token_counter\n    self.continuation_marker = continuation_marker\n\n    # Explicit type validation for sentence_splitter\n    if sentence_splitter is not None and not isinstance(\n        sentence_splitter, BaseSplitter\n    ):\n        raise InvalidInputError(\n            f\"The provided sentence_splitter must be an instance of BaseSplitter, \"\n            f\"but got {type(sentence_splitter).__name__}.\"\n        )\n\n    self.plain_text_chunker = PlainTextChunker(\n        sentence_splitter=sentence_splitter,\n        verbose=self._verbose,\n        continuation_marker=self.continuation_marker,\n        token_counter=self.token_counter,\n    )\n\n    self.processors = {\n        \".pdf\": pdf_processor.PDFProcessor,\n        \".epub\": epub_processor.EPUBProcessor,\n        \".docx\": docx_processor.DOCXProcessor,\n        \".odt\": odt_processor.ODTProcessor,\n    }\n    self.converters = {\n        \".html\": html_2_md.html_to_md,\n        \".hml\": html_2_md.html_to_md,\n        \".rst\": rst_2_md.rst_to_md,\n        \".tex\": latex_2_md.latex_to_md,\n        \".csv\": table_2_md.table_to_md,\n        \".xlsx\": table_2_md.table_to_md,\n    }\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(sentence_splitter)","title":"<code>sentence_splitter</code>","text":"(<code>BaseSplitter | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional BaseSplitter instance. If None, a default SentenceSplitter will be initialized.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(continuation_marker)","title":"<code>continuation_marker</code>","text":"(<code>str</code>, default:                   <code>'...'</code> )           \u2013            <p>The marker to prepend to unfitted clauses. Defaults to '...'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker(token_counter)","title":"<code>token_counter</code>","text":"(<code>Callable[[str], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Function that counts tokens in text. If None, must be provided when calling chunk() methods.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.supported_extensions","title":"supported_extensions  <code>property</code>","text":"<pre><code>supported_extensions\n</code></pre> <p>Get the supported extensions, including the custom ones.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.verbose","title":"verbose  <code>property</code> <code>writable</code>","text":"<pre><code>verbose: bool\n</code></pre> <p>Get the verbosity status.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.batch_chunk","title":"batch_chunk","text":"<pre><code>batch_chunk(\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Batch chunk multiple document files.</p> Note <p>Deprecated since v2.2.0. Will be removed in v3.0.0. Use <code>chunk_files</code> instead.</p> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"chunk_files\", deprecated_in=\"2.2.0\", removed_in=\"3.0.0\"\n)\ndef batch_chunk(  # pragma: no cover\n    self,\n    paths: \"restricted_iterable(str | Path)\",  # noqa: F722\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Batch chunk multiple document files.\n\n    Note:\n        Deprecated since v2.2.0. Will be removed in v3.0.0. Use `chunk_files` instead.\n    \"\"\"\n    params = {k: v for k, v in locals().items() if k != \"self\"}\n    params[\"token_counter\"] = params[\"token_counter\"] or self.token_counter\n    yield from self.chunk_files(**params)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunk a document file into semantic pieces.</p> Note <p>Deprecated since v2.2.0. Will be removed in v3.0.0. Use <code>chunk_file</code> instead.</p> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"chunk_file\", deprecated_in=\"2.2.0\", removed_in=\"3.0.0\"\n)\ndef chunk(  # pragma: no cover\n    self,\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunk a document file into semantic pieces.\n\n    Note:\n        Deprecated since v2.2.0. Will be removed in v3.0.0. Use `chunk_file` instead.\n    \"\"\"\n    params = {k: v for k, v in locals().items() if k != \"self\"}\n    params[\"token_counter\"] = params[\"token_counter\"] or self.token_counter\n    return self.chunk_file(**params)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file","title":"chunk_file","text":"<pre><code>chunk_file(\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks a single document from a given path.</p> <p>This method automatically detects the file type and uses the appropriate processor to extract text before chunking. It then adds document-level metadata to each resulting chunk.</p> <p>Parameters:</p> <ul> <li>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>The path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and  tags. Must be &gt;= 1. <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each representing</p> </li> <li> <code>list[Box]</code>           \u2013            <p>a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk_file(\n    self,\n    path: str | Path,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks a single document from a given path.\n\n    This method automatically detects the file type and uses the appropriate\n    processor to extract text before chunking. It then adds document-level\n    metadata to each resulting chunk.\n\n    Args:\n        path (str | Path): The path to the document file.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk.\n            Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and &lt;details&gt; tags.\n            Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each representing\n        a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    path = Path(path)\n    ext = self._validate_and_get_extension(path)\n\n    text_content_or_generator, document_metadata = self._extract_data(path, ext)\n\n    if not isinstance(text_content_or_generator, str):\n        raise UnsupportedFileTypeError(\n            f\"File type '{ext}' is not supported by the general chunk method.\\n\"\n            \"Reason: The processor for this file returns iterable, \"\n            \"so it must be processed in parallel for efficiency.\\n\"\n            \"\ud83d\udca1 Hint: use `chunker.chunk_files([file.ext])` for this file type.\"\n        )\n\n    log_info(self.verbose, \"Starting chunk processing for path: {}.\", path)\n\n    text_content = text_content_or_generator\n\n    chunk_boxes = self.plain_text_chunker.chunk(\n        text=text_content,\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n    )\n\n    for chunk in chunk_boxes:\n        chunk.metadata.update(document_metadata)\n\n    log_info(self.verbose, \"Generated {} chunks for {}.\", len(chunk_boxes), path)\n\n    return chunk_boxes\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_file(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files","title":"chunk_files","text":"<pre><code>chunk_files(\n    paths: restricted_iterable(str | Path),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Chunks multiple documents from a list of file paths.</p> <p>This method is a memory-efficient generator that yields chunks as they are processed, without loading all documents into memory at once. It handles various file types.</p> <p>Parameters:</p> <ul> <li>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(paths)","title":"<code>paths</code>","text":"(<code>restricted_iterable[str | Path]</code>)           \u2013            <p>A restricted iterable of paths to the document files.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and  tags. Must be &gt;= 1. <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If provided file path not found.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk_files(\n    self,\n    paths: \"restricted_iterable(str | Path)\",  # noqa: F722\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Chunks multiple documents from a list of file paths.\n\n    This method is a memory-efficient generator that yields chunks as they\n    are processed, without loading all documents into memory at once. It\n    handles various file types.\n\n    Args:\n        paths (restricted_iterable[str | Path]): A restricted iterable of paths to the document files.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk.\n            Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and &lt;details&gt; tags.\n            Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n            Note: None cannot be used as a separator.\n\n        n_jobs (int | None): Number of parallel workers to use. If None, uses all available CPUs.\n               Must be &gt;= 1 if specified.\n        show_progress (bool): Flag to show or disable the loading bar.\n        on_errors: How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        FileNotFoundError: If provided file path not found.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    sentinel = object()\n\n    gathered_data = self._gather_all_data(paths, on_errors)\n\n    all_chunks_gen = self.plain_text_chunker.batch_chunk(\n        texts=list(gathered_data[\"all_texts_gen\"]),\n        lang=lang,\n        max_tokens=max_tokens,\n        max_sentences=max_sentences,\n        max_section_breaks=max_section_breaks,\n        overlap_percent=overlap_percent,\n        offset=offset,\n        token_counter=token_counter or self.token_counter,\n        separator=sentinel,\n        n_jobs=n_jobs,\n        show_progress=show_progress,\n        on_errors=on_errors,\n    )\n\n    all_chunk_groups = split_at(all_chunks_gen, lambda x: x is sentinel)\n    path_section_counts = gathered_data[\"path_section_counts\"]\n    all_metadata = gathered_data[\"all_metadata\"]\n\n    # HACK: Since a sentinel is always at the end of the gen,\n    # the last list of the groups will be an empty one.\n    # The only work-around to add a sentinel at paths\n    paths = list(path_section_counts.keys()) + [None]\n\n    # If no files were successfully processed, return empty\n    if not path_section_counts:\n        return\n\n    doc_count = 0\n    curr_path = paths[0]\n    for chunks in all_chunk_groups:\n        if path_section_counts.get(curr_path, 0) == 0:\n            if separator is not None:\n                yield separator\n\n            doc_count += 1\n            curr_path = paths[doc_count]\n            if curr_path is None:\n                return\n\n        for i, ch in enumerate(chunks, start=1):\n            doc_metadata = all_metadata[doc_count]\n            doc_metadata[\"section_count\"] = path_section_counts[curr_path]\n            doc_metadata[\"curr_section\"] = i\n\n            ch[\"metadata\"].update(doc_metadata)\n            yield ch\n\n        path_section_counts[curr_path] -= 1\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed. Note: None cannot be used as a separator.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers to use. If None, uses all available CPUs.    Must be &gt;= 1 if specified.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to show or disable the loading bar.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_files(on_errors)","title":"<code>on_errors</code>","text":"(<code>Literal['raise', 'skip', 'break']</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors during processing. Can be 'raise', 'ignore', or 'break'.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text","title":"chunk_text","text":"<pre><code>chunk_text(\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None\n) -&gt; list[Box]\n</code></pre> <p>Chunks raw text content.</p> <p>Parameters:</p> <ul> <li>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The raw text to chunk.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and  tags. Must be &gt;= 1. <p>Returns:</p> <ul> <li> <code>list[Box]</code>           \u2013            <p>list[Box]: A list of <code>Box</code> objects, each representing a chunk.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk_text(\n    self,\n    text: str,\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n) -&gt; list[Box]:\n    \"\"\"\n    Chunks raw text content.\n\n    Args:\n        text (str): The raw text to chunk.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk.\n            Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and &lt;details&gt; tags.\n            Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n\n    Returns:\n        list[Box]: A list of `Box` objects, each representing a chunk.\n    \"\"\"\n    params = {k: v for k, v in locals().items() if k != \"self\"}\n    params[\"token_counter\"] = params.get(\"token_counter\") or self.token_counter\n    return self.plain_text_chunker.chunk(**params)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_text(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts","title":"chunk_texts","text":"<pre><code>chunk_texts(\n    texts: restricted_iterable(str),\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    max_section_breaks: Annotated[\n        int | None, Field(ge=1)\n    ] = None,\n    overlap_percent: Annotated[\n        int, Field(ge=0, le=75)\n    ] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\"\n) -&gt; Generator[Box, None, None]\n</code></pre> <p>Chunks multiple text contents.</p> <p>Parameters:</p> <ul> <li>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(texts)","title":"<code>texts</code>","text":"(<code>restricted_iterable(str)</code>)           \u2013            <p>A restricted iterable of texts to chunk.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of tokens per chunk. Must be &gt;= 12.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(max_sentences)","title":"<code>max_sentences</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of sentences per chunk. Must be &gt;= 1.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(max_section_breaks)","title":"<code>max_section_breaks</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of section breaks per chunk. Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and  tags. Must be &gt;= 1. <p>Yields:</p> <ul> <li> <code>Box</code> (              <code>Box</code> )          \u2013            <p><code>Box</code> object, representing a chunk with its content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidInputError</code>             \u2013            <p>If the input arguments aren't valid.</p> </li> <li> <code>UnsupportedFileTypeError</code>             \u2013            <p>If the file extension is not supported or is missing.</p> </li> <li> <code>MissingTokenCounterError</code>             \u2013            <p>If <code>max_tokens</code> is provided but no <code>token_counter</code> is provided.</p> </li> <li> <code>CallbackError</code>             \u2013            <p>If a callback function (e.g., custom processors callbacks) fails during execution.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/document_chunker.py</code> <pre><code>@validate_input\ndef chunk_texts(\n    self,\n    texts: \"restricted_iterable(str)\",  # noqa: F722\n    *,\n    lang: str = \"auto\",\n    max_tokens: Annotated[int | None, Field(ge=12)] = None,\n    max_sentences: Annotated[int | None, Field(ge=1)] = None,\n    max_section_breaks: Annotated[int | None, Field(ge=1)] = None,\n    overlap_percent: Annotated[int, Field(ge=0, le=75)] = 20,\n    offset: Annotated[int, Field(ge=0)] = 0,\n    token_counter: Callable[[str], int] | None = None,\n    base_metadata: dict[str, Any] | None = None,\n    separator: Any = None,\n    n_jobs: Annotated[int, Field(ge=1)] | None = None,\n    show_progress: bool = True,\n    on_errors: Literal[\"raise\", \"skip\", \"break\"] = \"raise\",\n) -&gt; Generator[Box, None, None]:\n    \"\"\"\n    Chunks multiple text contents.\n\n    Args:\n        texts (restricted_iterable(str)): A restricted iterable of texts to chunk.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to \"auto\".\n        max_tokens (int, optional): Maximum number of tokens per chunk. Must be &gt;= 12.\n        max_sentences (int, optional): Maximum number of sentences per chunk. Must be &gt;= 1.\n        max_section_breaks (int, optional): Maximum number of section breaks per chunk.\n            Section breaks include Markdown headings (# to ######), horizontal rules (---, ***, ___), and &lt;details&gt; tags.\n            Must be &gt;= 1.\n        overlap_percent (int | float): Percentage of overlap between chunks (0-85).\n        offset (int): Starting sentence offset for chunking. Defaults to 0.\n        token_counter (callable | None): Optional token counting function.\n            Required if `max_tokens` is provided.\n        base_metadata (dict[str, Any], optional): Optional dictionary to be included with each chunk.\n        separator (Any): A value to be yielded after the chunks of each text are processed.\n        n_jobs (int | None): Number of parallel workers.\n        show_progress (bool): Show progress bar.\n        on_errors (str): How to handle errors.\n\n    yields:\n        Box: `Box` object, representing a chunk with its content and metadata.\n\n    Raises:\n        InvalidInputError: If the input arguments aren't valid.\n        UnsupportedFileTypeError: If the file extension is not supported or is missing.\n        MissingTokenCounterError: If `max_tokens` is provided but no `token_counter` is provided.\n        CallbackError: If a callback function (e.g., custom processors callbacks) fails during execution.\n    \"\"\"\n    params = {k: v for k, v in locals().items() if k != \"self\"}\n    params[\"token_counter\"] = params[\"token_counter\"] or self.token_counter\n    yield from self.plain_text_chunker.batch_chunk(**params)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(overlap_percent)","title":"<code>overlap_percent</code>","text":"(<code>int | float</code>, default:                   <code>20</code> )           \u2013            <p>Percentage of overlap between chunks (0-85).</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(offset)","title":"<code>offset</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Starting sentence offset for chunking. Defaults to 0.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(token_counter)","title":"<code>token_counter</code>","text":"(<code>callable | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional token counting function. Required if <code>max_tokens</code> is provided.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(base_metadata)","title":"<code>base_metadata</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary to be included with each chunk.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(separator)","title":"<code>separator</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A value to be yielded after the chunks of each text are processed.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(n_jobs)","title":"<code>n_jobs</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of parallel workers.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(show_progress)","title":"<code>show_progress</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Show progress bar.</p>"},{"location":"reference/chunklet/document_chunker/document_chunker/#chunklet.document_chunker.document_chunker.DocumentChunker.chunk_texts(on_errors)","title":"<code>on_errors</code>","text":"(<code>str</code>, default:                   <code>'raise'</code> )           \u2013            <p>How to handle errors.</p>"},{"location":"reference/chunklet/document_chunker/processors/","title":"processors","text":""},{"location":"reference/chunklet/document_chunker/processors/#chunklet.document_chunker.processors","title":"chunklet.document_chunker.processors","text":"<p>Modules:</p> <ul> <li> <code>base_processor</code>           \u2013            </li> <li> <code>docx_processor</code>           \u2013            </li> <li> <code>epub_processor</code>           \u2013            </li> <li> <code>odt_processor</code>           \u2013            </li> <li> <code>pdf_processor</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/","title":"base_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor","title":"chunklet.document_chunker.processors.base_processor","text":"<p>Classes:</p> <ul> <li> <code>BaseProcessor</code>           \u2013            <p>Abstract base class for document processors, providing a unified interface</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor","title":"BaseProcessor","text":"<pre><code>BaseProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for document processors, providing a unified interface for extracting text and metadata from documents.</p> <p>Initializes the processor with the path to the document.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the document.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields text content from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the document file.</p>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_metadata","title":"extract_metadata  <code>abstractmethod</code>","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the document.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: Dictionary containing document metadata.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts metadata from the document.\n\n    Returns:\n        dict[str, Any]: Dictionary containing document metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/base_processor/#chunklet.document_chunker.processors.base_processor.BaseProcessor.extract_text","title":"extract_text  <code>abstractmethod</code>","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields text content from the document.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Text content chunks from the document.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>@abstractmethod\ndef extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields text content from the document.\n\n    Yields:\n        str: Text content chunks from the document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/","title":"docx_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor","title":"chunklet.document_chunker.processors.docx_processor","text":"<p>Classes:</p> <ul> <li> <code>DOCXProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from DOCX files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor","title":"DOCXProcessor","text":"<pre><code>DOCXProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from DOCX files.</p> <p>Text content is extracted, images are replaced with a placeholder, and the resulting text is formatted using Markdown conversion.</p> <p>This class extracts metadata which typically uses a mix of Open Packaging Conventions (OPC) properties and elements that align with Dublin Core standards.</p> <p>For more details on the DOCX core properties processed, refer to the <code>python-docx</code> documentation: https://python-docx.readthedocs.io/en/latest/dev/analysis/features/coreprops.html</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/base_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the processor with the path to the document.\n\n    Args:\n        file_path (str): Path to the document file.\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - publisher - last_modified_by - created - modified - rights - version</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts core properties (a mix of OPC and Dublin Core elements) from the DOCX file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - publisher\n            - last_modified_by\n            - created\n            - modified\n            - rights\n            - version\n    \"\"\"\n    try:\n        from docx import Document\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            \"The 'python-docx' library is not installed. \"\n            \"Please install it with 'pip install 'python-docx&gt;=1.2.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        ) from e\n\n    doc = Document(self.file_path)\n    props = doc.core_properties\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        value = getattr(props, field, \"\")\n        if value:\n            metadata[field] = str(value)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/docx_processor/#chunklet.document_chunker.processors.docx_processor.DOCXProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.</p> <p>Images are replaced with a placeholder \"[Image - num]\". Text is yielded in chunks of approximately 4000 characters each to simulate pages and enhance parallel execution.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A chunk of text, approximately 4000 characters each.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/docx_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Extracts text content from DOCX file in Markdown format, yielding chunks for efficient processing.\n\n    Images are replaced with a placeholder \"[Image - num]\".\n    Text is yielded in chunks of approximately 4000 characters each to simulate pages and enhance parallel execution.\n\n    Yields:\n        str: A chunk of text, approximately 4000 characters each.\n    \"\"\"\n    try:  # Lazy import\n        import mammoth\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            \"The 'mammoth' library is not installed. \"\n            \"Please install it with 'pip install 'mammoth&gt;=1.9.0'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        ) from e\n\n    count = 0\n\n    def placeholder_images(image):\n        \"\"\"Replace all images with a placeholder text.\"\"\"\n        nonlocal count\n        count += 1\n        return [mammoth.html.text(f\"[Image - {count}]\")]\n\n    with open(self.file_path, \"rb\") as docx_file:\n        # Convert DOCX to HTML first\n        result = mammoth.convert_to_html(\n            docx_file, convert_image=placeholder_images\n        )\n        markdown_content = html_to_md(raw_text=result.value)\n\n    # Split into paragraphs and accumulate by character count (~4000 chars per chunk)\n    curr_chunk = []\n    curr_size = 0\n    max_size = 4000\n\n    for paragraph in markdown_content.split(\"\\n\\n\"):\n        para_len = len(paragraph)\n\n        # If adding this paragraph would exceed the limit, yield current chunk\n        if curr_size + para_len &gt; max_size and curr_chunk:\n            yield \"\\n\\n\".join(curr_chunk)\n            curr_chunk = []\n            curr_size = 0\n\n        curr_chunk.append(paragraph)\n        curr_size += para_len\n\n    # Yield any remaining content\n    if curr_chunk:\n        yield \"\\n\\n\".join(curr_chunk)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/","title":"epub_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor","title":"chunklet.document_chunker.processors.epub_processor","text":"<p>Classes:</p> <ul> <li> <code>EPUBProcessor</code>           \u2013            <p>Processor class for extracting text and metadata from EPUB files.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor","title":"EPUBProcessor","text":"<pre><code>EPUBProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>Processor class for extracting text and metadata from EPUB files.</p> <p>Text content is extracted by concatenating the text from all HTML content documents within the EPUB container.</p> <p>This processor focuses on extracting core metadata following the Dublin Core Metadata Initiative (DCMI) standard, which is the common practice in EPUB files. Not all available metadata fields are extracted.</p> <p>For more details on EPUB metadata and the Dublin Core standard, refer to the <code>ebooklib</code> tutorial:</p> <p>https://docs.sourcefabric.org/projects/ebooklib/en/latest/tutorial.html</p> <p>Initializes the EpubProcessor with a path to the EPUB file and reads the EPUB book into memory.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts Dublin Core metadata from the EPUB file.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yields Markdown-converted text from all document items in the EPUB file.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"\n    Initializes the EpubProcessor with a path to the EPUB file\n    and reads the EPUB book into memory.\n\n    Args:\n        file_path (str): Path to the EPUB file.\n    \"\"\"\n    if not epub:\n        raise ImportError(\n            \"The 'ebooklib' library is not installed. \"\n            \"Please install it with 'pip install 'ebooklib&gt;=0.19'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        )\n    self.file_path = file_path\n    self.book = epub.read_epub(file_path)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the EPUB file.</p>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts Dublin Core metadata from the EPUB file.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields. - title - creator - contributor - publisher - date - rights</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts Dublin Core metadata from the EPUB file.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields.\n            - title\n            - creator\n            - contributor\n            - publisher\n            - date\n            - rights\n    \"\"\"\n    metadata = {\"source\": str(self.file_path)}\n    for field in self.METADATA_FIELDS:\n        values = [v[0] for v in self.book.get_metadata(\"DC\", field)]\n        if values:\n            metadata[field] = \", \".join(values)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/epub_processor/#chunklet.document_chunker.processors.epub_processor.EPUBProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yields Markdown-converted text from all document items in the EPUB file.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Markdown-formatted text of each document item.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/epub_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Yields Markdown-converted text from all document items in the EPUB file.\n\n    Yields:\n        str: Markdown-formatted text of each document item.\n    \"\"\"\n    for idref, _ in self.book.spine:\n        item = self.book.get_item_with_id(idref)\n        html_content = item.get_body_content().decode(\"utf-8\", errors=\"ignore\")\n        md_content = html_to_md(raw_text=html_content)\n        yield md_content.strip()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/","title":"odt_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor","title":"chunklet.document_chunker.processors.odt_processor","text":"<p>Classes:</p> <ul> <li> <code>ODTProcessor</code>           \u2013            <p>ODT extraction and processing utility using <code>odfpy</code>.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor","title":"ODTProcessor","text":"<pre><code>ODTProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>ODT extraction and processing utility using <code>odfpy</code>.</p> <p>Provides methods to extract text and metadata from ODT (OpenDocument Text) files, while processing the extracted text into manageable chunks.</p> <p>This processor extracts metadata from the ODT document's Dublin Core and OpenDocument standard properties.</p> <p>For more details on ODF metadata fields and <code>odfpy</code> usage, refer to: https://odfpy.readthedocs.io/en/latest/</p> <p>Initialize the ODTProcessor.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Extracts text content from ODT paragraphs, yielding chunks for efficient processing.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize the ODTProcessor.\n\n    Args:\n        file_path (str): Path to the ODT file.\n    \"\"\"\n    try:\n        from odf.opendocument import load\n\n        self._load_odf = load\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            \"The 'odfpy' library is not installed. \"\n            \"Please install it with 'pip install odfpy&gt;=1.4.1' or install the document processing extras \"\n            \"with 'pip install chunklet-py[structured-document]'\"\n        ) from e\n\n    self.file_path = file_path\n    self.doc = self._load_odf(self.file_path)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the ODT file.</p>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.</p> <p>Parses the document's metadata elements, extracting fields such as:</p> <p>Only present fields are included in the returned dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields:  - title  - creator  - initial_creator  - created  - chapter  - author_name</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts metadata from the ODT file, focusing on Dublin Core and OpenDocument fields.\n\n    Parses the document's metadata elements, extracting fields such as:\n\n    Only present fields are included in the returned dictionary.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n             - title\n             - creator\n             - initial_creator\n             - created\n             - chapter\n             - author_name\n\n    \"\"\"\n    from odf import dc, meta, text\n\n    metadata = {}\n    for field in [\n        dc.Title,\n        dc.Creator,\n        meta.InitialCreator,\n        meta.CreationDate,\n        text.Chapter,\n        text.AuthorName,\n    ]:\n        elems = self.doc.getElementsByType(field)\n        value = \"\".join(\n            node.data\n            for e in elems\n            for node in e.childNodes\n            if node.nodeType == node.TEXT_NODE\n        ).strip()\n        if value:  # Only store if not empty\n            key = field.__name__\n            key = self.ODT_METADATA_KEY_MAP.get(key, key)\n\n            metadata[key.lower()] = value\n\n    metadata[\"source\"] = str(self.file_path)\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/odt_processor/#chunklet.document_chunker.processors.odt_processor.ODTProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Extracts text content from ODT paragraphs, yielding chunks for efficient processing.</p> <p>Iterates through paragraph elements in the document, extracting text content and buffering it into chunks of approximately 4000 characters. This allows for memory-efficient processing of large documents by yielding text blocks that simulate pages and enhance parallel execution.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A chunk of text, approximately 4000 characters each.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/odt_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Extracts text content from ODT paragraphs, yielding chunks for efficient processing.\n\n    Iterates through paragraph elements in the document, extracting text content\n    and buffering it into chunks of approximately 4000 characters. This allows for memory-efficient\n    processing of large documents by yielding text blocks that simulate pages and enhance parallel execution.\n\n    Yields:\n        str: A chunk of text, approximately 4000 characters each.\n    \"\"\"\n    from odf import text\n\n    curr_chunk = []\n    curr_size = 0\n    max_size = 4000\n\n    for p_elem in self.doc.getElementsByType(text.P):\n        para_text = \"\".join(\n            node.data\n            for node in p_elem.childNodes\n            if node.nodeType == node.TEXT_NODE\n        ).strip()\n        if not para_text:\n            continue\n\n        para_len = len(para_text)\n\n        # If adding this paragraph would exceed the limit, yield current chunk\n        if curr_size + para_len &gt; max_size and curr_chunk:\n            yield \"\\n\".join(curr_chunk)\n            curr_chunk = []\n            curr_size = 0\n\n        curr_chunk.append(para_text)\n        curr_size += para_len\n\n    # Yield any remaining content\n    if curr_chunk:\n        yield \"\\n\".join(curr_chunk)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/","title":"pdf_processor","text":""},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor","title":"chunklet.document_chunker.processors.pdf_processor","text":"<p>Classes:</p> <ul> <li> <code>PDFProcessor</code>           \u2013            <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor","title":"PDFProcessor","text":"<pre><code>PDFProcessor(file_path: str)\n</code></pre> <p>               Bases: <code>BaseProcessor</code></p> <p>PDF extraction and cleanup utility using <code>pdfminer.six</code>.</p> <p>Provides methods to extract text and metadata from PDF files, while cleaning and normalizing the extracted text using regex patterns.</p> <p>This processor extracts metadata from the PDF document's information dictionary, focusing on core metadata rather than all available fields.</p> <p>For more details on PDF metadata extraction using <code>pdfminer.six</code>, refer to this relevant Stack Overflow discussion:</p> <p>https://stackoverflow.com/questions/75591385/extract-metadata-info-from-online-pdf-using-pdfminer-in-python</p> <p>Initialize the PDFProcessor.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>extract_metadata</code>             \u2013              <p>Extracts metadata from the PDF document's information dictionary.</p> </li> <li> <code>extract_text</code>             \u2013              <p>Yield cleaned text from each PDF page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize the PDFProcessor.\n\n    Args:\n        file_path (str): Path to the PDF file.\n    \"\"\"\n    try:\n        from pdfminer.layout import LAParams\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            \"The 'pdfminer.six' library is not installed. \"\n            \"Please install it with 'pip install 'pdfminer.six&gt;=20250324'' or install the document processing extras \"\n            \"with 'pip install 'chunklet-py[structured-document]''\"\n        ) from e\n    self.file_path = file_path\n    self.laparams = LAParams(\n        line_margin=0.5,\n    )\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the PDF file.</p>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_metadata","title":"extract_metadata","text":"<pre><code>extract_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Extracts metadata from the PDF document's information dictionary.</p> <p>Includes source path, page count, and PDF info fields.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: A dictionary containing metadata fields: - title - author - creator - producer - publisher - created - modified</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_metadata(self) -&gt; dict[str, Any]:\n    \"\"\"Extracts metadata from the PDF document's information dictionary.\n\n    Includes source path, page count, and PDF info fields.\n\n    Returns:\n        dict[str, Any]: A dictionary containing metadata fields:\n            - title\n            - author\n            - creator\n            - producer\n            - publisher\n            - created\n            - modified\n    \"\"\"\n    from pdfminer.pdfdocument import PDFDocument\n    from pdfminer.pdfpage import PDFPage\n    from pdfminer.pdfparser import PDFParser\n\n    metadata = {\"source\": str(self.file_path), \"page_count\": 0}\n    with open(self.file_path, \"rb\") as f:\n        # Initialize parser on the file stream\n        parser = PDFParser(f)\n\n        # PDFDocument reads file structure, consuming the file pointer\n        doc = PDFDocument(parser)\n\n        # Reset pointer to start of file stream for accurate page counting\n        f.seek(0)\n\n        metadata[\"page_count\"] = ilen(PDFPage.get_pages(f))\n        metadata.update(self._extract_info_metadata(doc))\n\n    return metadata\n</code></pre>"},{"location":"reference/chunklet/document_chunker/processors/pdf_processor/#chunklet.document_chunker.processors.pdf_processor.PDFProcessor.extract_text","title":"extract_text","text":"<pre><code>extract_text() -&gt; Generator[str, None, None]\n</code></pre> <p>Yield cleaned text from each PDF page.</p> <p>Extracts text content page by page using pdfminer.high_level.extract_text for efficient processing. Each page is processed individually to avoid memory issues with large PDF files. The extracted text is cleaned using the _cleanup_text method to remove artifacts and normalize formatting.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Cleaned text content from each PDF page.</p> </li> </ul> Source code in <code>src/chunklet/document_chunker/processors/pdf_processor.py</code> <pre><code>def extract_text(self) -&gt; Generator[str, None, None]:\n    \"\"\"Yield cleaned text from each PDF page.\n\n    Extracts text content page by page using pdfminer.high_level.extract_text\n    for efficient processing. Each page is processed individually to avoid\n    memory issues with large PDF files. The extracted text is cleaned using\n    the _cleanup_text method to remove artifacts and normalize formatting.\n\n    Yields:\n        str: Cleaned text content from each PDF page.\n    \"\"\"\n    from pdfminer.high_level import extract_text\n    from pdfminer.pdfpage import PDFPage\n\n    with open(self.file_path, \"rb\") as fp:\n        page_count = ilen(PDFPage.get_pages(fp))\n\n        for page_num in range(page_count):\n            # Call extract_text on the file path, specifying the page number.\n            # This is efficient as it avoids repeated file seeks/parsing\n            # within the loop that was present in the old `extract_text_to_fp` approach.\n            raw_text = extract_text(\n                self.file_path,\n                page_numbers=[page_num],\n                laparams=self.laparams,\n            )\n            yield self._cleanup_text(raw_text)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/","title":"registry","text":""},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry","title":"chunklet.document_chunker.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomProcessorRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry","title":"CustomProcessorRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered processors from the registry.</p> </li> <li> <code>extract_data</code>             \u2013              <p>Processes a file using a processor registered for the given file extension.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a document processor is registered for the given file extension.</p> </li> <li> <code>register</code>             \u2013              <p>Register a document processor callback for one or more file extensions.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove document processor(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>processors</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered processors.</p> </li> </ul>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.processors","title":"processors  <code>property</code>","text":"<pre><code>processors\n</code></pre> <p>Returns a shallow copy of the dictionary of registered processors.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered processors from the registry.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered processors from the registry.\n    \"\"\"\n    self._processors.clear()\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data","title":"extract_data","text":"<pre><code>extract_data(\n    file_path: str, ext: str\n) -&gt; tuple[ReturnType, str]\n</code></pre> <p>Processes a file using a processor registered for the given file extension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[ReturnType, str]</code>           \u2013            <p>tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the processor callback fails or returns the wrong type.</p> </li> <li> <code>InvalidInputError</code>             \u2013            <p>If no processor is registered for the extension.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n&gt;&gt;&gt; registry = CustomProcessorRegistry()\n&gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n...     with open(file_path, 'r') as f:\n...         content = f.read()\n...     return content, {\"source\": file_path}\n&gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n&gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n&gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n</code></pre> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef extract_data(self, file_path: str, ext: str) -&gt; tuple[ReturnType, str]:\n    \"\"\"\n    Processes a file using a processor registered for the given file extension.\n\n    Args:\n        file_path (str): The path to the file.\n        ext (str): The file extension.\n\n    Returns:\n        tuple[ReturnType, str]: A tuple containing the extracted data and the name of the processor used.\n\n    Raises:\n        CallbackError: If the processor callback fails or returns the wrong type.\n        InvalidInputError: If no processor is registered for the extension.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.document_chunker.registry import CustomProcessorRegistry\n        &gt;&gt;&gt; registry = CustomProcessorRegistry()\n        &gt;&gt;&gt; @registry.register(\".txt\", name=\"my_txt_processor\")\n        ... def process_txt(file_path: str) -&gt; tuple[str, dict]:\n        ...     with open(file_path, 'r') as f:\n        ...         content = f.read()\n        ...     return content, {\"source\": file_path}\n        &gt;&gt;&gt; # Assuming 'sample.txt' exists with some content\n        &gt;&gt;&gt; # result, processor_name = registry.extract_data(\"sample.txt\", \".txt\")\n        &gt;&gt;&gt; # print(f\"Extracted by {processor_name}: {result[0][:20]}...\")\n    \"\"\"\n    processor_info = self._processors.get(ext)\n    if not processor_info:\n        raise InvalidInputError(\n            f\"No document processor registered for file extension '{ext}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `register('{ext}', callback=your_function)` first.\"\n        )\n\n    name, callback = processor_info\n\n    try:\n        # Validate the return type\n        result = callback(file_path)\n        validator = TypeAdapter(ReturnType)\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = (\n            \"\ud83d\udca1Hint: Make sure your processor returns a tuple of (text/texts, metadata_dict).\"\n            \" An empty dict can be provided if there's no metadata.\"\n        )\n\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Processor '{name}' for extension '{ext}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(file_path)","title":"<code>file_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to the file.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.extract_data(ext)","title":"<code>ext</code>","text":"(<code>str</code>)           \u2013            <p>The file extension.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(ext: str) -&gt; bool\n</code></pre> <p>Check if a document processor is registered for the given file extension.</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, ext: str) -&gt; bool:\n    \"\"\"\n    Check if a document processor is registered for the given file extension.\n    \"\"\"\n    return ext in self._processors\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a document processor callback for one or more file extensions.</p> <p>This method can be used in two ways:</p> <ol> <li> <p>As a decorator:     @registry.register(\".json\", \".xml\", name=\"my_processor\")     def my_processor(file_path):         ...</p> </li> <li> <p>As a direct function call:     registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")</p> </li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a document processor callback for one or more file extensions.\n\n    This method can be used in two ways:\n\n    1. As a decorator:\n        @registry.register(\".json\", \".xml\", name=\"my_processor\")\n        def my_processor(file_path):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_processor, \".json\", \".xml\", name=\"my_processor\")\n\n    Args:\n        *args: The arguments, which can be either (ext1, ext2, ...) for a decorator\n               or (callback, ext1, ext2, ...) for a direct call.\n        name (str | None): The name of the processor. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\n            \"At least one file extension or a callback must be provided.\"\n        )\n\n    if callable(args[0]):\n        # Direct call: register(callback, ext1, ext2, ...)\n        callback = args[0]\n        exts = args[1:]\n        if not exts:\n            raise ValueError(\n                \"At least one file extension must be provided for the callback.\"\n            )\n        self._register_logic(exts, callback, name)\n        return callback\n    else:\n        # Decorator: @register(ext1, ext2, ...)\n        exts = args\n\n        def decorator(cb: Callable):\n            self._register_logic(exts, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (ext1, ext2, ...) for a decorator    or (callback, ext1, ext2, ...) for a direct call.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.register(name)","title":"<code>name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the processor. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*exts: str) -&gt; None\n</code></pre> <p>Remove document processor(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/document_chunker/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *exts: str) -&gt; None:\n    \"\"\"\n    Remove document processor(s) from the registry.\n\n    Args:\n        *exts: File extensions to remove.\n    \"\"\"\n    for ext in exts:\n        self._processors.pop(ext, None)\n</code></pre>"},{"location":"reference/chunklet/document_chunker/registry/#chunklet.document_chunker.registry.CustomProcessorRegistry.unregister(*exts)","title":"<code>*exts</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>File extensions to remove.</p>"},{"location":"reference/chunklet/exceptions/","title":"exceptions","text":""},{"location":"reference/chunklet/exceptions/#chunklet.exceptions","title":"chunklet.exceptions","text":"<p>Classes:</p> <ul> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>ChunkletError</code>           \u2013            <p>Base exception for chunking and splitting</p> </li> <li> <code>FileProcessingError</code>           \u2013            <p>Raised when a file cannot be loaded, opened, or</p> </li> <li> <code>InvalidInputError</code>           \u2013            <p>Raised when one or multiple invalid input(s) are</p> </li> <li> <code>MissingTokenCounterError</code>           \u2013            <p>Raised when a token_counter is required but not</p> </li> <li> <code>TokenLimitError</code>           \u2013            <p>Raised when max_tokens constraint is exceeded.</p> </li> <li> <code>UnsupportedFileTypeError</code>           \u2013            <p>Raised when a file type is not supported for a given operation.</p> </li> </ul>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.ChunkletError","title":"ChunkletError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for chunking and splitting operations.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.FileProcessingError","title":"FileProcessingError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a file cannot be loaded, opened, or accessed.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.InvalidInputError","title":"InvalidInputError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when one or multiple invalid input(s) are encountered.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.MissingTokenCounterError","title":"MissingTokenCounterError","text":"<pre><code>MissingTokenCounterError(msg: str = '')\n</code></pre> <p>               Bases: <code>InvalidInputError</code></p> <p>Raised when a token_counter is required but not provided.</p> Source code in <code>src/chunklet/exceptions.py</code> <pre><code>def __init__(self, msg: str = \"\"):\n    self.msg = msg or (\n        \"A token_counter is required for token-based chunking.\\n\"\n        \"\ud83d\udca1 Hint: Pass a token counting function to the chunking method, like `chunker.chunk_text(..., token_counter=tk)`\\n\"\n        \"or configure it in the class initialization: `.*Chunker(token_counter=tk)`\"\n    )\n    super().__init__(self.msg)\n</code></pre>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.TokenLimitError","title":"TokenLimitError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when max_tokens constraint is exceeded.</p>"},{"location":"reference/chunklet/exceptions/#chunklet.exceptions.UnsupportedFileTypeError","title":"UnsupportedFileTypeError","text":"<p>               Bases: <code>FileProcessingError</code></p> <p>Raised when a file type is not supported for a given operation.</p> <p>This can happen if: - The file extension is not in the supported list - The file has no extension - The processor returns an iterable (requires batch processing)</p>"},{"location":"reference/chunklet/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter","title":"chunklet.sentence_splitter","text":"<p>Modules:</p> <ul> <li> <code>languages</code>           \u2013            <p>This module contains the language sets for the supported sentence splitters.</p> </li> <li> <code>registry</code>           \u2013            </li> <li> <code>sentence_splitter</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Base class for sentence splitting.</p> </li> <li> <code>CallbackError</code>           \u2013            <p>Raised when a callback function provided to chunker</p> </li> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>deprecated_callable</code>             \u2013              <p>Decorate a function or class with warning message.</p> </li> <li> <code>log_info</code>             \u2013              <p>Log an info message if verbose is enabled.</p> </li> <li> <code>pretty_errors</code>             \u2013              <p>Formats Pydantic validation errors into a human-readable string.</p> </li> <li> <code>read_text_file</code>             \u2013              <p>Read text file with automatic encoding detection.</p> </li> <li> <code>validate_input</code>             \u2013              <p>A decorator that validates function inputs and outputs</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>Base class for sentence splitting. Defines the interface that all splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into sentences.</p> </li> <li> <code>split_text</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Split text into sentences.</p> Note <p>Deprecated since 2.2.0. Will be removed in 3.0.0. Use <code>split_text</code> instead.</p> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"split_text\", deprecated_in=\"2.2.0\", removed_in=\"3.0.0\"\n)\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:  # pragma: no cover\n    \"\"\"\n    Split text into sentences.\n\n    Note:\n        Deprecated since 2.2.0. Will be removed in 3.0.0. Use `split_text` instead.\n    \"\"\"\n    return self.split_text(text, lang)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split_text","title":"split_text","text":"<pre><code>split_text(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>def split_text(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"Splits the given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement 'split_text'.\")\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split_text(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.BaseSplitter.split_text(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto').</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CallbackError","title":"CallbackError","text":"<p>               Bases: <code>ChunkletError</code></p> <p>Raised when a callback function provided to chunker or splitter fails during execution.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways:</p> <ol> <li> <p>As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> </li> <li> <p>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</p> </li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        list[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split_file</code>             \u2013              <p>Read and split a file into sentences.</p> </li> <li> <code>split_text</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for language detection\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    log_info(\n        self.verbose,\n        \"Language detection: '{}' with confidence {}.\",\n        lang_detected,\n        f\"{round(confidence) * 10}/10\",\n    )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_file","title":"split_file","text":"<pre><code>split_file(\n    path: str | Path, lang: str = \"auto\"\n) -&gt; list[str]\n</code></pre> <p>Read and split a file into sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the file.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>def split_file(self, path: str | Path, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Read and split a file into sentences.\n\n    Args:\n        path: Path to the file to read.\n        lang: The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to 'auto'.\n\n    Returns:\n        list[str]: A list of sentences extracted from the file.\n    \"\"\"\n    content = read_text_file(path)\n    return self.split_text(content, lang)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the file to read.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_file(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to 'auto'.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_text","title":"split_text","text":"<pre><code>split_text(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split_text(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split_text(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split_text(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        log_info(self.verbose, \"Input text is empty. Returning empty list.\")\n        return []\n\n    if lang == \"auto\":\n        logger.warning(\n            \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n        )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = custom_splitter_registry.split(text, lang)\n        log_info(self.verbose, \"Using registered splitter: {}\", splitter_name)\n    else:\n        sentences = None\n        for lang_set, handler in self.LANGUAGE_HANDLERS.items():\n            if lang in lang_set:\n                sentences = handler(lang, text)\n                break\n\n        # If no handler found, use fallback\n        if sentences is None:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n            sentences = self.fallback_splitter.split(text)\n\n    processed_sentences = self._filter_sentences(sentences)\n\n    log_info(\n        self.verbose,\n        \"Text splitted into sentences. Total sentences detected: {}\",\n        len(processed_sentences),\n    )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_text(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.SentenceSplitter.split_text(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.deprecated_callable","title":"deprecated_callable","text":"<pre><code>deprecated_callable(\n    use_instead: str, deprecated_in: str, removed_in: str\n) -&gt; Callable\n</code></pre> <p>Decorate a function or class with warning message.</p> <p>This decorator marks a function or class as deprecated.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Callable</code> (              <code>Callable</code> )          \u2013            <p>Decorator function that wraps the source function/class.</p> </li> </ul> Source code in <code>src/chunklet/common/deprecation.py</code> <pre><code>def deprecated_callable(\n    use_instead: str,\n    deprecated_in: str,\n    removed_in: str,\n) -&gt; Callable:\n    \"\"\"Decorate a function or class with warning message.\n\n    This decorator marks a function or class as deprecated.\n\n    Args:\n        use_instead (str): Replacement name (e.g., \"split_text\", \"DocumentChunker\", or \"chunk_text or chunk_file\").\n        deprecated_in (str): Version when the function was deprecated (e.g., \"2.2.0\").\n        removed_in (str): Version when the function will be removed (e.g., \"3.0.0\").\n\n    Returns:\n        Callable: Decorator function that wraps the source function/class.\n    \"\"\"\n\n    def decorator(func_or_cls: Callable) -&gt; Callable:\n        warn_message = (\n            f\"`{func_or_cls.__qualname__}` was deprecated since v{deprecated_in} \"\n            f\"in favor of `{use_instead}`. It will be removed in v{removed_in}.\"\n        )\n        remove_message = (\n            f\"`{func_or_cls.__qualname__}` was removed in v{removed_in}. \"\n            f\"Use `{use_instead}` instead.\"\n        )\n\n        @functools.wraps(func_or_cls)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            if Version(CURRENT_VERSION) &gt;= Version(removed_in):\n                raise AttributeError(remove_message)\n            warnings.warn(warn_message, FutureWarning, stacklevel=2)\n            return func_or_cls(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.deprecated_callable(use_instead)","title":"<code>use_instead</code>","text":"(<code>str</code>)           \u2013            <p>Replacement name (e.g., \"split_text\", \"DocumentChunker\", or \"chunk_text or chunk_file\").</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.deprecated_callable(deprecated_in)","title":"<code>deprecated_in</code>","text":"(<code>str</code>)           \u2013            <p>Version when the function was deprecated (e.g., \"2.2.0\").</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.deprecated_callable(removed_in)","title":"<code>removed_in</code>","text":"(<code>str</code>)           \u2013            <p>Version when the function will be removed (e.g., \"3.0.0\").</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.log_info","title":"log_info","text":"<pre><code>log_info(verbose: bool, *args, **kwargs) -&gt; None\n</code></pre> <p>Log an info message if verbose is enabled.</p> <p>This is a convenience function that only logs when verbose mode is enabled, avoiding unnecessary log output in production.</p> <p>Parameters:</p> Example <p>log_info(True, \"Processing file: {}\", filepath) Processing file: /path/to/file log_info(False, \"This will not be logged\") (no output)</p> Source code in <code>src/chunklet/common/logging_utils.py</code> <pre><code>def log_info(verbose: bool, *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message if verbose is enabled.\n\n    This is a convenience function that only logs when verbose mode is enabled,\n    avoiding unnecessary log output in production.\n\n    Args:\n        verbose: If True, logs the message; if False, does nothing.\n        *args: Positional arguments passed to logger.info().\n        **kwargs: Keyword arguments passed to logger.info().\n\n    Example:\n        &gt;&gt;&gt; log_info(True, \"Processing file: {}\", filepath)\n        Processing file: /path/to/file\n        &gt;&gt;&gt; log_info(False, \"This will not be logged\")\n        (no output)\n    \"\"\"\n    if verbose:\n        logger.info(*args, **kwargs)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.log_info(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>)           \u2013            <p>If True, logs the message; if False, does nothing.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.log_info(*args)","title":"<code>*args</code>","text":"\u2013            <p>Positional arguments passed to logger.info().</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.log_info(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Keyword arguments passed to logger.info().</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.pretty_errors","title":"pretty_errors","text":"<pre><code>pretty_errors(error: ValidationError) -&gt; str\n</code></pre> <p>Formats Pydantic validation errors into a human-readable string.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def pretty_errors(error: ValidationError) -&gt; str:\n    \"\"\"Formats Pydantic validation errors into a human-readable string.\"\"\"\n    lines = [\n        f\"{error.error_count()} validation error for {getattr(error, 'subtitle', '') or error.title}.\"\n    ]\n    for ind, err in enumerate(error.errors(), start=1):\n        msg = err[\"msg\"]\n\n        loc = err.get(\"loc\", [])\n        formatted_loc = \"\"\n        if len(loc) &gt;= 1:\n            formatted_loc = str(loc[0]) + \"\".join(f\"[{step!r}]\" for step in loc[1:])\n            formatted_loc = f\"({formatted_loc})\" if formatted_loc else \"\"\n\n        input_value = err[\"input\"]\n        input_type = type(input_value).__name__\n\n        # Sliced to avoid overflowing screen\n        input_value = (\n            input_value\n            if len(str(input_value)) &lt; 500\n            else str(input_value)[:500] + \"...\"\n        )\n\n        lines.append(\n            (\n                f\"{ind}) {formatted_loc} {msg}.\\n\"\n                f\"  Found: (input={input_value!r}, type={input_type})\"\n            )\n        )\n\n    lines.append(\"  \" + getattr(error, \"hint\", \"\"))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.read_text_file","title":"read_text_file","text":"<pre><code>read_text_file(path: str | Path) -&gt; str\n</code></pre> <p>Read text file with automatic encoding detection.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>File content.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileProcessingError</code>             \u2013            <p>If file cannot be read.</p> </li> </ul> Source code in <code>src/chunklet/common/path_utils.py</code> <pre><code>@validate_input\ndef read_text_file(path: str | Path) -&gt; str:\n    \"\"\"Read text file with automatic encoding detection.\n\n    Args:\n        path: File path to read.\n\n    Returns:\n        str: File content.\n\n    Raises:\n        FileProcessingError: If file cannot be read.\n    \"\"\"\n    from charset_normalizer import from_path\n\n    path = Path(path)\n\n    if not path.exists():\n        raise FileProcessingError(f\"File does not exist: {path}\")\n\n    if _is_binary_file(path):\n        raise FileProcessingError(f\"Binary file not supported: {path}\")\n\n    match = from_path(str(path)).best()\n    return str(match) if match else \"\"\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.read_text_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>File path to read.</p>"},{"location":"reference/chunklet/sentence_splitter/#chunklet.sentence_splitter.validate_input","title":"validate_input","text":"<pre><code>validate_input(fn)\n</code></pre> <p>A decorator that validates function inputs and outputs</p> <p>A wrapper around Pydantic's <code>validate_call</code> that catches<code>ValidationError</code> and re-raises it as a more user-friendly <code>InvalidInputError</code>.</p> Source code in <code>src/chunklet/common/validation.py</code> <pre><code>def validate_input(fn):\n    \"\"\"\n    A decorator that validates function inputs and outputs\n\n    A wrapper around Pydantic's `validate_call` that catches`ValidationError` and re-raises it as a more user-friendly `InvalidInputError`.\n    \"\"\"\n    validated_fn = validate_call(fn, config=ConfigDict(arbitrary_types_allowed=True))\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            return validated_fn(*args, **kwargs)\n        except ValidationError as e:\n            raise InvalidInputError(pretty_errors(e)) from None\n\n    return wrapper\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/","title":"_fallback_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter","title":"chunklet.sentence_splitter._fallback_splitter","text":"<p>Classes:</p> <ul> <li> <code>FallbackSplitter</code>           \u2013            <p>Rule-based, language-agnostic sentence boundary detector.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter","title":"FallbackSplitter","text":"<pre><code>FallbackSplitter()\n</code></pre> <p>Rule-based, language-agnostic sentence boundary detector.</p> <p>A rule-based, sentence boundary detection tool that doesn't rely on hardcoded lists of abbreviations or sentence terminators, making it adaptable to various text formats and domains.</p> <p>FallbackSplitter uses regex patterns to split text into sentences, handling:   - Common sentence-ending punctuation (., !, ?)   - Abbreviations and acronyms (e.g., Dr., Ph.D., U.S.)   - Numbered lists and headings   - Multi-punctuation sequences (e.g., ! ! !, ?!)   - Line breaks and whitespace normalization   - Decimal numbers and inline numbers</p> <p>Sentences are conservatively segmented, prioritizing context over aggressive splitting, which reduces false splits inside abbreviations, multi-punctuation sequences, or numeric constructs.</p> <p>Initializes regex patterns for sentence splitting.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Splits text into sentences using rule-based regex patterns.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes regex patterns for sentence splitting.\"\"\"\n    self.sentence_terminators = \"\".join(GLOBAL_SENTENCE_TERMINATORS)\n\n    # Patterns for handling numbered lists\n    self.flattened_numbered_list_pattern = re.compile(\n        rf\"(?&lt;=[{self.sentence_terminators}:])\\s+(\\p{{N}}\\.)+\"\n    )\n\n    self.numbered_list_pattern = re.compile(r\"([\\n:]\\s*)(\\p{N})\\.\")\n    self.norm_numbered_list_pattern = re.compile(r\"(\\s*)(\\p{N})&lt;DOT&gt;\")\n\n    # Core sentence split regex\n    self.sentence_end_pattern = re.compile(\n        rf\"\"\"\n        (?&lt;!\\b(\\p{{Lu}}\\p{{Ll}}{{1, 5}}\\.)*)   # negative lookbehind for abbreviations\n        (?&lt;=[{self.sentence_terminators}]        # sentence-ending punctuation\n        [\\\"'\u300b\u300d\\p{{pf}}\\p{{pe}}]*)                  # optional quotes or closing chars\n        (?=\\s+\\p{{Lu}}|\\s*\\n|\\s*$)               # followed by uppercase or end of text\n        \"\"\",\n        re.VERBOSE | re.UNICODE,\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Splits text into sentences using rule-based regex patterns.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences after segmentation.</p> </li> </ul> Notes <ul> <li>Normalizes numbered lists during splitting and restores them afterward.</li> <li>Handles punctuation, newlines, and common edge cases.</li> </ul> Source code in <code>src/chunklet/sentence_splitter/_fallback_splitter.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"\n    Splits text into sentences using rule-based regex patterns.\n\n    Args:\n        text (str): The input text to be segmented into sentences.\n\n    Returns:\n        list[str]: A list of sentences after segmentation.\n\n    Notes:\n        - Normalizes numbered lists during splitting and restores them afterward.\n        - Handles punctuation, newlines, and common edge cases.\n    \"\"\"\n    # Stage 1: handle flattened numbered lists\n    text = self.flattened_numbered_list_pattern.sub(r\"\\n \\1\", text.strip())\n\n    # Stage 2: normalize numbered lists\n    text = self.numbered_list_pattern.sub(r\"\\1\\2&lt;DOT&gt;\", text.strip())\n\n    # Stage 3: first pass - punctuation-based split\n    sentences = self.sentence_end_pattern.split(text.strip())\n\n    # Stage 4: remove empty strings and strip whitespace\n    fixed_sentences = [s.strip() for s in sentences if s and s.strip()]\n\n    # Stage 5: second pass - split further on newline (if not at start)\n    final_sentences = []\n    for sent in fixed_sentences:\n        final_sentences.extend(sent.splitlines())\n\n    # Stage 6: remove _ in numbered list numbers\n    return [\n        self.norm_numbered_list_pattern.sub(r\"\\1\\2.\", sent).rstrip()\n        for sent in final_sentences\n        if sent.strip()\n    ]\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/_fallback_splitter/#chunklet.sentence_splitter._fallback_splitter.FallbackSplitter.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be segmented into sentences.</p>"},{"location":"reference/chunklet/sentence_splitter/languages/","title":"languages","text":""},{"location":"reference/chunklet/sentence_splitter/languages/#chunklet.sentence_splitter.languages","title":"chunklet.sentence_splitter.languages","text":"<p>This module contains the language sets for the supported sentence splitters. Each set is filtered to contain only the languages truly unique to that library.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/","title":"registry","text":""},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry","title":"chunklet.sentence_splitter.registry","text":"<p>Classes:</p> <ul> <li> <code>CustomSplitterRegistry</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry","title":"CustomSplitterRegistry","text":"<p>Methods:</p> <ul> <li> <code>clear</code>             \u2013              <p>Clears all registered splitters from the registry.</p> </li> <li> <code>is_registered</code>             \u2013              <p>Check if a splitter is registered for the given language.</p> </li> <li> <code>register</code>             \u2013              <p>Register a splitter callback for one or more languages.</p> </li> <li> <code>split</code>             \u2013              <p>Processes a text using a splitter registered for the given language.</p> </li> <li> <code>unregister</code>             \u2013              <p>Remove splitter(s) from the registry.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>splitters</code>           \u2013            <p>Returns a shallow copy of the dictionary of registered splitters.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.splitters","title":"splitters  <code>property</code>","text":"<pre><code>splitters\n</code></pre> <p>Returns a shallow copy of the dictionary of registered splitters.</p> <p>This prevents external modification of the internal registry state.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clears all registered splitters from the registry.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears all registered splitters from the registry.\n    \"\"\"\n    self._splitters.clear()\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.is_registered","title":"is_registered","text":"<pre><code>is_registered(lang: str) -&gt; bool\n</code></pre> <p>Check if a splitter is registered for the given language.</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef is_registered(self, lang: str) -&gt; bool:\n    \"\"\"\n    Check if a splitter is registered for the given language.\n    \"\"\"\n    return lang in self._splitters\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register","title":"register","text":"<pre><code>register(*args: Any, name: str | None = None)\n</code></pre> <p>Register a splitter callback for one or more languages.</p> <p>This method can be used in two ways:</p> <ol> <li> <p>As a decorator:     @registry.register(\"en\", \"fr\", name=\"my_splitter\")     def my_splitter(text):         ...</p> </li> <li> <p>As a direct function call:     registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")</p> </li> </ol> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>def register(self, *args: Any, name: str | None = None):\n    \"\"\"\n    Register a splitter callback for one or more languages.\n\n    This method can be used in two ways:\n\n    1. As a decorator:\n        @registry.register(\"en\", \"fr\", name=\"my_splitter\")\n        def my_splitter(text):\n            ...\n\n    2. As a direct function call:\n        registry.register(my_splitter, \"en\", \"fr\", name=\"my_splitter\")\n\n    Args:\n        *args: The arguments, which can be either (lang1, lang2, ...) for a decorator\n               or (callback, lang1, lang2, ...) for a direct call.\n        name (str, optional): The name of the splitter. If None, attempts to use the callback's name.\n    \"\"\"\n    if not args:\n        raise ValueError(\"At least one language or a callback must be provided.\")\n\n    if callable(args[0]):\n        # Direct call: register(callback, lang1, lang2, ...)\n        callback = args[0]\n        langs = args[1:]\n        if not langs:\n            raise ValueError(\n                \"At least one language must be provided for the callback.\"\n            )\n        self._register_logic(langs, callback, name)\n        return callback\n    else:\n        # Decorator: @register(lang1, lang2, ...)\n        langs = args\n\n        def decorator(cb: Callable):\n            self._register_logic(langs, cb, name)\n            return cb\n\n        return decorator\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(*args)","title":"<code>*args</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments, which can be either (lang1, lang2, ...) for a decorator    or (callback, lang1, lang2, ...) for a direct call.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.register(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the splitter. If None, attempts to use the callback's name.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split","title":"split","text":"<pre><code>split(text: str, lang: str) -&gt; tuple[list[str], str]\n</code></pre> <p>Processes a text using a splitter registered for the given language.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], str]</code>           \u2013            <p>tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CallbackError</code>             \u2013            <p>If the splitter callback fails.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the splitter returns the wrong type.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n&gt;&gt;&gt; registry = CustomSplitterRegistry()\n&gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n... def custom_splitter(text: str) -&gt; list[str]:\n...     return text.split(\" \")\n&gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n(['Hello', 'World'], 'custom_splitter')\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef split(self, text: str, lang: str) -&gt; tuple[list[str], str]:\n    \"\"\"\n    Processes a text using a splitter registered for the given language.\n\n    Args:\n        text (str): The text to split.\n        lang (str): The language of the text.\n\n    Returns:\n        tuple[list[str], str]: A tuple containing a list of sentences and the name of the splitter used.\n\n    Raises:\n        CallbackError: If the splitter callback fails.\n        TypeError: If the splitter returns the wrong type.\n\n    Examples:\n        &gt;&gt;&gt; from chunklet.sentence_splitter import CustomSplitterRegistry\n        &gt;&gt;&gt; registry = CustomSplitterRegistry()\n        &gt;&gt;&gt; @registry.register(\"xx\", name=\"custom_splitter\")\n        ... def custom_splitter(text: str) -&gt; list[str]:\n        ...     return text.split(\" \")\n        &gt;&gt;&gt; registry.split(\"Hello World\", \"xx\")\n        (['Hello', 'World'], 'custom_splitter')\n    \"\"\"\n    splitter_info = self._splitters.get(lang)\n    if not splitter_info:\n        raise CallbackError(\n            f\"No splitter registered for language '{lang}'.\\n\"\n            f\"\ud83d\udca1Hint: Use `.register('{lang}', fn=your_function)` first.\"\n        )\n\n    name, callback = splitter_info\n\n    try:\n        # Validate the return type\n        result = callback(text)\n        validator = TypeAdapter(list[str])\n        validator.validate_python(result)\n    except ValidationError as e:\n        e.subtitle = f\"{name} result\"\n        e.hint = \"\ud83d\udca1Hint: Make sure your splitter returns a list of strings.\"\n        raise CallbackError(f\"{pretty_errors(e)}.\\n\") from None\n    except Exception as e:\n        raise CallbackError(\n            f\"Splitter '{name}' for lang '{lang}' raised an exception.\\nDetails: {e}\"\n        ) from None\n\n    return result, name\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The text to split.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.split(lang)","title":"<code>lang</code>","text":"(<code>str</code>)           \u2013            <p>The language of the text.</p>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister","title":"unregister","text":"<pre><code>unregister(*langs: str) -&gt; None\n</code></pre> <p>Remove splitter(s) from the registry.</p> <p>Parameters:</p> Source code in <code>src/chunklet/sentence_splitter/registry.py</code> <pre><code>@validate_input\ndef unregister(self, *langs: str) -&gt; None:\n    \"\"\"\n    Remove splitter(s) from the registry.\n\n    Args:\n        *langs: Language codes to remove\n    \"\"\"\n    for lang in langs:\n        self._splitters.pop(lang, None)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/registry/#chunklet.sentence_splitter.registry.CustomSplitterRegistry.unregister(*langs)","title":"<code>*langs</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Language codes to remove</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/","title":"sentence_splitter","text":""},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter","title":"chunklet.sentence_splitter.sentence_splitter","text":"<p>Classes:</p> <ul> <li> <code>BaseSplitter</code>           \u2013            <p>Base class for sentence splitting.</p> </li> <li> <code>SentenceSplitter</code>           \u2013            <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter","title":"BaseSplitter","text":"<p>Base class for sentence splitting. Defines the interface that all splitter implementations must adhere to.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into sentences.</p> </li> <li> <code>split_text</code>             \u2013              <p>Splits the given text into a list of sentences.</p> </li> </ul>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split","title":"split","text":"<pre><code>split(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Split text into sentences.</p> Note <p>Deprecated since 2.2.0. Will be removed in 3.0.0. Use <code>split_text</code> instead.</p> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@deprecated_callable(\n    use_instead=\"split_text\", deprecated_in=\"2.2.0\", removed_in=\"3.0.0\"\n)\ndef split(self, text: str, lang: str = \"auto\") -&gt; list[str]:  # pragma: no cover\n    \"\"\"\n    Split text into sentences.\n\n    Note:\n        Deprecated since 2.2.0. Will be removed in 3.0.0. Use `split_text` instead.\n    \"\"\"\n    return self.split_text(text, lang)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split_text","title":"split_text","text":"<pre><code>split_text(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits the given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the text.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>def split_text(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"Splits the given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str): The language of the text (e.g., 'en', 'fr', 'auto').\n\n    Returns:\n        list[str]: A list of sentences extracted from the text.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement 'split_text'.\")\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split_text(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.BaseSplitter.split_text(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto').</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter","title":"SentenceSplitter","text":"<pre><code>SentenceSplitter(verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseSplitter</code></p> <p>A robust and versatile utility dedicated to precisely segmenting text into individual sentences.</p> <p>Key Features: - Multilingual Support: Leverages language-specific algorithms and detection for broad coverage. - Custom Splitters: Uses centralized registry for custom splitting logic. - Fallback Mechanism: Employs a universal rule-based splitter for unsupported languages. - Robust Error Handling: Provides clear error reporting for issues with custom splitters. - Intelligent Post-processing: Cleans up split sentences by filtering empty strings and rejoining stray punctuation.</p> <p>Initializes the SentenceSplitter.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>detected_top_language</code>             \u2013              <p>Detects the top language of the given text using py3langid.</p> </li> <li> <code>split_file</code>             \u2013              <p>Read and split a file into sentences.</p> </li> <li> <code>split_text</code>             \u2013              <p>Splits a given text into a list of sentences.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef __init__(self, verbose: bool = False):\n    \"\"\"\n    Initializes the SentenceSplitter.\n\n    Args:\n        verbose (bool, optional): If True, enables verbose logging for debugging and informational messages.\n    \"\"\"\n    self.verbose = verbose\n    self.fallback_splitter = FallbackSplitter()\n\n    # Create a normalized identifier for language detection\n    self.identifier = LanguageIdentifier.from_pickled_model(\n        MODEL_FILE, norm_probs=True\n    )\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables verbose logging for debugging and informational messages.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language","title":"detected_top_language","text":"<pre><code>detected_top_language(text: str) -&gt; tuple[str, float]\n</code></pre> <p>Detects the top language of the given text using py3langid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the detected language code and its confidence.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef detected_top_language(self, text: str) -&gt; tuple[str, float]:\n    \"\"\"\n    Detects the top language of the given text using py3langid.\n\n    Args:\n        text (str): The input text to detect the language for.\n\n    Returns:\n        tuple[str, float]: A tuple containing the detected language code and its confidence.\n    \"\"\"\n    lang_detected, confidence = self.identifier.classify(text)\n    log_info(\n        self.verbose,\n        \"Language detection: '{}' with confidence {}.\",\n        lang_detected,\n        f\"{round(confidence) * 10}/10\",\n    )\n    return lang_detected, confidence\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.detected_top_language(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to detect the language for.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_file","title":"split_file","text":"<pre><code>split_file(\n    path: str | Path, lang: str = \"auto\"\n) -&gt; list[str]\n</code></pre> <p>Read and split a file into sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences extracted from the file.</p> </li> </ul> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>def split_file(self, path: str | Path, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Read and split a file into sentences.\n\n    Args:\n        path: Path to the file to read.\n        lang: The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to 'auto'.\n\n    Returns:\n        list[str]: A list of sentences extracted from the file.\n    \"\"\"\n    content = read_text_file(path)\n    return self.split_text(content, lang)\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_file(path)","title":"<code>path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to the file to read.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_file(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr', 'auto'). Defaults to 'auto'.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_text","title":"split_text","text":"<pre><code>split_text(text: str, lang: str = 'auto') -&gt; list[str]\n</code></pre> <p>Splits a given text into a list of sentences.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of sentences.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; splitter = SentenceSplitter()\n&gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"en\")\n['Hello world.', 'How are you?']\n&gt;&gt;&gt; splitter.split_text(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n['Bonjour le monde.', 'Comment allez-vous?']\n&gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"auto\")\n['Hello world.', 'How are you?']\n</code></pre> Source code in <code>src/chunklet/sentence_splitter/sentence_splitter.py</code> <pre><code>@validate_input\ndef split_text(self, text: str, lang: str = \"auto\") -&gt; list[str]:\n    \"\"\"\n    Splits a given text into a list of sentences.\n\n    Args:\n        text (str): The input text to be split.\n        lang (str, optional): The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'\n\n    Returns:\n        list[str]: A list of sentences.\n\n    Examples:\n        &gt;&gt;&gt; splitter = SentenceSplitter()\n        &gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"en\")\n        ['Hello world.', 'How are you?']\n        &gt;&gt;&gt; splitter.split_text(\"Bonjour le monde. Comment allez-vous?\", \"fr\")\n        ['Bonjour le monde.', 'Comment allez-vous?']\n        &gt;&gt;&gt; splitter.split_text(\"Hello world. How are you?\", \"auto\")\n        ['Hello world.', 'How are you?']\n    \"\"\"\n    if not text:\n        log_info(self.verbose, \"Input text is empty. Returning empty list.\")\n        return []\n\n    if lang == \"auto\":\n        logger.warning(\n            \"The language is set to `auto`. Consider setting the `lang` parameter to a specific language to improve reliability.\"\n        )\n        lang_detected, confidence = self.detected_top_language(text)\n        lang = lang_detected if confidence &gt;= 0.7 else lang\n\n    # Prioritize custom splitters from registry\n    if custom_splitter_registry.is_registered(lang):\n        sentences, splitter_name = custom_splitter_registry.split(text, lang)\n        log_info(self.verbose, \"Using registered splitter: {}\", splitter_name)\n    else:\n        sentences = None\n        for lang_set, handler in self.LANGUAGE_HANDLERS.items():\n            if lang in lang_set:\n                sentences = handler(lang, text)\n                break\n\n        # If no handler found, use fallback\n        if sentences is None:\n            logger.warning(\n                \"Using a universal rule-based splitter.\\n\"\n                \"Reason: Language not supported or detected with low confidence.\"\n            )\n            sentences = self.fallback_splitter.split(text)\n\n    processed_sentences = self._filter_sentences(sentences)\n\n    log_info(\n        self.verbose,\n        \"Text splitted into sentences. Total sentences detected: {}\",\n        len(processed_sentences),\n    )\n\n    return processed_sentences\n</code></pre>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_text(text)","title":"<code>text</code>","text":"(<code>str</code>)           \u2013            <p>The input text to be split.</p>"},{"location":"reference/chunklet/sentence_splitter/sentence_splitter/#chunklet.sentence_splitter.sentence_splitter.SentenceSplitter.split_text(lang)","title":"<code>lang</code>","text":"(<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The language of the text (e.g., 'en', 'fr'). Defaults to 'auto'</p>"},{"location":"reference/chunklet/sentence_splitter/terminators/","title":"terminators","text":""},{"location":"reference/chunklet/sentence_splitter/terminators/#chunklet.sentence_splitter.terminators","title":"chunklet.sentence_splitter.terminators","text":""},{"location":"reference/chunklet/visualizer/","title":"visualizer","text":""},{"location":"reference/chunklet/visualizer/#chunklet.visualizer","title":"chunklet.visualizer","text":"<p>Modules:</p> <ul> <li> <code>visualizer</code>           \u2013            </li> </ul>"},{"location":"reference/chunklet/visualizer/visualizer/","title":"visualizer","text":""},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer","title":"chunklet.visualizer.visualizer","text":"<p>Classes:</p> <ul> <li> <code>Visualizer</code>           \u2013            <p>A FastAPI-based web interface for visualizing document and code chunks.</p> </li> </ul>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer","title":"Visualizer","text":"<pre><code>Visualizer(\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    token_counter: Callable[[str], int] | None = None,\n)\n</code></pre> <p>A FastAPI-based web interface for visualizing document and code chunks.</p> <p>This server allows users to upload text or code files, processes them with Chunklet's <code>DocumentChunker</code> or <code>CodeChunker</code>, and returns the chunked data along with statistics. A minimal frontend interface is served at the root endpoint.</p> <p>Attributes:</p> <ul> <li> <code>host</code>               (<code>str</code>)           \u2013            <p>Host IP to bind the FastAPI server.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Port number to run the server on.</p> </li> <li> <code>document_chunker</code>               (<code>DocumentChunker</code>)           \u2013            <p>Chunklet document chunker instance.</p> </li> <li> <code>code_chunker</code>               (<code>CodeChunker</code>)           \u2013            <p>Chunklet code chunker instance.</p> </li> <li> <code>app</code>               (<code>FastAPI</code>)           \u2013            <p>FastAPI application instance.</p> </li> </ul> <p>Initializes the Visualizer server and configures chunkers.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>serve</code>             \u2013              <p>Starts the FastAPI server and prints the server URL.</p> </li> </ul> Source code in <code>src/chunklet/visualizer/visualizer.py</code> <pre><code>@validate_input\ndef __init__(\n    self,\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    token_counter: Callable[[str], int] | None = None,\n):\n    \"\"\"Initializes the Visualizer server and configures chunkers.\n\n    Args:\n        host (str): Host IP to run the server. Defaults to \"127.0.0.1\".\n        port (int): Port number to run the server. Defaults to 8000.\n        token_counter (Optional[Callable[[str], int]]): Function to count tokens\n            in text/code. Required for chunkers if used with `max_tokens`.\n    \"\"\"\n    if FastAPI is None:\n        raise ImportError(\n            \"The 'fastapi' library is not installed. \"\n            \"Please install it with 'pip install fastapi&gt;=0.115.12' or install the visualization extras \"\n            \"with 'pip install 'chunklet-py[visualization]''\"\n        )\n\n    self.host = host\n    self.port = port\n    self._token_counter = token_counter\n\n    self.app = FastAPI()\n\n    # Initialize chunkers\n    self.document_chunker = DocumentChunker(token_counter=token_counter)\n    self.code_chunker = CodeChunker(token_counter=token_counter)\n\n    self.static_dir = Path(__file__).parent / \"static\"\n\n    self.app.mount(\n        \"/static\", StaticFiles(directory=str(self.static_dir)), name=\"static\"\n    )\n\n    # API endpoints\n    self.app.get(\"/api/token_counter_status\")(self._get_token_counter_status)\n    self.app.get(\"/health\")(self._get_health_check)\n    self.app.get(\"/\")(self._get_index)\n    self.app.post(\"/api/chunk\")(self._chunk_file)\n</code></pre>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(host)","title":"<code>host</code>","text":"(<code>str</code>, default:                   <code>'127.0.0.1'</code> )           \u2013            <p>Host IP to run the server. Defaults to \"127.0.0.1\".</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(port)","title":"<code>port</code>","text":"(<code>int</code>, default:                   <code>8000</code> )           \u2013            <p>Port number to run the server. Defaults to 8000.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer(token_counter)","title":"<code>token_counter</code>","text":"(<code>Optional[Callable[[str], int]]</code>, default:                   <code>None</code> )           \u2013            <p>Function to count tokens in text/code. Required for chunkers if used with <code>max_tokens</code>.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer.token_counter","title":"token_counter  <code>property</code> <code>writable</code>","text":"<pre><code>token_counter\n</code></pre> <p>Get the current token counter function.</p>"},{"location":"reference/chunklet/visualizer/visualizer/#chunklet.visualizer.visualizer.Visualizer.serve","title":"serve","text":"<pre><code>serve()\n</code></pre> <p>Starts the FastAPI server and prints the server URL.</p> Source code in <code>src/chunklet/visualizer/visualizer.py</code> <pre><code>def serve(self):\n    \"\"\"Starts the FastAPI server and prints the server URL.\"\"\"\n    if uvicorn is None:\n        raise ImportError(\n            \"The 'uvicorn' library is not installed. \"\n            \"Please install it with 'pip install uvicorn&gt;=0.34.0' or install the visualization extras \"\n            \"with 'pip install 'chunklet-py[visualization]''\"\n        )\n\n    print(\" =\" * 20)\n    print(\"\\nTEXT CHUNK VISUALIZER\")\n    print(\"= \" * 20)\n    print(f\"URL: http://{self.host}:{self.port}\")\n\n    uvicorn.run(self.app, host=self.host, port=self.port)\n</code></pre>"}]}